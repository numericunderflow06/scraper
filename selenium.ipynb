{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ef4fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting urllib3<3,>=1.26 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Using cached trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2021.10.8 (from selenium)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting typing_extensions~=4.9 (from selenium)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting idna (from trio~=0.17->selenium)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Using cached selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl (182 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, urllib3, typing_extensions, sniffio, pysocks, pycparser, idna, h11, certifi, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-25.3.0 certifi-2025.4.26 cffi-1.17.1 h11-0.16.0 idna-3.10 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.32.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 urllib3-2.4.0 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://selenium.dev/')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "507ab0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Incremental Scroll for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_incremental_scroll' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Body element loaded.\n",
      "Looking for the first pop-up ('跳过')...\n",
      "First pop-up ('跳过') not found or timed out.\n",
      "Looking for the second pop-up ('X')...\n",
      "Second pop-up ('X') not found or timed out.\n",
      "\n",
      "--- Starting Incremental Scroll and Screenshot Process ---\n",
      "--- Scroll Attempt #1 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_01_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 482, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #2 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_02_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 963, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #3 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_03_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 1445, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #4 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_04_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 1926, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #5 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_05_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 2408, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #6 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_06_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 2890, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #7 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_07_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 3371, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #8 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_08_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 3853, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #9 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_09_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 4322, Viewport height: 602, Total scrollHeight: 4924\n",
      "Reached the bottom of the page.\n",
      "Saved final bottom screenshot: screenshots_incremental_scroll\\scroll_09_at_bottom.png\n",
      "\n",
      "--- Incremental Scroll and Screenshot Process Finished ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "--- Script Finished ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'screenshot_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    147\u001b[39m scroll_and_screenshot_by_distance(\n\u001b[32m    148\u001b[39m     target_url,\n\u001b[32m    149\u001b[39m     scroll_pause_time=\u001b[32m2.0\u001b[39m,\n\u001b[32m    150\u001b[39m     scroll_increment_js=\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollBy(0, window.innerHeight * 0.8);\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    151\u001b[39m     )\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Script Finished ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheck the \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mscreenshot_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m folder for screenshots.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'screenshot_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException\n",
    ")\n",
    "\n",
    "def scroll_and_screenshot_by_distance(url, scroll_pause_time=1.5, scroll_increment_js=\"window.scrollBy(0, window.innerHeight);\"):\n",
    "    \"\"\"\n",
    "    Navigates to a URL, handles popups, scrolls down in increments, taking a screenshot\n",
    "    at each step until the bottom of the page is reached.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the page to scroll.\n",
    "        scroll_pause_time (float): Time in seconds to wait after each scroll\n",
    "                                   for content to potentially load.\n",
    "        scroll_increment_js (str): JavaScript to execute for scrolling.\n",
    "                                   Default scrolls by one viewport height.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Keep browser visible to observe\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,800\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_incremental_scroll\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        # driver.maximize_window() # Maximize or set specific size above\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            print(\"Body element loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Page body did not become present within timeout.\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        # --- Handle Initial Pop-ups (Integrated robust logic) ---\n",
    "        try:\n",
    "            print(\"Looking for the first pop-up ('跳过')...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            print(\"Clicking '跳过'...\"); driver.execute_script(\"arguments[0].click();\", skip_button); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"First pop-up ('跳过') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error handling first pop-up: {e}\")\n",
    "        try:\n",
    "            print(\"Looking for the second pop-up ('X')...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]\", \"//i[contains(@class, 'cube-dialog-close')]\", \"//div[contains(@class, 'Modal_modal')]//i[contains(@class, 'Modal_closeIcon')]\", \"//button[@aria-label='Close']\" ]\n",
    "            close_button_found = False\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, xpath))); print(f\"Clicking second pop-up 'X'...\"); driver.execute_script(\"arguments[0].click();\", close_button); close_button_found = True; time.sleep(0.5); break\n",
    "                 except TimeoutException: continue\n",
    "                 except Exception: continue\n",
    "            if not close_button_found: print(\"Second pop-up ('X') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error during second pop-up handling: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # Optional: Click 'Comments' Tab\n",
    "        # try:\n",
    "        #     print(\"Looking for and clicking '评论' tab before scrolling...\")\n",
    "        #     tab_xpath = \"//span[text()='评论' and contains(@class,'tabs__item__title')]/ancestor::div[contains(@class,'tabs__item')] | //div[contains(@class, 'tabs__item') and .//span[text()='评论']] | //div[contains(@class, 'action-bar__item') and contains(., '评论')]\"\n",
    "        #     comments_tab_element = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, tab_xpath)))\n",
    "        #     driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', inline: 'nearest'});\", comments_tab_element)\n",
    "        #     time.sleep(0.5)\n",
    "        #     comments_tab_clickable = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, tab_xpath)))\n",
    "        #     driver.execute_script(\"arguments[0].click();\", comments_tab_clickable)\n",
    "        #     print(\"Clicked '评论' tab. Waiting before scroll...\")\n",
    "        #     time.sleep(2.5)\n",
    "        # except Exception as e_tab:\n",
    "        #     print(f\"Could not click '评论' tab before scrolling (maybe not needed or error): {e_tab}\")\n",
    "\n",
    "        print(\"\\n--- Starting Incremental Scroll and Screenshot Process ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_attempt = 0\n",
    "\n",
    "        while True:\n",
    "            scroll_attempt += 1\n",
    "            print(f\"--- Scroll Attempt #{scroll_attempt} ---\")\n",
    "\n",
    "            screenshot_path = os.path.join(screenshot_dir, f\"scroll_{scroll_attempt:02d}_before.png\")\n",
    "            try:\n",
    "                driver.save_screenshot(screenshot_path)\n",
    "                print(f\"Saved screenshot: {screenshot_path}\")\n",
    "            except Exception as e_ss:\n",
    "                print(f\"Failed to save screenshot {screenshot_path}: {e_ss}\")\n",
    "\n",
    "            print(f\"Scrolling down using: {scroll_increment_js}\")\n",
    "            driver.execute_script(scroll_increment_js)\n",
    "\n",
    "            print(f\"Pausing for {scroll_pause_time} seconds...\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            current_scroll_y = driver.execute_script(\"return window.pageYOffset || document.documentElement.scrollTop;\")\n",
    "            viewport_height = driver.execute_script(\"return window.innerHeight;\")\n",
    "            print(f\"  Current scrollY: {round(current_scroll_y)}, Viewport height: {round(viewport_height)}, Total scrollHeight: {new_height}\")\n",
    "\n",
    "            if current_scroll_y + viewport_height >= new_height - 10:\n",
    "                print(\"Reached the bottom of the page.\")\n",
    "                final_bottom_path = os.path.join(screenshot_dir, f\"scroll_{scroll_attempt:02d}_at_bottom.png\")\n",
    "                try: driver.save_screenshot(final_bottom_path)\n",
    "                except Exception as e_fin_ss: print(f\"Could not save final screenshot: {e_fin_ss}\")\n",
    "                print(f\"Saved final bottom screenshot: {final_bottom_path}\")\n",
    "                break\n",
    "            elif scroll_attempt > 50:\n",
    "                print(\"Reached max scroll attempts (50). Stopping.\")\n",
    "                break\n",
    "            last_height = new_height\n",
    "        print(\"\\n--- Incremental Scroll and Screenshot Process Finished ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        # --- CORRECTED INDENTATION FOR ERROR SCREENSHOT TRY-EXCEPT ---\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error_scroll_script.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved error screenshot: {error_ss_path}\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "        # --- END CORRECTION ---\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing the browser...\")\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Incremental Scroll for URL: {target_url} ---\")\n",
    "    scroll_and_screenshot_by_distance(\n",
    "        target_url,\n",
    "        scroll_pause_time=2.0,\n",
    "        scroll_increment_js=\"window.scrollBy(0, window.innerHeight * 0.8);\"\n",
    "        )\n",
    "    print(\"--- Script Finished ---\")\n",
    "    print(f\"Check the '{screenshot_dir}' folder for screenshots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6347192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content...\n",
      "Post content scraped (length: 627).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Loop/Scroll attempt #1 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags.\n",
      "    Added 2 new unique comments.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "--- Loop/Scroll attempt #2 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "--- Loop/Scroll attempt #3 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "Scroll height unchanged and no new comments for 2 strikes. Assuming all loaded or stuck.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 2 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "...\n",
      "\n",
      "--- Comments (2) ---\n",
      "不一定会成日鬼那样的社会，天朝…\n",
      "社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'screenshot_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 197\u001b[39m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> No comments were scraped. <<<\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheck console logs and \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mscreenshot_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m folder for details.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'screenshot_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException\n",
    ")\n",
    "\n",
    "def scrape_post_and_comments_on_scroll(url, max_scroll_loops=15, scroll_pause_time=2.5):\n",
    "    \"\"\"\n",
    "    Navigates to a Xueqiu post, handles popups, scrolls to load comments,\n",
    "    and attempts to scrape them. Also clicks 'expand replies' and 'load more'.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        short_wait = WebDriverWait(driver, 7)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load. Page might be different.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\") # Confirmation\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # Scrape Main Post Content\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        no_new_content_strikes = 0\n",
    "\n",
    "        for i in range(max_scroll_loops):\n",
    "            print(f\"--- Loop/Scroll attempt #{i+1} ---\")\n",
    "            initial_comment_count = len(unique_comment_texts_scraped)\n",
    "\n",
    "            # 1. Click \"查看N条回复\"\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            try:\n",
    "                expand_buttons = driver.find_elements(By.XPATH, expand_reply_xpath)\n",
    "                if expand_buttons:\n",
    "                    print(f\"Found {len(expand_buttons)} 'Expand Replies' links.\")\n",
    "                    for button_idx, button in enumerate(expand_buttons):\n",
    "                        try:\n",
    "                            if button.is_displayed():\n",
    "                                print(f\"  Clicking 'Expand Replies' #{button_idx+1}...\"); driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button); time.sleep(0.3)\n",
    "                                driver.execute_script(\"arguments[0].click();\", button); time.sleep(1.5)\n",
    "                        except StaleElementReferenceException: print(\"  Stale 'Expand Replies' link, skipping.\")\n",
    "                        except ElementNotInteractableException: print(\"  'Expand Replies' link not interactable, skipping.\")\n",
    "                        except Exception as e_expand: print(f\"  Error clicking 'Expand Replies': {e_expand}\")\n",
    "            except Exception as e_find_expand: print(f\"Could not search for 'Expand Replies' buttons: {e_find_expand}\")\n",
    "\n",
    "            # 2. Scroll down\n",
    "            print(\"Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight + 500);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            # 3. Attempt to scrape comments\n",
    "            comment_text_xpath = \"//div[@class='comment__item__main']/p\"\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "                if comment_p_tags:\n",
    "                    print(f\"  Found {len(comment_p_tags)} potential comment <p> tags.\")\n",
    "                    new_comments_found_this_pass = 0\n",
    "                    for p_tag in comment_p_tags:\n",
    "                        try:\n",
    "                            comment_text = p_tag.text.strip()\n",
    "                            if comment_text and comment_text not in unique_comment_texts_scraped:\n",
    "                                unique_comment_texts_scraped.add(comment_text)\n",
    "                                new_comments_found_this_pass +=1\n",
    "                        except StaleElementReferenceException: continue\n",
    "                        except Exception as e_text: print(f\"    Error getting text from a p_tag: {e_text}\")\n",
    "                    if new_comments_found_this_pass > 0: print(f\"    Added {new_comments_found_this_pass} new unique comments.\")\n",
    "            except Exception as e_find: print(f\"  Error finding comment <p> tags: {e_find}\")\n",
    "\n",
    "            # 4. Click \"展开查看更多\"\n",
    "            load_more_comments_xpath = \"//div[contains(@class,'more-comment') and (contains(., '展开查看更多') or contains(., '加载更多'))]\"\n",
    "            try:\n",
    "                load_more_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多/加载更多' button. Clicking...\"); driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button); time.sleep(0.3)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button); print(\"  Clicked '展开查看更多/加载更多'.\"); time.sleep(scroll_pause_time)\n",
    "            except TimeoutException: pass\n",
    "            except Exception as e_load_more: print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            print(f\"  Current total comments scraped: {len(unique_comment_texts_scraped)}. Scroll height: {current_height}\")\n",
    "            if len(unique_comment_texts_scraped) > initial_comment_count: no_new_content_strikes = 0\n",
    "            else: no_new_content_strikes += 1\n",
    "\n",
    "            if current_height == last_height and no_new_content_strikes >= 2 :\n",
    "                print(\"Scroll height unchanged and no new comments for 2 strikes. Assuming all loaded or stuck.\")\n",
    "                break\n",
    "            elif no_new_content_strikes >= 3:\n",
    "                print(\"No new comments found for 3 consecutive strikes. Assuming all loaded.\")\n",
    "                break\n",
    "            last_height = current_height\n",
    "            if i == max_scroll_loops -1 : print(\"Reached max scroll loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"loop_end_{i+1}.png\"))\n",
    "\n",
    "        scraped_data[\"comments\"] = list(unique_comment_texts_scraped)\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        # --- CORRECTED INDENTATION FOR ERROR SCREENSHOT TRY-EXCEPT ---\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "        # --- END CORRECTION ---\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing the browser...\")\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_and_comments_on_scroll(target_url, max_scroll_loops=10, scroll_pause_time=3.0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\")\n",
    "        print(data[\"post_content\"][:500] + \"...\" if len(data[\"post_content\"]) > 500 else data[\"post_content\"])\n",
    "    else:\n",
    "        print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment in enumerate(data[\"comments\"][:20]): # Print first 20\n",
    "            print(f\"{i+1}. {comment[:150]}\" + \"...\" if len(comment)>150 else comment)\n",
    "        if len(data[\"comments\"]) > 20:\n",
    "            print(f\"... and {len(data['comments']) - 20} more comments.\")\n",
    "    else:\n",
    "        print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Check console logs and '{screenshot_dir}' folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f0ee95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_post_all_comments' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content...\n",
      "Post content scraped (length: 706).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Main Loop Iteration #1 ---\n",
      "  Scrolling down...\n",
      "    Added 18 new unique comments this pass.\n",
      "  '展开查看更多/加载更多' button not found or not clickable this pass.\n",
      "  Loop 1 end. Total unique comments: 18. Scroll height: 4924. Last height: 4924\n",
      "--- Main Loop Iteration #2 ---\n",
      "  Scrolling down...\n",
      "  '展开查看更多/加载更多' button not found or not clickable this pass.\n",
      "  Loop 2 end. Total unique comments: 18. Scroll height: 4924. Last height: 4924\n",
      "  No new actions or comments strike: 1\n",
      "--- Main Loop Iteration #3 ---\n",
      "  Scrolling down...\n",
      "  '展开查看更多/加载更多' button not found or not clickable this pass.\n",
      "  Loop 3 end. Total unique comments: 18. Scroll height: 4924. Last height: 4924\n",
      "  No new actions or comments strike: 2\n",
      "No new comments found and no interaction buttons successfully clicked for 2 consecutive loops. Assuming completion.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 18 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "零食甚至“宠物饮品”一应俱全\n",
      "。\n",
      "独酌时\n",
      "，\n",
      "希望宠物也能“陪一杯”\n",
      "。\n",
      "\n",
      "3️⃣ 陪伴型机器人：技术不卖功能\n",
      "，\n",
      "卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT\n",
      "，\n",
      "不扫地\n",
      "、\n",
      "不聊天\n",
      "、\n",
      "不能干活——但它会“撒娇”\n",
      "、\n",
      "会“跟着你”\n",
      "、\n",
      "会“让你想抱”\n",
      "。\n",
      "\n",
      "这是一台卖情绪价值的机器人\n",
      "。\n",
      "\n",
      "$恒生指数(HKHSI)$ $上证指数(SH000001)$ $招商银行(SH600036)$\n",
      "\n",
      "--- Comments (18) ---\n",
      "1. 神经，愚蠢没有逻辑，人类是群居动物，它们渴望伴侣，阶段性的单身是因为经济原因，因为穷，男的穷养不起家，结不了婚，女的穷，养不起自己，并且找不到能养起自己的人，所以不嫁。\n",
      "未来随着机器人人工智能，经济指数级增长，人类会涨校园里的大学生一样无忧无虑，男人和女人因为爱情成群结队。\n",
      "2. 有个手机就够了，老公老婆都可以不要\n",
      "3. 十年二十年后，大概也这样了，会出什么龙头公司股票呢？\n",
      "4. 总需求停滞，新需求就是存量竞争，从物质需求到精神需求，从奢靡需求到精简需求，从传统需求到新型，个性需求。。但事实上，看看日本的股市龙头，这些需求转向基本做不大，市值最大的还是一些大企业，银行，保险，电信，商社，汽车，电气，半导体，互联网；到了中国大概率还是中字头，资源类，制造业龙头，龙头科创互联网。。\n",
      "5. 不婚不育最根本原因是上世纪不准生，一旦生态链损毁，修复何其难。\n",
      "6. 没有人喜欢照顾别人的情绪，但是所有人都想从别人那里找到情绪价值。\n",
      "7. 我这段时间的观察与思考和这部分内容是一致的。我女儿最喜欢的便利店是全家Family Mart，里面的海苔饭团、咖喱鸡饭都是冷藏食品，她一点不嫌弃，反而很热爱，还兴高采烈地告诉我，她很喜欢这些食品。\n",
      "宠物这一部分我前段时间也在关注，今早还发了一条有关于此的消息。\n",
      "五一节期间和一位50岁的大哥聊天，他说令他意外的是，成都天府广场一座被传统零售商业模式淘汰的商场，现在被谷子经济救活了，里面人山人海，交易量火爆。\n",
      "8. 不可能的。两个国家本质上的价值观完全不一样。我身边蛮多00后，目前没听到一个说30以后不结婚的。穷女和稳定男是最迫切想结婚的人群。这还是经常打游戏的宅群。\n",
      "9. 我们的社会确实朝着这个方向走\n",
      "个人取代家庭，成为社会的最小单元了\n",
      "不管我们承不承认，年轻人的消费习惯和我们完全不一样了\n",
      "$泡泡玛特(09992)$\n",
      "10. 性爱机器人产品会供不应求，这类公司会卷出一个品牌。\n",
      "11. 不一定会成日鬼那样的社会，天朝…\n",
      "12. 我们这现在最先来的应该是老年经济\n",
      "13. 内卷国家的必然\n",
      "14. 如果真能成为日本那样的发达国家就知足吧、优衣库2000多美刀的收入，物价再贵去掉吃喝拉撒也能一个月换个苹果手机了，这就是我们这里看不起的售货员.\n",
      "15. 东亚社畜主义社会\n",
      "16. 文明的诅咒\n",
      "17. 社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "18. 中国人均财富水平距离日本还差的太远，更大的可能性是未富先老，或许更应该思考的是一大堆拮据的老年人最可能消费什么\n",
      "\n",
      "==============================\n",
      "Check console logs and folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def scrape_post_and_all_comments(url, max_main_loops=15, scroll_pause_time=2.5):\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_all_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        short_wait = WebDriverWait(driver, 5) # Shorter wait for elements within loops\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "\n",
    "        # Scrape Main Post Content\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        no_new_actions_or_comments_strikes = 0\n",
    "\n",
    "        for i in range(max_main_loops):\n",
    "            print(f\"--- Main Loop Iteration #{i+1} ---\")\n",
    "            action_taken_this_loop = False\n",
    "            comments_found_before_interactions = len(unique_comment_texts_scraped)\n",
    "\n",
    "            # 1. Click ALL visible \"查看N条回复\" (Expand Replies)\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            # Loop to click these as new ones might appear after expanding others\n",
    "            while True:\n",
    "                clicked_an_expand_button_this_pass = False\n",
    "                try:\n",
    "                    expand_buttons = driver.find_elements(By.XPATH, expand_reply_xpath)\n",
    "                    if not expand_buttons: # print(\"  No 'Expand Replies' links found this pass.\");\n",
    "                        break\n",
    "\n",
    "                    # Filter only visible buttons before attempting to click\n",
    "                    visible_expand_buttons = [b for b in expand_buttons if b.is_displayed()]\n",
    "                    if not visible_expand_buttons: # print(\"  No *visible* 'Expand Replies' links.\");\n",
    "                        break\n",
    "\n",
    "                    print(f\"  Found {len(visible_expand_buttons)} visible 'Expand Replies' links to click.\")\n",
    "                    for button in visible_expand_buttons:\n",
    "                        try:\n",
    "                            # print(\"    Scrolling to 'Expand Replies' and clicking...\")\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", button)\n",
    "                            time.sleep(0.5) # Wait for scroll\n",
    "                            # Use a short wait for clickability\n",
    "                            button_to_click = short_wait.until(EC.element_to_be_clickable(button))\n",
    "                            driver.execute_script(\"arguments[0].click();\", button_to_click) # JS click\n",
    "                            # button_to_click.click() # Regular click\n",
    "                            print(f\"    Clicked 'Expand Replies': {button.text[:20]}\")\n",
    "                            action_taken_this_loop = True\n",
    "                            clicked_an_expand_button_this_pass = True\n",
    "                            time.sleep(1.5) # Wait for replies to load\n",
    "                            # After clicking, the DOM might change, so we might need to re-find buttons in the next 'while True' iteration\n",
    "                        except StaleElementReferenceException: print(\"    Stale 'Expand Replies' link during click, will re-evaluate.\"); break # Break inner for to re-find\n",
    "                        except ElementNotInteractableException: print(\"    'Expand Replies' link not interactable, might be covered or disabled.\")\n",
    "                        except TimeoutException: print(\"    Timeout waiting for 'Expand Replies' to be clickable.\")\n",
    "                        except Exception as e_expand: print(f\"    Error clicking one 'Expand Replies': {e_expand}\")\n",
    "                    if not clicked_an_expand_button_this_pass: # No visible ones were successfully clicked\n",
    "                        break\n",
    "                except Exception as e_find_expand: print(f\"  Error finding 'Expand Replies': {e_find_expand}\"); break\n",
    "                if not clicked_an_expand_button_this_pass: break # If loop completes without any clicks\n",
    "\n",
    "            # 2. Scroll down\n",
    "            print(\"  Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            # 3. Attempt to scrape comments\n",
    "            comment_text_xpath = \"//div[@class='comment__item__main']/p\" # This XPath seems correct\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "                if comment_p_tags:\n",
    "                    # print(f\"  Found {len(comment_p_tags)} potential comment <p> tags on this pass.\")\n",
    "                    new_comments_this_pass = 0\n",
    "                    for p_tag in comment_p_tags:\n",
    "                        try:\n",
    "                            comment_text = p_tag.text.strip()\n",
    "                            if comment_text and comment_text not in unique_comment_texts_scraped:\n",
    "                                unique_comment_texts_scraped.add(comment_text)\n",
    "                                new_comments_this_pass += 1\n",
    "                        except StaleElementReferenceException: continue\n",
    "                        except Exception: continue # Ignore errors for individual comment text retrieval\n",
    "                    if new_comments_this_pass > 0:\n",
    "                        print(f\"    Added {new_comments_this_pass} new unique comments this pass.\")\n",
    "                        action_taken_this_loop = True # Finding new comments is an action\n",
    "            except Exception as e_find_comments: print(f\"  Error finding comment <p> tags: {e_find_comments}\")\n",
    "\n",
    "            # 4. Click \"展开查看更多\" (Load More Main Comments)\n",
    "            load_more_comments_xpath = \"//div[contains(@class,'more-comment') and (contains(., '展开查看更多') or contains(., '加载更多'))]\"\n",
    "            try:\n",
    "                # Use short_wait for this button as it might appear/disappear\n",
    "                load_more_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多/加载更多' button. Clicking...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", load_more_button); time.sleep(0.3)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                print(\"    Clicked '展开查看更多/加载更多'.\")\n",
    "                action_taken_this_loop = True\n",
    "                time.sleep(scroll_pause_time + 1) # Wait longer after this action\n",
    "            except TimeoutException: print(\"  '展开查看更多/加载更多' button not found or not clickable this pass.\")\n",
    "            except Exception as e_load_more: print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            print(f\"  Loop {i+1} end. Total unique comments: {len(unique_comment_texts_scraped)}. Scroll height: {current_height}. Last height: {last_height}\")\n",
    "\n",
    "            if not action_taken_this_loop and len(unique_comment_texts_scraped) == comments_found_before_interactions:\n",
    "                no_new_actions_or_comments_strikes += 1\n",
    "                print(f\"  No new actions or comments strike: {no_new_actions_or_comments_strikes}\")\n",
    "            else:\n",
    "                no_new_actions_or_comments_strikes = 0 # Reset if something happened\n",
    "\n",
    "            if no_new_actions_or_comments_strikes >= 2:\n",
    "                print(\"No new comments found and no interaction buttons successfully clicked for 2 consecutive loops. Assuming completion.\")\n",
    "                break\n",
    "\n",
    "            last_height = current_height\n",
    "            if i == max_main_loops - 1: print(\"Reached max main loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"main_loop_end_{i+1}.png\"))\n",
    "\n",
    "        scraped_data[\"comments\"] = list(unique_comment_texts_scraped)\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_and_all_comments(target_url, max_main_loops=10, scroll_pause_time=2.5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\")\n",
    "        print(data[\"post_content\"]) # Print full post content\n",
    "    else:\n",
    "        print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment in enumerate(data[\"comments\"]):\n",
    "            print(f\"{i+1}. {comment}\") # Print full comment\n",
    "    else:\n",
    "        print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Check console logs and folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "511f1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content...\n",
      "Post content scraped (length: 706).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Main Loop Iteration #1 ---\n",
      "  Scrolling down...\n",
      "    Added 18 new unique comments from page.\n",
      "  Found '展开查看更多' button. Clicking...\n",
      "    Clicked '展开查看更多'.\n",
      "  Loop 1 end. Total unique comments: 18. Previously: -1\n",
      "--- Main Loop Iteration #2 ---\n",
      "  Scrolling down...\n",
      "    Added 15 new unique comments from page.\n",
      "  '展开查看更多' button not found or not clickable this pass (might be all loaded).\n",
      "  Loop 2 end. Total unique comments: 33. Previously: 18\n",
      "--- Main Loop Iteration #3 ---\n",
      "  Scrolling down...\n",
      "  '展开查看更多' button not found or not clickable this pass (might be all loaded).\n",
      "  Loop 3 end. Total unique comments: 33. Previously: 33\n",
      "No actions taken (no buttons clicked, no new comments found) and comment count unchanged. Assuming completion.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 33 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "零食甚至“宠物饮品”一应俱全\n",
      "。\n",
      "独酌时\n",
      "，\n",
      "希望宠物也能“陪一杯”\n",
      "。\n",
      "\n",
      "3️⃣ 陪伴型机器人：技术不卖功能\n",
      "，\n",
      "卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT\n",
      "，\n",
      "不扫地\n",
      "、\n",
      "不聊天\n",
      "、\n",
      "不能干活——但它会“撒娇”\n",
      "、\n",
      "会“跟着你”\n",
      "、\n",
      "会“让你想抱”\n",
      "。\n",
      "\n",
      "这是一台卖情绪价值的机器人\n",
      "。\n",
      "\n",
      "$恒生指数(HKHSI)$ $上证指数(SH000001)$ $招商银行(SH600036)$\n",
      "\n",
      "--- Comments (33) ---\n",
      "1. 我这段时间的观察与思考和这部分内容是一致的。我女儿最喜欢的便利店是全家Family Mart，里面的海苔饭团、咖喱鸡饭都是冷藏食品，她一点不嫌弃，反而很热爱，还兴高采烈地告诉我，她很喜欢这些食品。\n",
      "宠物这一部分我前段时间也在关注，今早还发了一条有关于此的消息。\n",
      "五一节期间和一位50岁的大哥聊天，他说令他意外的是，成都天府广场一座被传统零售商业模式淘汰的商场，现在被谷子经济救活了，里面人山人海，交易量火爆。\n",
      "2. 不可能的。两个国家本质上的价值观完全不一样。我身边蛮多00后，目前没听到一个说30以后不结婚的。穷女和稳定男是最迫切想结婚的人群。这还是经常打游戏的宅群。\n",
      "3. 干宠物经济就行了\n",
      "4. 不婚不育最根本原因是上世纪不准生，一旦生态链损毁，修复何其难。\n",
      "5. 中国人均财富水平距离日本还差的太远，更大的可能性是未富先老，或许更应该思考的是一大堆拮据的老年人最可能消费什么\n",
      "6. 我们的社会确实朝着这个方向走\n",
      "个人取代家庭，成为社会的最小单元了\n",
      "不管我们承不承认，年轻人的消费习惯和我们完全不一样了\n",
      "$泡泡玛特(09992)$\n",
      "7. 情绪经济啊\n",
      "8. 内卷国家的必然\n",
      "9. AI娃娃\n",
      "10. 东亚社畜主义社会\n",
      "11. 文明的诅咒\n",
      "12. 社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "13. 神经，愚蠢没有逻辑，人类是群居动物，它们渴望伴侣，阶段性的单身是因为经济原因，因为穷，男的穷养不起家，结不了婚，女的穷，养不起自己，并且找不到能养起自己的人，所以不嫁。\n",
      "未来随着机器人人工智能，经济指数级增长，人类会涨校园里的大学生一样无忧无虑，男人和女人因为爱情成群结队。\n",
      "14. 不一定会成日鬼那样的社会，天朝…\n",
      "15. 没有人喜欢照顾别人的情绪，但是所有人都想从别人那里找到情绪价值。\n",
      "16. 性爱机器人产品会供不应求，这类公司会卷出一个品牌。\n",
      "17. 手动转存\n",
      "18. 养娃成本太高了，养差了自己又觉得对不起孩子，还不如不生。我亲戚去帮人补课，机构收1200一节课，她自己得100块，这种价格很多家庭都是难以承受的。各种培训班真的该好好打一下，往死里打。不然就会引起人的攀比心理，越攀比越累，干脆不生。\n",
      "19. 圣杯\n",
      "20. 如果真能成为日本那样的发达国家就知足吧、优衣库2000多美刀的收入，物价再贵去掉吃喝拉撒也能一个月换个苹果手机了，这就是我们这里看不起的售货员.\n",
      "21. mark\n",
      "22. 转发。感觉陪伴型机器人在AI时代有巨大的发展潜力。\n",
      "23. 感觉怡红院很快会出牌照？？\n",
      "24. 总需求停滞，新需求就是存量竞争，从物质需求到精神需求，从奢靡需求到精简需求，从传统需求到新型，个性需求。。但事实上，看看日本的股市龙头，这些需求转向基本做不大，市值最大的还是一些大企业，银行，保险，电信，商社，汽车，电气，半导体，互联网；到了中国大概率还是中字头，资源类，制造业龙头，龙头科创互联网。。\n",
      "25. 转：\n",
      "1980年代，日本一人户占比只有20%，如今逼近40%。未来的日本将成为半数人口是单身的“超级单身社会”。\n",
      "而在中国，独居、晚婚、不婚人群也正快速上升。\n",
      "这不是偶然，而是结构性变化。\n",
      "一个人生活，意味着从吃饭、出行、情感，到陪伴、安全感，都要独自完成。这背后，藏着海量“新需求”和“新供给”。\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭、饭团、即食热食，成了日本最大的餐饮品牌。\n",
      "2023财年营收破10万亿日元（约5000亿人民币），稳压传统连锁餐饮。\n",
      "东京的面馆、咖啡馆大量设置“一个人座位”，配隔板、配平板，边吃边追剧，社交压力为零——孤独，是可以被尊重的生活方式。\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万，远超15岁以下儿童。孤独都市人，把情感投射给了猫狗。\n",
      "日本宠物主对待他们的宠物就像对待家人一样，甚至比自己的地位还高。宠物用品已全面“母婴化”：推车、衣服、零食甚至“宠物饮品”一应俱全。独酌时，希望宠物也能“陪一杯”。\n",
      "3️⃣ 陪伴型机器人：技术不卖功能，卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT，不扫地、不聊天、不能干活——但它会“撒娇”、会“跟着你”、会“让你想抱”。\n",
      "这是一台卖情绪价值的机器人\n",
      "26. 转\n",
      "27. 存\n",
      "28. 为宠物股炒作提供注释\n",
      "29. 有个手机就够了，老公老婆都可以不要\n",
      "30. 十年二十年后，大概也这样了，会出什么龙头公司股票呢？\n",
      "31. 东达不可避免\n",
      "32. 我们这现在最先来的应该是老年经济\n",
      "33. 利好养老类股票。\n",
      "\n",
      "==============================\n",
      "Check console logs and folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def scrape_post_and_all_comments(url, max_main_loops=20, scroll_pause_time=2.5): # Increased max_main_loops\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_all_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        # Shorter wait for elements that appear/disappear within loops\n",
    "        interaction_wait = WebDriverWait(driver, 7)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "\n",
    "        # Scrape Main Post Content\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_total_comments = -1 # Initialize to a value different from initial count\n",
    "\n",
    "        for i in range(max_main_loops):\n",
    "            print(f\"--- Main Loop Iteration #{i+1} ---\")\n",
    "            action_taken_this_loop = False\n",
    "            comments_at_loop_start = len(unique_comment_texts_scraped)\n",
    "\n",
    "            # 1. Click ALL visible \"查看N条回复\" (Expand Replies)\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            # Inner loop to keep clicking expand replies as long as new ones appear or are clickable\n",
    "            expand_attempts = 0\n",
    "            while expand_attempts < 5: # Limit attempts to avoid infinite loop if something goes wrong\n",
    "                expand_attempts += 1\n",
    "                clicked_an_expand_button_this_pass = False\n",
    "                try:\n",
    "                    # Re-find elements each time as DOM changes\n",
    "                    visible_expand_buttons = [b for b in driver.find_elements(By.XPATH, expand_reply_xpath) if b.is_displayed()]\n",
    "                    if not visible_expand_buttons: break # No more visible expand buttons\n",
    "\n",
    "                    print(f\"  Found {len(visible_expand_buttons)} visible 'Expand Replies' links.\")\n",
    "                    for button in visible_expand_buttons:\n",
    "                        try:\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", button); time.sleep(0.4)\n",
    "                            # Using a short wait for the specific button to ensure it's ready\n",
    "                            button_to_click = WebDriverWait(driver, 3).until(EC.element_to_be_clickable(button))\n",
    "                            driver.execute_script(\"arguments[0].click();\", button_to_click)\n",
    "                            print(f\"    Clicked 'Expand Replies': {button.text[:20]}\")\n",
    "                            action_taken_this_loop = True\n",
    "                            clicked_an_expand_button_this_pass = True\n",
    "                            time.sleep(1.5) # Wait for replies\n",
    "                        except (StaleElementReferenceException, TimeoutException, ElementNotInteractableException): continue # Try next button or re-evaluate\n",
    "                        except Exception as e_expand: print(f\"    Error clicking one 'Expand Replies': {e_expand}\")\n",
    "                    if not clicked_an_expand_button_this_pass: break # No more were clicked this pass\n",
    "                except Exception as e_find_expand: print(f\"  Error finding 'Expand Replies': {e_find_expand}\"); break\n",
    "\n",
    "\n",
    "            # 2. Scroll down (helps reveal \"Load More\" and more comments)\n",
    "            print(\"  Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time) # Allow content to load after scroll\n",
    "\n",
    "            # 3. Attempt to scrape comments (after potential expansions and scroll)\n",
    "            comment_text_xpath = \"//div[@class='comment__item__main']/p\"\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "                new_comments_this_pass = 0\n",
    "                for p_tag in comment_p_tags:\n",
    "                    try:\n",
    "                        comment_text = p_tag.text.strip()\n",
    "                        if comment_text and comment_text not in unique_comment_texts_scraped:\n",
    "                            unique_comment_texts_scraped.add(comment_text)\n",
    "                            new_comments_this_pass += 1\n",
    "                    except StaleElementReferenceException: continue\n",
    "                if new_comments_this_pass > 0:\n",
    "                    print(f\"    Added {new_comments_this_pass} new unique comments from page.\")\n",
    "                    action_taken_this_loop = True\n",
    "            except Exception as e_find_comments: print(f\"  Error finding comment <p> tags: {e_find_comments}\")\n",
    "\n",
    "\n",
    "            # 4. Click \"展开查看更多\" (Load More Main Comments)\n",
    "            # UPDATED XPATH based on your screenshot:\n",
    "            load_more_comments_xpath = \"//a[@class='show_more' and .//span[text()='展开查看更多']]\"\n",
    "            try:\n",
    "                # Use interaction_wait for this button\n",
    "                load_more_button = interaction_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多' button. Clicking...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", load_more_button); time.sleep(0.4)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                print(\"    Clicked '展开查看更多'.\")\n",
    "                action_taken_this_loop = True\n",
    "                time.sleep(scroll_pause_time + 0.5) # Wait longer after this significant action\n",
    "            except TimeoutException:\n",
    "                print(\"  '展开查看更多' button not found or not clickable this pass (might be all loaded).\")\n",
    "            except Exception as e_load_more:\n",
    "                print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_total_comments = len(unique_comment_texts_scraped)\n",
    "            print(f\"  Loop {i+1} end. Total unique comments: {current_total_comments}. Previously: {last_total_comments}\")\n",
    "\n",
    "            if not action_taken_this_loop and current_total_comments == last_total_comments:\n",
    "                print(\"No actions taken (no buttons clicked, no new comments found) and comment count unchanged. Assuming completion.\")\n",
    "                break\n",
    "            \n",
    "            last_total_comments = current_total_comments\n",
    "            if i == max_main_loops - 1: print(\"Reached max main loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"main_loop_end_{i+1}.png\"))\n",
    "\n",
    "        scraped_data[\"comments\"] = list(unique_comment_texts_scraped)\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_and_all_comments(target_url, max_main_loops=20, scroll_pause_time=2.5) # Increased max_main_loops\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\"); print(data[\"post_content\"])\n",
    "    else: print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment in enumerate(data[\"comments\"]): print(f\"{i+1}. {comment}\")\n",
    "    else: print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30); print(f\"Check console logs and folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51925b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_post_authors_comments' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article page indicators loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post author and content...\n",
      "Post Author: 麻瓜价投 (ID: 5669998349)\n",
      "Post content scraped (length: 627).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Main Loop Iteration #1 ---\n",
      "  Scrolling down...\n",
      "  Found '展开查看更多' button. Clicking...\n",
      "    Clicked '展开查看更多'.\n",
      "  Loop 1 end. Total unique comments in set: 0. Previously: -1\n",
      "--- Main Loop Iteration #2 ---\n",
      "  Scrolling down...\n",
      "  '展开查看更多' button not found or not clickable (might be all loaded).\n",
      "  Loop 2 end. Total unique comments in set: 0. Previously: 0\n",
      "No actions taken and comment count unchanged. Assuming completion.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 0 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "Author: 麻瓜价投 (ID: 5669998349)\n",
      "Content: 转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "零食甚至“宠物饮品”一应俱全\n",
      "。\n",
      "独酌时\n",
      "，\n",
      "希望宠物也能“陪一杯\n",
      "\n",
      "3️⃣ 陪伴型机器人技术不卖功能\n",
      "卖被需要\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT\n",
      "不扫地\n",
      "不聊天\n",
      "不能干活但它会撒娇\n",
      "会跟着你\n",
      "会让你想抱\n",
      "\n",
      "这是一台卖情绪价值的机器人\n",
      "\n",
      ">>> No comments were scraped. <<<\n",
      "\n",
      "==============================\n",
      "Check console logs and folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def scrape_post_comments_with_authors(url, max_main_loops=20, scroll_pause_time=2.5):\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_authors_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_author\": None, \"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    # Store comments as dictionaries: {\"author_name\": \"...\", \"author_id\": \"...\", \"text\": \"...\"}\n",
    "    scraped_data = {\"post_author_name\": None, \"post_author_id\": None, \"post_content\": None, \"comments\": []}\n",
    "    # Use a set of tuples (author_id, comment_text_start) to identify unique comments\n",
    "    # This helps if a user posts the exact same comment twice (rare but possible)\n",
    "    # or if our scraping picks up the same comment multiple times due to DOM re-renders.\n",
    "    unique_comments_tracker = set()\n",
    "\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        interaction_wait = WebDriverWait(driver, 7)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__author')]\"))) # Wait for author block\n",
    "            print(\"Article page indicators loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article page indicators did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "\n",
    "\n",
    "        # Scrape Main Post Author and Content\n",
    "        try:\n",
    "            print(\"Scraping main post author and content...\")\n",
    "            # XPath for author link - LIKELY NEEDS INSPECTION AND ADJUSTMENT\n",
    "            # Look for an <a> tag with class 'name' inside 'article__author' or similar\n",
    "            post_author_link_xpath = \"//div[contains(@class, 'article__author')]//a[contains(@class,'name')]\"\n",
    "            author_link_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_author_link_xpath)))\n",
    "            scraped_data[\"post_author_name\"] = author_link_element.text.strip()\n",
    "            author_href = author_link_element.get_attribute('href')\n",
    "            # Extract ID from href like \"https://xueqiu.com/u/USERID\" or \"/USERID\"\n",
    "            if author_href:\n",
    "                parts = author_href.split('/')\n",
    "                scraped_data[\"post_author_id\"] = parts[-1] if parts[-1] else parts[-2] # Handles trailing slash\n",
    "            print(f\"Post Author: {scraped_data['post_author_name']} (ID: {scraped_data['post_author_id']})\")\n",
    "\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = driver.find_element(By.XPATH, post_content_xpath) # Already waited for author, so content should be there\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post author or content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_total_comments_in_set = -1\n",
    "\n",
    "        for i in range(max_main_loops):\n",
    "            print(f\"--- Main Loop Iteration #{i+1} ---\")\n",
    "            action_taken_this_loop = False\n",
    "            comments_found_before_interactions = len(unique_comments_tracker)\n",
    "\n",
    "            # 1. Click ALL visible \"查看N条回复\"\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            expand_attempts = 0\n",
    "            while expand_attempts < 5:\n",
    "                expand_attempts += 1; clicked_an_expand_button_this_pass = False\n",
    "                try:\n",
    "                    visible_expand_buttons = [b for b in driver.find_elements(By.XPATH, expand_reply_xpath) if b.is_displayed()]\n",
    "                    if not visible_expand_buttons: break\n",
    "                    print(f\"  Found {len(visible_expand_buttons)} visible 'Expand Replies' links.\")\n",
    "                    for button in visible_expand_buttons:\n",
    "                        try:\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", button); time.sleep(0.4)\n",
    "                            button_to_click = WebDriverWait(driver, 3).until(EC.element_to_be_clickable(button))\n",
    "                            driver.execute_script(\"arguments[0].click();\", button_to_click)\n",
    "                            print(f\"    Clicked 'Expand Replies': {button.text[:20]}\"); action_taken_this_loop = True; clicked_an_expand_button_this_pass = True; time.sleep(1.5)\n",
    "                        except (StaleElementReferenceException, TimeoutException, ElementNotInteractableException): continue\n",
    "                        except Exception as e_expand: print(f\"    Error clicking one 'Expand Replies': {e_expand}\")\n",
    "                    if not clicked_an_expand_button_this_pass: break\n",
    "                except Exception as e_find_expand: print(f\"  Error finding 'Expand Replies': {e_find_expand}\"); break\n",
    "\n",
    "            # 2. Scroll down\n",
    "            print(\"  Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            # 3. Attempt to scrape comments with authors\n",
    "            # Each comment item is likely a div, e.g., class=\"comment_item\" (needs inspection)\n",
    "            # Relative to this item, find author and text.\n",
    "            # Based on previous screenshot: comments are within <div class=\"comment__list\"> which contains <div class=\"comment_item\">\n",
    "            # And each comment_item has <div class=\"comment__item__main\"> for the actual content.\n",
    "            comment_item_xpath = \"//div[@class='comment__list']/div[contains(@class,'comment_item')]\" # XPath for each comment's main container\n",
    "            try:\n",
    "                comment_items = driver.find_elements(By.XPATH, comment_item_xpath)\n",
    "                new_comments_this_pass = 0\n",
    "                if comment_items:\n",
    "                    print(f\"  Found {len(comment_items)} potential comment item containers.\")\n",
    "                for item_div in comment_items:\n",
    "                    try:\n",
    "                        # XPath relative to item_div to find author name/link and comment text\n",
    "                        # LIKELY NEEDS INSPECTION AND ADJUSTMENT\n",
    "                        author_name_element = item_div.find_element(By.XPATH, \".//div[@class='comment__item__main']//a[contains(@class,'name')]\")\n",
    "                        comment_author_name = author_name_element.text.strip()\n",
    "                        author_href = author_name_element.get_attribute('href')\n",
    "                        comment_author_id = None\n",
    "                        if author_href:\n",
    "                            parts = author_href.split('/')\n",
    "                            comment_author_id = parts[-1] if parts[-1] else parts[-2]\n",
    "\n",
    "                        comment_text_element = item_div.find_element(By.XPATH, \".//div[@class='comment__item__main']/p\")\n",
    "                        comment_text = comment_text_element.text.strip()\n",
    "\n",
    "                        # Use a tuple of (author_id, first ~50 chars of text) for uniqueness tracking\n",
    "                        # This prevents adding the same comment if it's re-rendered or slightly changes non-text attributes\n",
    "                        comment_signature = (comment_author_id, comment_text[:50])\n",
    "\n",
    "                        if comment_text and comment_signature not in unique_comments_tracker:\n",
    "                            unique_comments_tracker.add(comment_signature)\n",
    "                            scraped_data[\"comments\"].append({\n",
    "                                \"author_name\": comment_author_name,\n",
    "                                \"author_id\": comment_author_id,\n",
    "                                \"text\": comment_text\n",
    "                            })\n",
    "                            new_comments_this_pass += 1\n",
    "                    except NoSuchElementException:\n",
    "                        # This means the structure within a comment_item was not as expected.\n",
    "                        # print(\"    Could not find author/text structure in a comment item.\")\n",
    "                        continue\n",
    "                    except StaleElementReferenceException:\n",
    "                        # print(\"    Stale element within comment item, skipping.\")\n",
    "                        continue\n",
    "                if new_comments_this_pass > 0:\n",
    "                    print(f\"    Added {new_comments_this_pass} new unique comments with author info.\")\n",
    "                    action_taken_this_loop = True\n",
    "            except Exception as e_find_comments: print(f\"  Error finding comment items: {e_find_comments}\")\n",
    "\n",
    "            # 4. Click \"展开查看更多\" (Load More Main Comments)\n",
    "            load_more_comments_xpath = \"//a[@class='show_more' and .//span[text()='展开查看更多']]\"\n",
    "            try:\n",
    "                load_more_button = interaction_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多' button. Clicking...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", load_more_button); time.sleep(0.4)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                print(\"    Clicked '展开查看更多'.\"); action_taken_this_loop = True; time.sleep(scroll_pause_time + 0.5)\n",
    "            except TimeoutException: print(\"  '展开查看更多' button not found or not clickable (might be all loaded).\")\n",
    "            except Exception as e_load_more: print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            # 5. Check for loop termination\n",
    "            current_total_comments_in_set = len(unique_comments_tracker)\n",
    "            print(f\"  Loop {i+1} end. Total unique comments in set: {current_total_comments_in_set}. Previously: {last_total_comments_in_set}\")\n",
    "            if not action_taken_this_loop and current_total_comments_in_set == last_total_comments_in_set:\n",
    "                print(\"No actions taken and comment count unchanged. Assuming completion.\"); break\n",
    "            last_total_comments_in_set = current_total_comments_in_set\n",
    "            if i == max_main_loops - 1: print(\"Reached max main loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"main_loop_end_{i+1}.png\"))\n",
    "\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\"); print(f\"Error Type: {type(e).__name__}\"); print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try: error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\"); driver.save_screenshot(error_ss_path); print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit: print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_comments_with_authors(target_url, max_main_loops=20, scroll_pause_time=2.5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\")\n",
    "        print(f\"Author: {data.get('post_author_name')} (ID: {data.get('post_author_id')})\")\n",
    "        print(f\"Content: {data['post_content']}\")\n",
    "    else: print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment_data in enumerate(data[\"comments\"]):\n",
    "            print(f\"{i+1}. Author: {comment_data['author_name']} (ID: {comment_data['author_id']})\")\n",
    "            print(f\"   Comment: {comment_data['text']}\")\n",
    "    else: print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30); print(f\"Check console logs and folder for details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
