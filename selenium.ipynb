{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ef4fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting urllib3<3,>=1.26 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Using cached trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2021.10.8 (from selenium)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting typing_extensions~=4.9 (from selenium)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting idna (from trio~=0.17->selenium)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Using cached selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl (182 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, urllib3, typing_extensions, sniffio, pysocks, pycparser, idna, h11, certifi, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-25.3.0 certifi-2025.4.26 cffi-1.17.1 h11-0.16.0 idna-3.10 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.32.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 urllib3-2.4.0 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://selenium.dev/')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99d2d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Looking for the first pop-up ('跳过')...\n",
      "First pop-up ('跳过') did not appear or was not found within timeout.\n",
      "Taking screenshot before handling second popup...\n",
      "Screenshot saved to screenshots/debug_before_second_popup.png\n",
      "Looking for the second pop-up (close button 'X')...\n",
      "Checking for known iframe patterns...\n",
      "No iframe found matching pattern, or 'X' button not found inside iframe. Proceeding to check main page.\n",
      "Looking for close button on the main page...\n",
      "Trying XPath: //div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\n",
      "Close button with XPath '//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]' not found or not clickable yet.\n",
      "Trying XPath: //div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]\n",
      "Close button with XPath '//div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]' not found or not clickable yet.\n",
      "Trying XPath: //i[contains(@class, 'cube-dialog-close')]\n",
      "Close button with XPath '//i[contains(@class, 'cube-dialog-close')]' not found or not clickable yet.\n",
      "Trying XPath: //div[contains(@class, 'modal-container')]//i[contains(@class, 'close')]\n",
      "Close button with XPath '//div[contains(@class, 'modal-container')]//i[contains(@class, 'close')]' not found or not clickable yet.\n",
      "Trying XPath: //div[contains(@class, 'dialog')]//i[contains(@class, 'close')]\n",
      "Close button with XPath '//div[contains(@class, 'dialog')]//i[contains(@class, 'close')]' not found or not clickable yet.\n",
      "Trying XPath: //button[@aria-label='Close']\n",
      "Close button with XPath '//button[@aria-label='Close']' not found or not clickable yet.\n",
      "Trying XPath: //button[contains(@class, 'close')]\n",
      "Close button with XPath '//button[contains(@class, 'close')]' not found or not clickable yet.\n",
      "Second pop-up (close 'X') did not appear or was not found with any attempted XPaths on main page.\n",
      "Scraping main post content...\n",
      "Main post content scraped.\n",
      "Looking for and clicking the '评论' (Comments) tab...\n",
      "Comments tab not found or not clickable.\n",
      "Scraping comments...\n",
      "Comments container or first comment did not load/appear within timeout.\n",
      "Closing the browser.\n",
      "\n",
      "--- Scraped Data ---\n",
      "\n",
      "Main Post:\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "零食甚至“宠物饮品”一应俱全\n",
      "。\n",
      "独酌时\n",
      "，\n",
      "希望宠物也能“陪一杯”\n",
      "。\n",
      "\n",
      "3️⃣ 陪伴型机器人：技术不卖功能\n",
      "，\n",
      "卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT\n",
      "，\n",
      "不扫地\n",
      "、\n",
      "不聊天\n",
      "、\n",
      "不能干活——但它会“撒娇”\n",
      "、\n",
      "会“跟着你”\n",
      "、\n",
      "会“让你想抱”\n",
      "。\n",
      "\n",
      "这是一台卖情绪价值的机器人\n",
      "。\n",
      "\n",
      "$恒生指数(HKHSI)$ $上证指数(SH000001)$ $招商银行(SH600036)$\n",
      "\n",
      "Comments:\n",
      "No comments found or scraping failed.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "import time\n",
    "import os # For screenshots\n",
    "\n",
    "def scrape_xueqiu_post_and_comments(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\") # Keep headless commented out for debugging popups\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\") # Add a common user agent\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    # Create a directory for screenshots if it doesn't exist\n",
    "    if not os.path.exists(\"screenshots\"):\n",
    "        os.makedirs(\"screenshots\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    scraped_data = {\"main_post\": None, \"comments\": []}\n",
    "    wait = WebDriverWait(driver, 15) # Adjust wait time if needed (15s is usually enough)\n",
    "\n",
    "    try:\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # --- Handle First Pop-up (Login - \"跳过\") ---\n",
    "        try:\n",
    "            print(\"Looking for the first pop-up ('跳过')...\")\n",
    "            skip_button_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"\n",
    "            skip_button = wait.until(EC.element_to_be_clickable((By.XPATH, skip_button_xpath)))\n",
    "            print(\"First pop-up '跳过' button found. Attempting to click...\")\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button)\n",
    "            print(\"Clicked '跳过'.\")\n",
    "            time.sleep(1) # Short pause after click\n",
    "        except TimeoutException:\n",
    "            print(\"First pop-up ('跳过') did not appear or was not found within timeout.\")\n",
    "            driver.save_screenshot(\"screenshots/debug_no_first_popup.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error handling first pop-up: {e}\")\n",
    "            driver.save_screenshot(\"screenshots/debug_error_first_popup.png\")\n",
    "\n",
    "        # --- Add Pause for Manual Inspection ---\n",
    "        # print(\">>> Pausing for 45 seconds for manual inspection of potential second pop-up... <<<\")\n",
    "        # print(\">>> Please inspect the 'X' button and check for iframes. Copy its XPath/Selector. <<<\")\n",
    "        # time.sleep(45)\n",
    "        # print(\">>> Resuming script... <<<\")\n",
    "        # --- OR Take screenshot for headless debugging ---\n",
    "        print(\"Taking screenshot before handling second popup...\")\n",
    "        driver.save_screenshot(\"screenshots/debug_before_second_popup.png\")\n",
    "        print(\"Screenshot saved to screenshots/debug_before_second_popup.png\")\n",
    "\n",
    "\n",
    "        # --- Handle Second Pop-up (Close \"X\") ---\n",
    "        close_button_found = False\n",
    "        try:\n",
    "            print(\"Looking for the second pop-up (close button 'X')...\")\n",
    "\n",
    "            # === OPTION A: Check for iframe first (if suspected) ===\n",
    "            iframe_xpath = \"//iframe[contains(@id, 'dialog') or contains(@class, 'modal-iframe')]\" # Example iframe locator - ADJUST THIS\n",
    "            try:\n",
    "                print(\"Checking for known iframe patterns...\")\n",
    "                iframe = wait.until(EC.presence_of_element_located((By.XPATH, iframe_xpath)))\n",
    "                print(\"Potential iframe found. Switching to iframe...\")\n",
    "                driver.switch_to.frame(iframe)\n",
    "                print(\"Successfully switched to iframe. Now looking for close button inside iframe...\")\n",
    "\n",
    "                # --- Find 'X' button INSIDE iframe ---\n",
    "                # Adjust this XPath based on inspection *inside* the iframe\n",
    "                iframe_close_button_xpath = \"//i[contains(@class, 'close')] | //button[contains(@aria-label, 'Close') or contains(text(), '关闭')]\"\n",
    "                close_button = wait.until(EC.element_to_be_clickable((By.XPATH, iframe_close_button_xpath)))\n",
    "                print(f\"Second pop-up 'X' button found INSIDE IFRAME. Clicking...\")\n",
    "                driver.execute_script(\"arguments[0].click();\", close_button)\n",
    "                print(\"Clicked 'X' inside iframe.\")\n",
    "                close_button_found = True\n",
    "                time.sleep(1)\n",
    "                print(\"Switching back to default content.\")\n",
    "                driver.switch_to.default_content() # IMPORTANT: Switch back\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(\"No iframe found matching pattern, or 'X' button not found inside iframe. Proceeding to check main page.\")\n",
    "                # If iframe was found but button wasn't, need to switch back\n",
    "                try:\n",
    "                    driver.switch_to.default_content()\n",
    "                except: pass # Already in default if iframe wasn't found\n",
    "\n",
    "            except Exception as e_iframe:\n",
    "                 print(f\"Error during iframe handling: {e_iframe}\")\n",
    "                 try:\n",
    "                     driver.switch_to.default_content() # Ensure switch back on error\n",
    "                 except: pass\n",
    "\n",
    "\n",
    "            # === OPTION B: Look for 'X' on the main page (if not found in iframe or no iframe suspected) ===\n",
    "            if not close_button_found:\n",
    "                print(\"Looking for close button on the main page...\")\n",
    "                # ADD THE XPATH YOU FOUND DURING MANUAL INSPECTION HERE!\n",
    "                # Example: close_button_xpaths = [\"//button[@aria-label='Close Dialog']\", ...]\n",
    "                close_button_xpaths = [\n",
    "                    # Add your best guess from inspection first:\n",
    "                    \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", # Common Xueqiu pattern\n",
    "                    \"//div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]\", # Another dialog pattern\n",
    "                    \"//i[contains(@class, 'cube-dialog-close')]\", # Seen on some Xueqiu popups\n",
    "                    \"//div[contains(@class, 'modal-container')]//i[contains(@class, 'close')]\",\n",
    "                    \"//div[contains(@class, 'dialog')]//i[contains(@class, 'close')]\",\n",
    "                    \"//button[@aria-label='Close']\", # Common accessibility pattern\n",
    "                    \"//button[contains(@class, 'close')]\"\n",
    "                ]\n",
    "                for xpath in close_button_xpaths:\n",
    "                    try:\n",
    "                        print(f\"Trying XPath: {xpath}\")\n",
    "                        close_button = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "                        print(f\"Second pop-up 'X' button found with XPath: {xpath}. Clicking...\")\n",
    "                        driver.execute_script(\"arguments[0].click();\", close_button)\n",
    "                        print(\"Clicked 'X' on the second pop-up (main page).\")\n",
    "                        close_button_found = True\n",
    "                        time.sleep(1) # Pause after click\n",
    "                        break # Exit loop once found and clicked\n",
    "                    except TimeoutException:\n",
    "                        print(f\"Close button with XPath '{xpath}' not found or not clickable yet.\")\n",
    "                        continue # Try next XPath\n",
    "                    except ElementClickInterceptedException:\n",
    "                         print(f\"Close button found ({xpath}) but click was intercepted. Trying JS click again forcefully.\")\n",
    "                         try:\n",
    "                             driver.execute_script(\"arguments[0].click();\", close_button)\n",
    "                             print(\"Clicked 'X' via JS after interception.\")\n",
    "                             close_button_found = True\n",
    "                             time.sleep(1)\n",
    "                             break\n",
    "                         except Exception as e_intercept:\n",
    "                             print(f\"Force JS click also failed: {e_intercept}\")\n",
    "                             driver.save_screenshot(f\"screenshots/debug_click_intercepted_{xpath.replace('/','_').replace('@','')}.png\")\n",
    "                             continue # Try next XPath\n",
    "                    except Exception as e_click:\n",
    "                         print(f\"Error clicking button with XPath '{xpath}': {e_click}\")\n",
    "                         driver.save_screenshot(f\"screenshots/debug_click_error_{xpath.replace('/','_').replace('@','')}.png\")\n",
    "                         continue # Try next XPath\n",
    "\n",
    "\n",
    "                if not close_button_found:\n",
    "                     print(\"Second pop-up (close 'X') did not appear or was not found with any attempted XPaths on main page.\")\n",
    "                     driver.save_screenshot(\"screenshots/debug_no_second_popup_found.png\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the second pop-up handling phase: {e}\")\n",
    "            driver.save_screenshot(\"screenshots/debug_error_second_popup_phase.png\")\n",
    "\n",
    "\n",
    "        # --- Scrape Main Post Content ---\n",
    "        # (Wait longer if popups might have delayed page load)\n",
    "        content_wait = WebDriverWait(driver, 20)\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_element = content_wait.until(EC.visibility_of_element_located((By.XPATH, \"//div[contains(@class, 'article__content') or contains(@class, 'article__bd__detail')]\")))\n",
    "            scraped_data[\"main_post\"] = post_content_element.text\n",
    "            print(\"Main post content scraped.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Main post content not found.\")\n",
    "            driver.save_screenshot(\"screenshots/debug_no_main_post.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping main post: {e}\")\n",
    "            driver.save_screenshot(\"screenshots/debug_error_main_post.png\")\n",
    "\n",
    "        # --- Click on Comments Tab ---\n",
    "        try:\n",
    "            print(\"Looking for and clicking the '评论' (Comments) tab...\")\n",
    "            comments_tab_xpath = \"//div[contains(@class, 'tab__item') and .//span[text()='评论']] | //div[contains(@class, 'action-bar__item') and contains(., '评论')] | //div[text()='评论' and contains(@class,'tab__control')]\"\n",
    "            comments_tab = wait.until(EC.element_to_be_clickable((By.XPATH, comments_tab_xpath)))\n",
    "            print(\"Comments tab found. Clicking...\")\n",
    "            # Scroll into view before clicking, sometimes helps\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", comments_tab)\n",
    "            time.sleep(0.5)\n",
    "            driver.execute_script(\"arguments[0].click();\", comments_tab)\n",
    "            print(\"Clicked '评论' tab.\")\n",
    "            time.sleep(3) # Wait for comments section to potentially load/animate\n",
    "        except TimeoutException:\n",
    "            print(\"Comments tab not found or not clickable.\")\n",
    "            driver.save_screenshot(\"screenshots/debug_no_comment_tab.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking comments tab: {e}\")\n",
    "            driver.save_screenshot(\"screenshots/debug_error_comment_tab.png\")\n",
    "\n",
    "\n",
    "        # --- Scrape Comments ---\n",
    "        try:\n",
    "            print(\"Scraping comments...\")\n",
    "            # Wait for the container of comments to be present\n",
    "            comments_container_xpath = \"//div[contains(@class, 'reply-list')]\"\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, comments_container_xpath)))\n",
    "            print(\"Comments container found.\")\n",
    "\n",
    "            # Wait for at least one comment text element to be visible\n",
    "            comment_text_xpath = \"//div[contains(@class, 'reply-item__main')]//div[contains(@class, 'reply-item__content')]\"\n",
    "            wait.until(EC.visibility_of_element_located((By.XPATH, f\"({comment_text_xpath})[1]\"))) # Wait for the first one\n",
    "            print(\"First comment text element visible.\")\n",
    "\n",
    "            # Find all comment text elements\n",
    "            comment_elements = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "\n",
    "            if comment_elements:\n",
    "                print(f\"Found {len(comment_elements)} potential comment elements.\")\n",
    "                for i, comment_el in enumerate(comment_elements):\n",
    "                    try:\n",
    "                        comment_text = comment_el.text.strip()\n",
    "                        if comment_text:\n",
    "                            scraped_data[\"comments\"].append(comment_text)\n",
    "                        # else: # Optional: log empty elements if debugging selector issues\n",
    "                        #     print(f\"Comment element {i+1} had empty text.\")\n",
    "                    except Exception as e_inner:\n",
    "                        print(f\"Could not extract text from comment element {i+1}: {e_inner}\")\n",
    "                print(f\"Successfully scraped {len(scraped_data['comments'])} comments.\")\n",
    "            else:\n",
    "                print(\"No comment text elements found with the specified XPath, although container was present.\")\n",
    "                driver.save_screenshot(\"screenshots/debug_no_comment_elements.png\")\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Comments container or first comment did not load/appear within timeout.\")\n",
    "            driver.save_screenshot(\"screenshots/debug_timeout_comments.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping comments: {e}\")\n",
    "            driver.save_screenshot(\"screenshots/debug_error_scraping_comments.png\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An critical error occurred: {e}\")\n",
    "        driver.save_screenshot(\"screenshots/debug_critical_error.png\")\n",
    "    finally:\n",
    "        print(\"Closing the browser.\")\n",
    "        #if driver:\n",
    "            #driver.quit()\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    data = scrape_xueqiu_post_and_comments(target_url)\n",
    "\n",
    "    print(\"\\n--- Scraped Data ---\")\n",
    "    print(\"\\nMain Post:\")\n",
    "    print(data[\"main_post\"] if data[\"main_post\"] else \"Not found or scraping failed.\")\n",
    "    print(\"\\nComments:\")\n",
    "    if data[\"comments\"]:\n",
    "        for i, comment in enumerate(data[\"comments\"]):\n",
    "            print(f\"{i+1}. {comment}\")\n",
    "    else:\n",
    "        print(\"No comments found or scraping failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7ff046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Xueqiu Debug Scroll/Screenshot Script ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_debug_scroll' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Looking for the first pop-up ('跳过')...\n",
      "First pop-up ('跳过') not found or timed out.\n",
      "Looking for the second pop-up ('X')...\n",
      "Second pop-up ('X') not found or timed out.\n",
      "Looking for the '评论' (Comments) tab...\n",
      "Error finding or clicking '评论' tab: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7E128A145+76773]\n",
      "\tGetHandleVerifier [0x00007FF7E128A1A0+76864]\n",
      "\t(No symbol) [0x00007FF7E1048F7A]\n",
      "\t(No symbol) [0x00007FF7E109F496]\n",
      "\t(No symbol) [0x00007FF7E109F74C]\n",
      "\t(No symbol) [0x00007FF7E10F2287]\n",
      "\t(No symbol) [0x00007FF7E10C739F]\n",
      "\t(No symbol) [0x00007FF7E10EF0CF]\n",
      "\t(No symbol) [0x00007FF7E10C7133]\n",
      "\t(No symbol) [0x00007FF7E10904D1]\n",
      "\t(No symbol) [0x00007FF7E1091263]\n",
      "\tGetHandleVerifier [0x00007FF7E154A8ED+2962317]\n",
      "\tGetHandleVerifier [0x00007FF7E1544EC2+2939234]\n",
      "\tGetHandleVerifier [0x00007FF7E1562FF3+3062419]\n",
      "\tGetHandleVerifier [0x00007FF7E12A4B9A+185914]\n",
      "\tGetHandleVerifier [0x00007FF7E12AC78F+217647]\n",
      "\tGetHandleVerifier [0x00007FF7E1292A44+111844]\n",
      "\tGetHandleVerifier [0x00007FF7E1292BF2+112274]\n",
      "\tGetHandleVerifier [0x00007FF7E1278A79+5401]\n",
      "\tBaseThreadInitThunk [0x00007FF816A1259D+29]\n",
      "\tRtlUserThreadStart [0x00007FF81812AF38+40]\n",
      "\n",
      "Skipping interaction loop because '评论' tab was not clicked.\n",
      "\n",
      "--- Taking final screenshot ---\n",
      "Saved final screenshot: screenshots_debug_scroll\\final_state.png\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "--- Debug Script Finished ---\n",
      "Check the 'screenshots_debug_scroll' folder.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def debug_scroll_and_screenshot(url, max_scroll_attempts=10):\n",
    "    \"\"\"\n",
    "    Loads Xueqiu page, handles popups, clicks comments, then repeatedly\n",
    "    scrolls, expands replies, clicks 'load more', taking screenshots.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the Xueqiu post.\n",
    "        max_scroll_attempts (int): Max number of scroll/interact cycles.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Keep browser visible for debugging\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1200,900\") # Moderate size\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_debug_scroll\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try:\n",
    "            os.makedirs(screenshot_dir)\n",
    "            print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating screenshot directory: {e}\")\n",
    "            return # Stop if we can't save screenshots\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        # Use shorter waits for finding elements to click in the loop\n",
    "        wait = WebDriverWait(driver, 15) # For initial setup\n",
    "        short_wait = WebDriverWait(driver, 3) # For finding dynamic elements in loop\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # --- Handle Initial Pop-ups (Reusing previous robust logic) ---\n",
    "        # First Pop-up (\"跳过\")\n",
    "        try:\n",
    "            print(\"Looking for the first pop-up ('跳过')...\")\n",
    "            skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"\n",
    "            skip_button = wait.until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            print(\"Clicking '跳过'...\")\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button)\n",
    "            time.sleep(0.5)\n",
    "        except TimeoutException: print(\"First pop-up ('跳过') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error handling first pop-up: {e}\")\n",
    "\n",
    "        # Second Pop-up (\"X\")\n",
    "        try:\n",
    "            print(\"Looking for the second pop-up ('X')...\")\n",
    "            close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]\", \"//i[contains(@class, 'cube-dialog-close')]\", \"//div[contains(@class, 'Modal_modal')]//i[contains(@class, 'Modal_closeIcon')]\", \"//button[@aria-label='Close']\" ]\n",
    "            close_button_found = False\n",
    "            for xpath in close_xpaths:\n",
    "                 try:\n",
    "                     close_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "                     print(f\"Clicking second pop-up 'X' (XPath: {xpath})...\")\n",
    "                     driver.execute_script(\"arguments[0].click();\", close_button)\n",
    "                     close_button_found = True\n",
    "                     time.sleep(0.5)\n",
    "                     break\n",
    "                 except TimeoutException: continue\n",
    "                 except Exception: continue # Ignore other errors for this debug script\n",
    "            if not close_button_found: print(\"Second pop-up ('X') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error during second pop-up handling: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # --- Click on Comments Tab ---\n",
    "        comments_tab_clicked = False\n",
    "        try:\n",
    "            print(\"Looking for the '评论' (Comments) tab...\")\n",
    "            tab_xpath = \"//div[contains(@class, 'tabs__item') and .//span[text()='评论']] | //div[contains(@class, 'action-bar__item') and contains(., '评论')]\"\n",
    "            comments_tab = wait.until(EC.element_to_be_clickable((By.XPATH, tab_xpath)))\n",
    "            print(\"Scrolling '评论' tab into view and clicking...\")\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', inline: 'nearest'});\", comments_tab)\n",
    "            time.sleep(0.3)\n",
    "            driver.execute_script(\"arguments[0].click();\", comments_tab)\n",
    "            print(\"Clicked '评论' tab.\")\n",
    "            comments_tab_clicked = True\n",
    "            time.sleep(1.5) # Wait for initial comments section to load\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding or clicking '评论' tab: {e}\")\n",
    "            if driver: driver.save_screenshot(os.path.join(screenshot_dir, \"error_clicking_comments_tab.png\"))\n",
    "\n",
    "        # --- Main Interaction Loop ---\n",
    "        if comments_tab_clicked:\n",
    "            print(\"\\n--- Starting Scroll and Interaction Loop ---\")\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            for i in range(max_scroll_attempts):\n",
    "                print(f\"\\n--- Iteration {i+1}/{max_scroll_attempts} ---\")\n",
    "                actions_taken_this_iteration = False\n",
    "\n",
    "                # 1. Take Screenshot Before Interaction\n",
    "                screenshot_path = os.path.join(screenshot_dir, f\"scroll_iter_{i+1}_before.png\")\n",
    "                try:\n",
    "                    driver.save_screenshot(screenshot_path)\n",
    "                    print(f\"Saved screenshot: {screenshot_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to save screenshot {screenshot_path}: {e}\")\n",
    "\n",
    "                # 2. Find and Click \"Expand Replies\" (查看N条回复)\n",
    "                # Use XPath that finds links/divs containing '查看' and '回复'\n",
    "                # Make sure they are visible/interactable\n",
    "                expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '回复')] | //div[contains(@class, 'reply-expander') and contains(., '查看') and contains(., '回复')]\"\n",
    "                try:\n",
    "                    expand_buttons = driver.find_elements(By.XPATH, expand_reply_xpath)\n",
    "                    print(f\"Found {len(expand_buttons)} potential 'Expand Replies' links/buttons.\")\n",
    "                    clicked_count = 0\n",
    "                    for j, button in enumerate(expand_buttons):\n",
    "                        try:\n",
    "                            # Check if it's visible before trying to click\n",
    "                            if button.is_displayed():\n",
    "                                print(f\"Attempting to click 'Expand Replies' #{j+1}...\")\n",
    "                                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button) # Scroll to button\n",
    "                                time.sleep(0.2)\n",
    "                                driver.execute_script(\"arguments[0].click();\", button)\n",
    "                                print(\"Clicked.\")\n",
    "                                actions_taken_this_iteration = True\n",
    "                                clicked_count += 1\n",
    "                                time.sleep(0.8) # Wait for replies to potentially load\n",
    "                            # else: # Debugging\n",
    "                            #    print(f\"Button #{j+1} found but not displayed.\")\n",
    "                        except (StaleElementReferenceException):\n",
    "                            print(\"Stale element reference for 'Expand Replies', likely disappeared after previous click. Skipping.\")\n",
    "                            continue # Element might have been removed from DOM after a previous click\n",
    "                        except (ElementClickInterceptedException, ElementNotInteractableException):\n",
    "                             print(\"Could not click 'Expand Replies' (intercepted or not interactable). Trying JS click again.\")\n",
    "                             try:\n",
    "                                 driver.execute_script(\"arguments[0].click();\", button)\n",
    "                                 print(\"Clicked via JS retry.\")\n",
    "                                 actions_taken_this_iteration = True\n",
    "                                 clicked_count += 1\n",
    "                                 time.sleep(0.8)\n",
    "                             except Exception as e_retry:\n",
    "                                 print(f\"JS retry failed: {e_retry}. Skipping this button.\")\n",
    "                        except Exception as e_click_reply:\n",
    "                            print(f\"Error clicking 'Expand Replies' #{j+1}: {e_click_reply}\")\n",
    "                    if clicked_count > 0:\n",
    "                         print(f\"Clicked {clicked_count} 'Expand Replies' buttons.\")\n",
    "                         # Take screenshot after expanding\n",
    "                         ss_path_expand = os.path.join(screenshot_dir, f\"scroll_iter_{i+1}_after_expand.png\")\n",
    "                         driver.save_screenshot(ss_path_expand)\n",
    "                         print(f\"Saved screenshot: {ss_path_expand}\")\n",
    "\n",
    "                except Exception as e_find_reply:\n",
    "                    print(f\"Error finding 'Expand Replies' buttons: {e_find_reply}\")\n",
    "\n",
    "\n",
    "                # 3. Scroll Down\n",
    "                print(\"Scrolling down...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.5) # Wait for page to potentially load new items after scroll\n",
    "\n",
    "                # 4. Find and Click \"Load More Comments\" (展开查看更多 / 加载更多)\n",
    "                # Use XPath targeting text, check for visibility/clickability\n",
    "                load_more_xpath = \"//*[contains(text(), '展开查看更多') or contains(text(), '加载更多')]\"\n",
    "                try:\n",
    "                    # Use short wait as it might appear after scrolling\n",
    "                    load_more_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_xpath)))\n",
    "                    print(\"Found 'Load More' button. Attempting to click...\")\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button) # Scroll to it\n",
    "                    time.sleep(0.3)\n",
    "                    driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                    print(\"Clicked 'Load More'.\")\n",
    "                    actions_taken_this_iteration = True\n",
    "                    time.sleep(2.5) # Wait longer after clicking load more\n",
    "                    # Take screenshot after load more\n",
    "                    ss_path_load = os.path.join(screenshot_dir, f\"scroll_iter_{i+1}_after_load_more.png\")\n",
    "                    driver.save_screenshot(ss_path_load)\n",
    "                    print(f\"Saved screenshot: {ss_path_load}\")\n",
    "\n",
    "                except TimeoutException:\n",
    "                    print(\"'Load More' button not found or not clickable.\")\n",
    "                except (ElementClickInterceptedException, ElementNotInteractableException):\n",
    "                    print(\"Could not click 'Load More' (intercepted or not interactable).\") # Don't retry here for simplicity\n",
    "                except Exception as e_load_more:\n",
    "                    print(f\"Error finding/clicking 'Load More': {e_load_more}\")\n",
    "\n",
    "                # 5. Check for End Condition (Simplified)\n",
    "                current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                print(f\"Current height: {current_height}, Last height: {last_height}\")\n",
    "                if not actions_taken_this_iteration and current_height == last_height:\n",
    "                    print(\"No actions taken and scroll height did not change. Ending loop.\")\n",
    "                    break\n",
    "                last_height = current_height\n",
    "            else:\n",
    "                print(f\"\\n--- Reached max iterations ({max_scroll_attempts}) ---\")\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping interaction loop because '评论' tab was not clicked.\")\n",
    "\n",
    "        print(\"\\n--- Taking final screenshot ---\")\n",
    "        final_ss_path = os.path.join(screenshot_dir, \"final_state.png\")\n",
    "        driver.save_screenshot(final_ss_path)\n",
    "        print(f\"Saved final screenshot: {final_ss_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred during the debug process ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved error screenshot: {error_ss_path}\")\n",
    "            except Exception as e_ss:\n",
    "                 print(f\"Could not save error screenshot: {e_ss}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing the browser...\")\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "        else:\n",
    "            print(\"Driver was not initialized.\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(\"--- Starting Xueqiu Debug Scroll/Screenshot Script ---\")\n",
    "    debug_scroll_and_screenshot(target_url, max_scroll_attempts=8) # Adjust max attempts as needed\n",
    "    print(\"--- Debug Script Finished ---\")\n",
    "    print(f\"Check the 'screenshots_debug_scroll' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3577d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Xueqiu Debug Scroll/Screenshot Script ---\n",
      "Setting up WebDriver...\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Looking for the first pop-up ('跳过')...\n",
      "First pop-up ('跳过') not found or timed out.\n",
      "Looking for the second pop-up ('X')...\n",
      "Second pop-up ('X') not found or timed out.\n",
      "Looking for the '评论' (Comments) tab...\n",
      "Using Tab XPath: //span[text()='评论' and contains(@class,'tabs__item__title')]/ancestor::div[contains(@class,'tabs__item')] | //div[contains(@class, 'tabs__item') and .//span[text()='评论']] | //div[contains(@class, 'action-bar__item') and contains(., '评论')]\n",
      "Waiting for tab presence...\n",
      "TimeoutException finding or clicking '评论' tab: \n",
      "Saved error screenshot: screenshots_debug_scroll\\error_timeout_comments_tab.png\n",
      "Skipping interaction loop because '评论' tab was not successfully clicked.\n",
      "\n",
      "--- Taking final screenshot ---\n",
      "Saved final screenshot: screenshots_debug_scroll\\final_state.png\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "--- Debug Script Finished ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'screenshot_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    178\u001b[39m debug_scroll_and_screenshot(target_url, max_scroll_attempts=\u001b[32m8\u001b[39m)\n\u001b[32m    179\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Debug Script Finished ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheck the \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mscreenshot_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m folder.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'screenshot_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def debug_scroll_and_screenshot(url, max_scroll_attempts=10):\n",
    "    \"\"\"\n",
    "    Loads Xueqiu page, handles popups, clicks comments, then repeatedly\n",
    "    scrolls, expands replies, clicks 'load more', taking screenshots.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the Xueqiu post.\n",
    "        max_scroll_attempts (int): Max number of scroll/interact cycles.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Keep browser visible for debugging\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_debug_scroll\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        # INCREASED Wait time for critical elements like the tab\n",
    "        wait = WebDriverWait(driver, 25) # Increased to 25 seconds\n",
    "        short_wait = WebDriverWait(driver, 5) # For loop interactions\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(2) # Add a small static pause after initial load\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for the first pop-up ('跳过')...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            print(\"Clicking '跳过'...\"); driver.execute_script(\"arguments[0].click();\", skip_button); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"First pop-up ('跳过') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error handling first pop-up: {e}\")\n",
    "        try:\n",
    "            print(\"Looking for the second pop-up ('X')...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]\", \"//i[contains(@class, 'cube-dialog-close')]\", \"//div[contains(@class, 'Modal_modal')]//i[contains(@class, 'Modal_closeIcon')]\", \"//button[@aria-label='Close']\" ]\n",
    "            close_button_found = False\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, xpath))); print(f\"Clicking second pop-up 'X' (XPath: {xpath})...\"); driver.execute_script(\"arguments[0].click();\", close_button); close_button_found = True; time.sleep(0.5); break\n",
    "                 except TimeoutException: continue\n",
    "                 except Exception: continue\n",
    "            if not close_button_found: print(\"Second pop-up ('X') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error during second pop-up handling: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # --- Click on Comments Tab ---\n",
    "        comments_tab_clicked = False\n",
    "        try:\n",
    "            print(\"Looking for the '评论' (Comments) tab...\")\n",
    "            tab_xpath = \"//span[text()='评论' and contains(@class,'tabs__item__title')]/ancestor::div[contains(@class,'tabs__item')] | //div[contains(@class, 'tabs__item') and .//span[text()='评论']] | //div[contains(@class, 'action-bar__item') and contains(., '评论')]\"\n",
    "            print(f\"Using Tab XPath: {tab_xpath}\")\n",
    "\n",
    "            print(\"Waiting for tab presence...\")\n",
    "            comments_tab_element = wait.until(EC.presence_of_element_located((By.XPATH, tab_xpath)))\n",
    "            print(\"Tab present. Scrolling tab into view...\")\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', inline: 'nearest'});\", comments_tab_element)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            print(\"Waiting for tab to be clickable...\")\n",
    "            comments_tab_clickable = wait.until(EC.element_to_be_clickable((By.XPATH, tab_xpath)))\n",
    "            print(\"Tab clickable. Clicking '评论' tab...\")\n",
    "            driver.execute_script(\"arguments[0].click();\", comments_tab_clickable)\n",
    "            print(\"Clicked '评论' tab.\")\n",
    "            comments_tab_clicked = True\n",
    "            time.sleep(2.0)\n",
    "\n",
    "        except TimeoutException as e_timeout:\n",
    "             print(f\"TimeoutException finding or clicking '评论' tab: {e_timeout.msg}\")\n",
    "             if driver:\n",
    "                  screenshot_path = os.path.join(screenshot_dir, \"error_timeout_comments_tab.png\")\n",
    "                  driver.save_screenshot(screenshot_path)\n",
    "                  print(f\"Saved error screenshot: {screenshot_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"General Error finding or clicking '评论' tab: {e}\")\n",
    "            if driver:\n",
    "                 screenshot_path = os.path.join(screenshot_dir, \"error_generic_comments_tab.png\")\n",
    "                 driver.save_screenshot(screenshot_path)\n",
    "                 print(f\"Saved error screenshot: {screenshot_path}\")\n",
    "\n",
    "        # --- Main Interaction Loop ---\n",
    "        if comments_tab_clicked:\n",
    "            print(\"\\n--- Starting Scroll and Interaction Loop ---\")\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            for i in range(max_scroll_attempts):\n",
    "                print(f\"\\n--- Iteration {i+1}/{max_scroll_attempts} ---\")\n",
    "                actions_taken_this_iteration = False\n",
    "                # 1. Screenshot Before\n",
    "                screenshot_path = os.path.join(screenshot_dir, f\"scroll_iter_{i+1}_before.png\"); driver.save_screenshot(screenshot_path); print(f\"Saved screenshot: {screenshot_path}\")\n",
    "                # 2. Click \"Expand Replies\"\n",
    "                expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '回复')] | //div[contains(@class, 'reply-expander') and contains(., '查看') and contains(., '回复')]\"\n",
    "                try:\n",
    "                    expand_buttons = driver.find_elements(By.XPATH, expand_reply_xpath); print(f\"Found {len(expand_buttons)} 'Expand Replies'.\")\n",
    "                    clicked_count = 0\n",
    "                    for j, button in enumerate(expand_buttons):\n",
    "                        try:\n",
    "                            if button.is_displayed():\n",
    "                                print(f\"Clicking 'Expand Replies' #{j+1}...\"); driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button); time.sleep(0.2)\n",
    "                                driver.execute_script(\"arguments[0].click();\", button); print(\"Clicked.\"); actions_taken_this_iteration = True; clicked_count += 1; time.sleep(0.8)\n",
    "                        except (StaleElementReferenceException): print(\"Stale element on 'Expand Replies', skipping.\"); continue\n",
    "                        except (ElementClickInterceptedException, ElementNotInteractableException):\n",
    "                             print(\"Click intercepted/not interactable on 'Expand Replies'. Trying JS retry.\");\n",
    "                             try: driver.execute_script(\"arguments[0].click();\", button); print(\"Clicked via JS retry.\"); actions_taken_this_iteration = True; clicked_count += 1; time.sleep(0.8)\n",
    "                             except Exception as e_retry: print(f\"JS retry failed: {e_retry}. Skipping.\")\n",
    "                        except Exception as e_click_reply: print(f\"Error clicking 'Expand Replies' #{j+1}: {e_click_reply}\")\n",
    "                    if clicked_count > 0: print(f\"Clicked {clicked_count} 'Expand Replies'.\"); ss_path_expand = os.path.join(screenshot_dir, f\"scroll_iter_{i+1}_after_expand.png\"); driver.save_screenshot(ss_path_expand); print(f\"Saved screenshot: {ss_path_expand}\")\n",
    "                except Exception as e_find_reply: print(f\"Error finding 'Expand Replies': {e_find_reply}\")\n",
    "                # 3. Scroll Down\n",
    "                print(\"Scrolling down...\"); driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\"); time.sleep(1.5)\n",
    "                # 4. Click \"Load More Comments\"\n",
    "                load_more_xpath = \"//*[contains(text(), '展开查看更多') or contains(text(), '加载更多')]\"\n",
    "                try:\n",
    "                    load_more_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_xpath)))\n",
    "                    print(\"Clicking 'Load More'...\"); driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button); time.sleep(0.3)\n",
    "                    driver.execute_script(\"arguments[0].click();\", load_more_button); print(\"Clicked 'Load More'.\"); actions_taken_this_iteration = True; time.sleep(2.5)\n",
    "                    ss_path_load = os.path.join(screenshot_dir, f\"scroll_iter_{i+1}_after_load_more.png\"); driver.save_screenshot(ss_path_load); print(f\"Saved screenshot: {ss_path_load}\")\n",
    "                except TimeoutException: print(\"'Load More' button not found or not clickable.\")\n",
    "                except (ElementClickInterceptedException, ElementNotInteractableException): print(\"Could not click 'Load More'.\")\n",
    "                except Exception as e_load_more: print(f\"Error clicking 'Load More': {e_load_more}\")\n",
    "                # 5. Check End Condition\n",
    "                current_height = driver.execute_script(\"return document.body.scrollHeight\"); print(f\"Current height: {current_height}, Last height: {last_height}\")\n",
    "                if not actions_taken_this_iteration and current_height == last_height: print(\"No actions taken and scroll height did not change. Ending loop.\"); break\n",
    "                last_height = current_height\n",
    "            else: print(f\"\\n--- Reached max iterations ({max_scroll_attempts}) ---\")\n",
    "        else: print(\"Skipping interaction loop because '评论' tab was not successfully clicked.\")\n",
    "\n",
    "        print(\"\\n--- Taking final screenshot ---\")\n",
    "        final_ss_path = os.path.join(screenshot_dir, \"final_state.png\"); driver.save_screenshot(final_ss_path); print(f\"Saved final screenshot: {final_ss_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        # --- CORRECTED INDENTATION FOR ERROR SCREENSHOT TRY-EXCEPT ---\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved error screenshot: {error_ss_path}\")\n",
    "            except Exception as e_ss:\n",
    "                 print(f\"Could not save error screenshot: {e_ss}\")\n",
    "        # --- END CORRECTION ---\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing the browser...\")\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "        else:\n",
    "            print(\"Driver was not initialized.\")\n",
    "    # Removed return as function doesn't explicitly return anything\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(\"--- Starting Xueqiu Debug Scroll/Screenshot Script ---\")\n",
    "    debug_scroll_and_screenshot(target_url, max_scroll_attempts=8)\n",
    "    print(\"--- Debug Script Finished ---\")\n",
    "    print(f\"Check the '{screenshot_dir}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea1c4be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Xueqiu Search Scraper for keyword: '公平' ---\n",
      "Setting up WebDriver...\n",
      "Navigating to: https://xueqiu.com/\n",
      "Looking for the first pop-up ('跳过')...\n",
      "First pop-up ('跳过') not found or timed out.\n",
      "Looking for the second pop-up ('X')...\n",
      "Second pop-up ('X') not found or timed out.\n",
      "Looking for the search bar...\n",
      "Browser closed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 163\u001b[39m\n\u001b[32m    161\u001b[39m search_keyword = \u001b[33m\"\u001b[39m\u001b[33m公平\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- Starting Xueqiu Search Scraper for keyword: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_keyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m results = \u001b[43msearch_and_scrape_xueqiu\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_keyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Scraping Process Finished ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    166\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36msearch_and_scrape_xueqiu\u001b[39m\u001b[34m(keyword, max_results)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Locator for the search input, often has a placeholder\u001b[39;00m\n\u001b[32m     75\u001b[39m search_input_xpath = \u001b[33m\"\u001b[39m\u001b[33m//input[@placeholder=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m搜索股票/组合/用户\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m search_input = \u001b[43mwait\u001b[49m\u001b[43m.\u001b[49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisibility_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_input_xpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSearch bar found. Typing keyword: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m search_input.send_keys(keyword)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cs06t\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:145\u001b[39m, in \u001b[36mWebDriverWait.until\u001b[39m\u001b[34m(self, method, message)\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time.monotonic() > end_time:\n\u001b[32m    144\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys # Needed for Enter key\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException\n",
    ")\n",
    "\n",
    "def search_and_scrape_xueqiu(keyword, max_results=20):\n",
    "    \"\"\"\n",
    "    Searches Xueqiu for a keyword and scrapes post titles and links from results.\n",
    "\n",
    "    Args:\n",
    "        keyword (str): The search term.\n",
    "        max_results (int): Approximate maximum number of results to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing 'title' and 'link'.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Keep browser visible for debugging\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_search\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return []\n",
    "\n",
    "    driver = None\n",
    "    scraped_results = []\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20) # Increased wait time\n",
    "        short_wait = WebDriverWait(driver, 5)\n",
    "\n",
    "        print(f\"Navigating to: https://xueqiu.com/\")\n",
    "        driver.get(\"https://xueqiu.com/\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups (Reusing previous robust logic) ---\n",
    "        try:\n",
    "            print(\"Looking for the first pop-up ('跳过')...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            print(\"Clicking '跳过'...\"); driver.execute_script(\"arguments[0].click();\", skip_button); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"First pop-up ('跳过') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error handling first pop-up: {e}\")\n",
    "        try:\n",
    "            print(\"Looking for the second pop-up ('X')...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]\", \"//i[contains(@class, 'cube-dialog-close')]\", \"//div[contains(@class, 'Modal_modal')]//i[contains(@class, 'Modal_closeIcon')]\", \"//button[@aria-label='Close']\" ]\n",
    "            close_button_found = False\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, xpath))); print(f\"Clicking second pop-up 'X'...\"); driver.execute_script(\"arguments[0].click();\", close_button); close_button_found = True; time.sleep(0.5); break\n",
    "                 except TimeoutException: continue\n",
    "                 except Exception: continue\n",
    "            if not close_button_found: print(\"Second pop-up ('X') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error during second pop-up handling: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # --- Find Search Bar and Search ---\n",
    "        try:\n",
    "            print(f\"Looking for the search bar...\")\n",
    "            # Locator for the search input, often has a placeholder\n",
    "            search_input_xpath = \"//input[@placeholder='搜索股票/组合/用户']\"\n",
    "            search_input = wait.until(EC.visibility_of_element_located((By.XPATH, search_input_xpath)))\n",
    "            print(f\"Search bar found. Typing keyword: '{keyword}'\")\n",
    "            search_input.send_keys(keyword)\n",
    "            time.sleep(0.5) # Brief pause after typing\n",
    "            print(\"Submitting search (pressing Enter)...\")\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "\n",
    "            # --- Wait for Search Results Page ---\n",
    "            print(\"Waiting for search results page to load...\")\n",
    "            # Wait for a container that holds search results\n",
    "            # Inspect the page after searching to find a reliable container ID/class\n",
    "            # Common patterns: 'search-result-list', 'result-list', 'timeline' section\n",
    "            results_container_xpath = \"//div[contains(@class, 'search__result')] | //div[contains(@class, 'timeline')]\" # Adjust based on inspection\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, results_container_xpath)))\n",
    "            print(\"Search results container found.\")\n",
    "            # Add a small pause for results within the container to render\n",
    "            time.sleep(2)\n",
    "\n",
    "            # --- Scrape Results ---\n",
    "            print(\"Scraping search results...\")\n",
    "            # Locator for individual result items (adjust based on inspection)\n",
    "            # Often divs or articles with classes like 'search__result__item', 'timeline__item'\n",
    "            result_item_xpath = \"//div[contains(@class, 'search__result__item')] | //article[contains(@class, 'timeline__item')]\"\n",
    "            result_items = driver.find_elements(By.XPATH, result_item_xpath)\n",
    "\n",
    "            print(f\"Found {len(result_items)} potential result items on the page.\")\n",
    "\n",
    "            count = 0\n",
    "            for item in result_items:\n",
    "                if count >= max_results:\n",
    "                    print(f\"Reached max results limit ({max_results}).\")\n",
    "                    break\n",
    "                try:\n",
    "                    # Find the title link within the item (adjust locator)\n",
    "                    # Often an 'a' tag inside a title div/heading\n",
    "                    title_link_element = item.find_element(By.XPATH, \".//a[contains(@class, 'title')] | .//div[contains(@class,'timeline__item__content')]//a[contains(@href,'/')]\") # Relative XPath\n",
    "\n",
    "                    title = title_link_element.text.strip()\n",
    "                    link = title_link_element.get_attribute('href')\n",
    "\n",
    "                    # Basic validation and filtering\n",
    "                    if title and link and link.startswith(\"http\"): # Ensure it's a valid link\n",
    "                        print(f\"  - Found: {title} ({link})\")\n",
    "                        scraped_results.append({\"title\": title, \"link\": link})\n",
    "                        count += 1\n",
    "                    elif title and link: # Handle relative links if necessary\n",
    "                        print(f\"  - Found (relative link): {title} ({link})\")\n",
    "                        # You might need to join this with the base URL if needed\n",
    "                        scraped_results.append({\"title\": title, \"link\": link})\n",
    "                        count += 1\n",
    "\n",
    "                except NoSuchElementException:\n",
    "                    # print(\"  - Could not find title/link in one item, skipping.\") # Can be verbose\n",
    "                    continue # Skip this item if structure is different\n",
    "                except Exception as e_item:\n",
    "                    print(f\"  - Error processing one item: {e_item}\")\n",
    "                    continue\n",
    "\n",
    "            if not scraped_results:\n",
    "                print(\"No results scraped. Check locators or if results loaded.\")\n",
    "                driver.save_screenshot(os.path.join(screenshot_dir, \"no_results_scraped.png\"))\n",
    "\n",
    "        except TimeoutException as e_search:\n",
    "            print(f\"Timeout error during search or results loading: {e_search.msg}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir, \"error_search_timeout.png\"))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during search or scraping: {e}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir, \"error_search_generic.png\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "        if driver:\n",
    "            try: driver.save_screenshot(os.path.join(screenshot_dir, \"critical_error.png\"))\n",
    "            except: pass\n",
    "    finally:\n",
    "        if driver:\n",
    "           # print(\"Closing the browser...\")\n",
    "           # driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "\n",
    "    return scraped_results\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    search_keyword = \"公平\"\n",
    "    print(f\"--- Starting Xueqiu Search Scraper for keyword: '{search_keyword}' ---\")\n",
    "    results = search_and_scrape_xueqiu(search_keyword, max_results=15)\n",
    "    print(\"--- Scraping Process Finished ---\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"      Scraped Search Results\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    if results:\n",
    "        print(f\"Scraped {len(results)} results:\")\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"{i+1}. Title: {result['title']}\")\n",
    "            print(f\"   Link: {result['link']}\")\n",
    "    else:\n",
    "        print(\">>> No results were successfully scraped. <<<\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Check console logs and folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e4fc6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3608465887.py, line 168)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif driver: try: driver.save_screenshot(os.path.join(screenshot_dir, \"critical_error.png\"))\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException\n",
    ")\n",
    "\n",
    "def search_and_scrape_xueqiu_revised(keyword, max_results=20):\n",
    "    \"\"\"\n",
    "    Searches Xueqiu homepage, clicks search button, and scrapes results.\n",
    "\n",
    "    Args:\n",
    "        keyword (str): The search term.\n",
    "        max_results (int): Approximate maximum number of results to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing 'title' and 'link'.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1366,768\") # Common size\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_search\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return []\n",
    "\n",
    "    driver = None\n",
    "    scraped_results = []\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window() # Still maximize\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        short_wait = WebDriverWait(driver, 5)\n",
    "\n",
    "        print(f\"Navigating to: https://xueqiu.com/\")\n",
    "        driver.get(\"https://xueqiu.com/\")\n",
    "        time.sleep(2) # Allow initial load\n",
    "\n",
    "        # --- Handle Initial Pop-ups (Keep trying, may not appear) ---\n",
    "        try:\n",
    "            print(\"Looking for the first pop-up ('跳过')...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 7).until(EC.element_to_be_clickable((By.XPATH, skip_xpath))) # Shorter wait\n",
    "            print(\"Clicking '跳过'...\"); driver.execute_script(\"arguments[0].click();\", skip_button); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"First pop-up ('跳过') not found or timed out (might be expected).\")\n",
    "        except Exception as e: print(f\"Error handling first pop-up: {e}\")\n",
    "        try:\n",
    "            print(\"Looking for the second pop-up ('X')...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]\", \"//i[contains(@class, 'cube-dialog-close')]\", \"//div[contains(@class, 'Modal_modal')]//i[contains(@class, 'Modal_closeIcon')]\", \"//button[@aria-label='Close']\" ]\n",
    "            close_button_found = False\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.XPATH, xpath))); print(f\"Clicking second pop-up 'X'...\"); driver.execute_script(\"arguments[0].click();\", close_button); close_button_found = True; time.sleep(0.5); break\n",
    "                 except TimeoutException: continue\n",
    "                 except Exception: continue\n",
    "            if not close_button_found: print(\"Second pop-up ('X') not found or timed out (might be expected).\")\n",
    "        except Exception as e: print(f\"Error during second pop-up handling: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # --- Find Search Bar and Search ---\n",
    "        try:\n",
    "            print(f\"Looking for the search bar...\")\n",
    "            search_input_xpath = \"//input[@placeholder='搜索股票/组合/用户']\"\n",
    "            search_button_xpath = \"//div[contains(@class,'navbar__search')]//i[contains(@class,'search-icon')]/parent::div\" # Find icon, go to parent div which is clickable\n",
    "\n",
    "            search_input = wait.until(EC.element_to_be_clickable((By.XPATH, search_input_xpath)))\n",
    "            print(f\"Search bar found. Clicking and typing keyword: '{keyword}'\")\n",
    "            # search_input.click() # Click first sometimes helps\n",
    "            # time.sleep(0.2)\n",
    "            search_input.clear() # Clear any potential default text\n",
    "            search_input.send_keys(keyword)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            print(\"Looking for search button...\")\n",
    "            search_button = wait.until(EC.element_to_be_clickable((By.XPATH, search_button_xpath)))\n",
    "            print(\"Search button found. Clicking search button...\")\n",
    "            search_button.click()\n",
    "\n",
    "            # --- Wait for Search Results Page ---\n",
    "            print(\"Waiting for search results page transition...\")\n",
    "            # Option 1: Wait for URL change\n",
    "            wait.until(EC.url_contains(\"/search?keyword=\"))\n",
    "            print(f\"URL changed, indicating search results page loaded: {driver.current_url}\")\n",
    "            # Option 2: Wait for a specific element unique to the results page\n",
    "            # results_page_marker_xpath = \"//div[contains(@class,'result-title-wrap')]\" # Adjust this! Inspect results page for a unique static element\n",
    "            # wait.until(EC.presence_of_element_located((By.XPATH, results_page_marker_xpath)))\n",
    "            # print(\"Results page marker element found.\")\n",
    "\n",
    "            # Save screenshot of results page\n",
    "            time.sleep(2) # Allow results rendering\n",
    "            results_screenshot_path = os.path.join(screenshot_dir, \"search_results_page.png\")\n",
    "            driver.save_screenshot(results_screenshot_path)\n",
    "            print(f\"Screenshot of search results page saved to: {results_screenshot_path}\")\n",
    "\n",
    "            # --- Scrape Results ---\n",
    "            print(\"Scraping search results...\")\n",
    "            # *** THESE LOCATORS MUST BE ADJUSTED BASED ON ACTUAL RESULTS PAGE INSPECTION ***\n",
    "            # Example based on common structures, VERY LIKELY NEEDS CORRECTION\n",
    "            result_item_xpath = \"//div[@class='search-item']\" # Example results item wrapper\n",
    "            title_link_relative_xpath = \".//a[contains(@class,'title')]\" # Example title link within item\n",
    "\n",
    "            # Find items using the potentially updated item xpath\n",
    "            result_items = driver.find_elements(By.XPATH, result_item_xpath)\n",
    "            print(f\"Found {len(result_items)} potential result items using XPath: {result_item_xpath}\")\n",
    "\n",
    "            count = 0\n",
    "            if not result_items:\n",
    "                print(\"No result items found using the current XPath. Please inspect the page and update 'result_item_xpath'.\")\n",
    "\n",
    "            for item in result_items:\n",
    "                if count >= max_results:\n",
    "                    print(f\"Reached max results limit ({max_results}).\")\n",
    "                    break\n",
    "                try:\n",
    "                    # Find link/title using the potentially updated relative xpath\n",
    "                    title_link_element = item.find_element(By.XPATH, title_link_relative_xpath)\n",
    "                    title = title_link_element.text.strip()\n",
    "                    link = title_link_element.get_attribute('href')\n",
    "\n",
    "                    if title and link and link.startswith(\"http\"):\n",
    "                        print(f\"  - Found: {title} ({link})\")\n",
    "                        scraped_results.append({\"title\": title, \"link\": link})\n",
    "                        count += 1\n",
    "                    elif title and link:\n",
    "                        # Handle relative URLs if needed by joining with base URL\n",
    "                        # from urllib.parse import urljoin\n",
    "                        # base_url = \"https://xueqiu.com\"\n",
    "                        # absolute_link = urljoin(base_url, link)\n",
    "                        # print(f\"  - Found (relative): {title} ({absolute_link})\")\n",
    "                        # scraped_results.append({\"title\": title, \"link\": absolute_link})\n",
    "                        print(f\"  - Found (relative/fragment?): {title} ({link})\")\n",
    "                        scraped_results.append({\"title\": title, \"link\": link}) # Store as is for now\n",
    "                        count += 1\n",
    "\n",
    "                except NoSuchElementException:\n",
    "                    print(\"  - Could not find title/link in one item using relative XPath, skipping. Please inspect and update 'title_link_relative_xpath'.\")\n",
    "                    continue\n",
    "                except Exception as e_item:\n",
    "                    print(f\"  - Error processing one item: {e_item}\")\n",
    "                    continue\n",
    "\n",
    "            if not scraped_results and result_items:\n",
    "                 print(\"Found result item containers, but failed to extract title/link. Check 'title_link_relative_xpath'.\")\n",
    "\n",
    "            if not scraped_results and not result_items:\n",
    "                print(\"No results scraped. Check results page structure and locators.\")\n",
    "                # Screenshot already taken, but could take another one here if needed\n",
    "\n",
    "        except TimeoutException as e_search:\n",
    "            print(f\"Timeout error during search or results loading: {e_search.msg}\")\n",
    "            if driver: driver.save_screenshot(os.path.join(screenshot_dir, \"error_search_timeout.png\"))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during search or scraping: {e}\")\n",
    "            if driver: driver.save_screenshot(os.path.join(screenshot_dir, \"error_search_generic.png\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\"); print(f\"Error: {e}\")\n",
    "        if driver: try: driver.save_screenshot(os.path.join(screenshot_dir, \"critical_error.png\"))\n",
    "                   except: pass\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "\n",
    "    return scraped_results\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    search_keyword = \"公平\"\n",
    "    print(f\"--- Starting Xueqiu Search Scraper for keyword: '{search_keyword}' ---\")\n",
    "    results = search_and_scrape_xueqiu_revised(search_keyword, max_results=15)\n",
    "    print(\"--- Scraping Process Finished ---\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Search Results\"); print(\"=\"*30)\n",
    "    if results:\n",
    "        print(f\"Scraped {len(results)} results:\")\n",
    "        for i, result in enumerate(results): print(f\"{i+1}. Title: {result['title']}\\n   Link: {result['link']}\")\n",
    "    else: print(\">>> No results were successfully scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30); print(f\"Check console logs and '{screenshot_dir}' folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "507ab0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Incremental Scroll for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_incremental_scroll' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Body element loaded.\n",
      "Looking for the first pop-up ('跳过')...\n",
      "First pop-up ('跳过') not found or timed out.\n",
      "Looking for the second pop-up ('X')...\n",
      "Second pop-up ('X') not found or timed out.\n",
      "\n",
      "--- Starting Incremental Scroll and Screenshot Process ---\n",
      "--- Scroll Attempt #1 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_01_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 482, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #2 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_02_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 963, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #3 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_03_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 1445, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #4 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_04_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 1926, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #5 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_05_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 2408, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #6 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_06_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 2890, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #7 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_07_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 3371, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #8 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_08_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 3853, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #9 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_09_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 4322, Viewport height: 602, Total scrollHeight: 4924\n",
      "Reached the bottom of the page.\n",
      "Saved final bottom screenshot: screenshots_incremental_scroll\\scroll_09_at_bottom.png\n",
      "\n",
      "--- Incremental Scroll and Screenshot Process Finished ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "--- Script Finished ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'screenshot_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    147\u001b[39m scroll_and_screenshot_by_distance(\n\u001b[32m    148\u001b[39m     target_url,\n\u001b[32m    149\u001b[39m     scroll_pause_time=\u001b[32m2.0\u001b[39m,\n\u001b[32m    150\u001b[39m     scroll_increment_js=\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollBy(0, window.innerHeight * 0.8);\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    151\u001b[39m     )\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Script Finished ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheck the \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mscreenshot_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m folder for screenshots.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'screenshot_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException\n",
    ")\n",
    "\n",
    "def scroll_and_screenshot_by_distance(url, scroll_pause_time=1.5, scroll_increment_js=\"window.scrollBy(0, window.innerHeight);\"):\n",
    "    \"\"\"\n",
    "    Navigates to a URL, handles popups, scrolls down in increments, taking a screenshot\n",
    "    at each step until the bottom of the page is reached.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the page to scroll.\n",
    "        scroll_pause_time (float): Time in seconds to wait after each scroll\n",
    "                                   for content to potentially load.\n",
    "        scroll_increment_js (str): JavaScript to execute for scrolling.\n",
    "                                   Default scrolls by one viewport height.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Keep browser visible to observe\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,800\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_incremental_scroll\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        # driver.maximize_window() # Maximize or set specific size above\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            print(\"Body element loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Page body did not become present within timeout.\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        # --- Handle Initial Pop-ups (Integrated robust logic) ---\n",
    "        try:\n",
    "            print(\"Looking for the first pop-up ('跳过')...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            print(\"Clicking '跳过'...\"); driver.execute_script(\"arguments[0].click();\", skip_button); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"First pop-up ('跳过') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error handling first pop-up: {e}\")\n",
    "        try:\n",
    "            print(\"Looking for the second pop-up ('X')...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]\", \"//i[contains(@class, 'cube-dialog-close')]\", \"//div[contains(@class, 'Modal_modal')]//i[contains(@class, 'Modal_closeIcon')]\", \"//button[@aria-label='Close']\" ]\n",
    "            close_button_found = False\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, xpath))); print(f\"Clicking second pop-up 'X'...\"); driver.execute_script(\"arguments[0].click();\", close_button); close_button_found = True; time.sleep(0.5); break\n",
    "                 except TimeoutException: continue\n",
    "                 except Exception: continue\n",
    "            if not close_button_found: print(\"Second pop-up ('X') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error during second pop-up handling: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # Optional: Click 'Comments' Tab\n",
    "        # try:\n",
    "        #     print(\"Looking for and clicking '评论' tab before scrolling...\")\n",
    "        #     tab_xpath = \"//span[text()='评论' and contains(@class,'tabs__item__title')]/ancestor::div[contains(@class,'tabs__item')] | //div[contains(@class, 'tabs__item') and .//span[text()='评论']] | //div[contains(@class, 'action-bar__item') and contains(., '评论')]\"\n",
    "        #     comments_tab_element = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, tab_xpath)))\n",
    "        #     driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', inline: 'nearest'});\", comments_tab_element)\n",
    "        #     time.sleep(0.5)\n",
    "        #     comments_tab_clickable = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, tab_xpath)))\n",
    "        #     driver.execute_script(\"arguments[0].click();\", comments_tab_clickable)\n",
    "        #     print(\"Clicked '评论' tab. Waiting before scroll...\")\n",
    "        #     time.sleep(2.5)\n",
    "        # except Exception as e_tab:\n",
    "        #     print(f\"Could not click '评论' tab before scrolling (maybe not needed or error): {e_tab}\")\n",
    "\n",
    "        print(\"\\n--- Starting Incremental Scroll and Screenshot Process ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_attempt = 0\n",
    "\n",
    "        while True:\n",
    "            scroll_attempt += 1\n",
    "            print(f\"--- Scroll Attempt #{scroll_attempt} ---\")\n",
    "\n",
    "            screenshot_path = os.path.join(screenshot_dir, f\"scroll_{scroll_attempt:02d}_before.png\")\n",
    "            try:\n",
    "                driver.save_screenshot(screenshot_path)\n",
    "                print(f\"Saved screenshot: {screenshot_path}\")\n",
    "            except Exception as e_ss:\n",
    "                print(f\"Failed to save screenshot {screenshot_path}: {e_ss}\")\n",
    "\n",
    "            print(f\"Scrolling down using: {scroll_increment_js}\")\n",
    "            driver.execute_script(scroll_increment_js)\n",
    "\n",
    "            print(f\"Pausing for {scroll_pause_time} seconds...\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            current_scroll_y = driver.execute_script(\"return window.pageYOffset || document.documentElement.scrollTop;\")\n",
    "            viewport_height = driver.execute_script(\"return window.innerHeight;\")\n",
    "            print(f\"  Current scrollY: {round(current_scroll_y)}, Viewport height: {round(viewport_height)}, Total scrollHeight: {new_height}\")\n",
    "\n",
    "            if current_scroll_y + viewport_height >= new_height - 10:\n",
    "                print(\"Reached the bottom of the page.\")\n",
    "                final_bottom_path = os.path.join(screenshot_dir, f\"scroll_{scroll_attempt:02d}_at_bottom.png\")\n",
    "                try: driver.save_screenshot(final_bottom_path)\n",
    "                except Exception as e_fin_ss: print(f\"Could not save final screenshot: {e_fin_ss}\")\n",
    "                print(f\"Saved final bottom screenshot: {final_bottom_path}\")\n",
    "                break\n",
    "            elif scroll_attempt > 50:\n",
    "                print(\"Reached max scroll attempts (50). Stopping.\")\n",
    "                break\n",
    "            last_height = new_height\n",
    "        print(\"\\n--- Incremental Scroll and Screenshot Process Finished ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        # --- CORRECTED INDENTATION FOR ERROR SCREENSHOT TRY-EXCEPT ---\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error_scroll_script.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved error screenshot: {error_ss_path}\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "        # --- END CORRECTION ---\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing the browser...\")\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Incremental Scroll for URL: {target_url} ---\")\n",
    "    scroll_and_screenshot_by_distance(\n",
    "        target_url,\n",
    "        scroll_pause_time=2.0,\n",
    "        scroll_increment_js=\"window.scrollBy(0, window.innerHeight * 0.8);\"\n",
    "        )\n",
    "    print(\"--- Script Finished ---\")\n",
    "    print(f\"Check the '{screenshot_dir}' folder for screenshots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c515e8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_post_comments' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Scraping main post content...\n",
      "Post content scraped (length: 627).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Loop/Scroll attempt #1 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags on this pass.\n",
      "    Added 2 new unique comments this pass.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "--- Loop/Scroll attempt #2 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags on this pass.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "--- Loop/Scroll attempt #3 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags on this pass.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "Scroll height unchanged and no new comments for 2 strikes. Assuming all loaded or stuck.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 2 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "...\n",
      "\n",
      "--- Comments (2) ---\n",
      "不一定会成日鬼那样的社会，天朝…\n",
      "社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'screenshot_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 215\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> No comments were scraped. <<<\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    214\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheck console logs and \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mscreenshot_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m folder for details.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'screenshot_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException\n",
    ")\n",
    "\n",
    "def scrape_post_and_comments_on_scroll(url, max_scroll_loops=15, scroll_pause_time=2.5):\n",
    "    \"\"\"\n",
    "    Navigates to a Xueqiu post, handles popups, scrolls to load comments,\n",
    "    and attempts to scrape them. Also clicks 'expand replies' and 'load more'.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\") # Wider for layout\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"comments\": []}\n",
    "    # Use a set to store scraped comment texts to automatically handle duplicates\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20) # For initial elements\n",
    "        short_wait = WebDriverWait(driver, 7) # For dynamic elements in loop\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load. Page might be different.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2) # Pause for popups\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue # Try next\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # Scrape Main Post Content\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        no_new_content_strikes = 0\n",
    "\n",
    "        for i in range(max_scroll_loops):\n",
    "            print(f\"--- Loop/Scroll attempt #{i+1} ---\")\n",
    "            initial_comment_count = len(unique_comment_texts_scraped)\n",
    "\n",
    "            # 1. Click \"查看N条回复\" (Expand Replies) - these often appear within comment threads\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            try:\n",
    "                # Find all such links that are currently visible\n",
    "                expand_buttons = driver.find_elements(By.XPATH, expand_reply_xpath)\n",
    "                if expand_buttons:\n",
    "                    print(f\"Found {len(expand_buttons)} 'Expand Replies' links.\")\n",
    "                    for button_idx, button in enumerate(expand_buttons):\n",
    "                        try:\n",
    "                            if button.is_displayed(): # Click only if visible\n",
    "                                print(f\"  Clicking 'Expand Replies' #{button_idx+1}...\")\n",
    "                                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button)\n",
    "                                time.sleep(0.3)\n",
    "                                driver.execute_script(\"arguments[0].click();\", button)\n",
    "                                time.sleep(1.5) # Wait for replies to load\n",
    "                        except StaleElementReferenceException: print(\"  Stale 'Expand Replies' link, skipping.\")\n",
    "                        except ElementNotInteractableException: print(\"  'Expand Replies' link not interactable, skipping.\")\n",
    "                        except Exception as e_expand: print(f\"  Error clicking 'Expand Replies': {e_expand}\")\n",
    "            except Exception as e_find_expand:\n",
    "                print(f\"Could not search for 'Expand Replies' buttons: {e_find_expand}\")\n",
    "\n",
    "\n",
    "            # 2. Scroll down\n",
    "            print(\"Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight + 500);\") # Scroll a bit past current perceived bottom\n",
    "            time.sleep(scroll_pause_time) # Wait for new content\n",
    "\n",
    "            # 3. Attempt to scrape comments\n",
    "            # XPath based on your \"Inspect Element\" screenshot: <div class=\"comment__item__main\"><p>...</p></div>\n",
    "            comment_text_xpath = \"//div[@class='comment__item__main']/p\"\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "                if comment_p_tags:\n",
    "                    print(f\"  Found {len(comment_p_tags)} potential comment <p> tags on this pass.\")\n",
    "                    new_comments_found_this_pass = 0\n",
    "                    for p_tag in comment_p_tags:\n",
    "                        try:\n",
    "                            comment_text = p_tag.text.strip()\n",
    "                            if comment_text and comment_text not in unique_comment_texts_scraped:\n",
    "                                # print(f\"    New Comment: {comment_text[:70]}...\") # Can be verbose\n",
    "                                unique_comment_texts_scraped.add(comment_text)\n",
    "                                new_comments_found_this_pass +=1\n",
    "                        except StaleElementReferenceException:\n",
    "                            # print(\"    Stale comment p_tag, skipping.\")\n",
    "                            continue # Element removed from DOM, common during dynamic updates\n",
    "                        except Exception as e_text:\n",
    "                            print(f\"    Error getting text from a p_tag: {e_text}\")\n",
    "                    if new_comments_found_this_pass > 0:\n",
    "                        print(f\"    Added {new_comments_found_this_pass} new unique comments this pass.\")\n",
    "            except Exception as e_find:\n",
    "                print(f\"  Error finding comment <p> tags: {e_find}\")\n",
    "\n",
    "\n",
    "            # 4. Click \"展开查看更多\" (Load More Main Comments) - usually at the very bottom of comment list\n",
    "            load_more_comments_xpath = \"//div[contains(@class,'more-comment') and contains(., '展开查看更多')]\" # Adjust if needed\n",
    "            try:\n",
    "                load_more_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多' button. Clicking...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button)\n",
    "                time.sleep(0.3)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                print(\"  Clicked '展开查看更多'.\")\n",
    "                time.sleep(scroll_pause_time) # Wait for more comments\n",
    "            except TimeoutException:\n",
    "                # print(\"  '展开查看更多' button not found or not clickable on this pass.\") # Expected if no more\n",
    "                pass\n",
    "            except Exception as e_load_more:\n",
    "                print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            print(f\"  Current total comments scraped: {len(unique_comment_texts_scraped)}. Scroll height: {current_height}\")\n",
    "            if len(unique_comment_texts_scraped) > initial_comment_count:\n",
    "                no_new_content_strikes = 0 # Reset if new comments were found\n",
    "            else:\n",
    "                no_new_content_strikes += 1\n",
    "\n",
    "            if current_height == last_height and no_new_content_strikes >= 2 : # If height hasn't changed for 2 loops AND no new comments\n",
    "                print(\"Scroll height unchanged and no new comments for 2 strikes. Assuming all loaded or stuck.\")\n",
    "                break\n",
    "            elif no_new_content_strikes >= 3: # If no new comments for 3 loops even if height changes (e.g. ads loading)\n",
    "                print(\"No new comments found for 3 consecutive strikes. Assuming all loaded.\")\n",
    "                break\n",
    "\n",
    "            last_height = current_height\n",
    "            if i == max_scroll_loops -1 :\n",
    "                print(\"Reached max scroll loops.\")\n",
    "\n",
    "            # Screenshot at end of loop\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"loop_end_{i+1}.png\"))\n",
    "\n",
    "\n",
    "        scraped_data[\"comments\"] = list(unique_comment_texts_scraped) # Convert set to list for output\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\"); print(f\"Error: {e}\")\n",
    "        if driver: driver.save_screenshot(os.path.join(screenshot_dir, \"critical_error.png\"))\n",
    "                   #except: pass\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_and_comments_on_scroll(target_url, max_scroll_loops=10, scroll_pause_time=3.0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\")\n",
    "        print(data[\"post_content\"][:500] + \"...\" if len(data[\"post_content\"]) > 500 else data[\"post_content\"])\n",
    "    else:\n",
    "        print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment in enumerate(data[\"comments\"][:20]): # Print first 20\n",
    "            print(f\"{i+1}. {comment[:150]}\" + \"...\" if len(comment)>150 else comment)\n",
    "        if len(data[\"comments\"]) > 20:\n",
    "            print(f\"... and {len(data['comments']) - 20} more comments.\")\n",
    "    else:\n",
    "        print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Check console logs and '{screenshot_dir}' folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6347192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content...\n",
      "Post content scraped (length: 627).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Loop/Scroll attempt #1 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags.\n",
      "    Added 2 new unique comments.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "--- Loop/Scroll attempt #2 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "--- Loop/Scroll attempt #3 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "Scroll height unchanged and no new comments for 2 strikes. Assuming all loaded or stuck.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 2 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "...\n",
      "\n",
      "--- Comments (2) ---\n",
      "不一定会成日鬼那样的社会，天朝…\n",
      "社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'screenshot_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 197\u001b[39m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> No comments were scraped. <<<\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheck console logs and \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mscreenshot_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m folder for details.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'screenshot_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException\n",
    ")\n",
    "\n",
    "def scrape_post_and_comments_on_scroll(url, max_scroll_loops=15, scroll_pause_time=2.5):\n",
    "    \"\"\"\n",
    "    Navigates to a Xueqiu post, handles popups, scrolls to load comments,\n",
    "    and attempts to scrape them. Also clicks 'expand replies' and 'load more'.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        short_wait = WebDriverWait(driver, 7)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load. Page might be different.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\") # Confirmation\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # Scrape Main Post Content\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        no_new_content_strikes = 0\n",
    "\n",
    "        for i in range(max_scroll_loops):\n",
    "            print(f\"--- Loop/Scroll attempt #{i+1} ---\")\n",
    "            initial_comment_count = len(unique_comment_texts_scraped)\n",
    "\n",
    "            # 1. Click \"查看N条回复\"\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            try:\n",
    "                expand_buttons = driver.find_elements(By.XPATH, expand_reply_xpath)\n",
    "                if expand_buttons:\n",
    "                    print(f\"Found {len(expand_buttons)} 'Expand Replies' links.\")\n",
    "                    for button_idx, button in enumerate(expand_buttons):\n",
    "                        try:\n",
    "                            if button.is_displayed():\n",
    "                                print(f\"  Clicking 'Expand Replies' #{button_idx+1}...\"); driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button); time.sleep(0.3)\n",
    "                                driver.execute_script(\"arguments[0].click();\", button); time.sleep(1.5)\n",
    "                        except StaleElementReferenceException: print(\"  Stale 'Expand Replies' link, skipping.\")\n",
    "                        except ElementNotInteractableException: print(\"  'Expand Replies' link not interactable, skipping.\")\n",
    "                        except Exception as e_expand: print(f\"  Error clicking 'Expand Replies': {e_expand}\")\n",
    "            except Exception as e_find_expand: print(f\"Could not search for 'Expand Replies' buttons: {e_find_expand}\")\n",
    "\n",
    "            # 2. Scroll down\n",
    "            print(\"Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight + 500);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            # 3. Attempt to scrape comments\n",
    "            comment_text_xpath = \"//div[@class='comment__item__main']/p\"\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "                if comment_p_tags:\n",
    "                    print(f\"  Found {len(comment_p_tags)} potential comment <p> tags.\")\n",
    "                    new_comments_found_this_pass = 0\n",
    "                    for p_tag in comment_p_tags:\n",
    "                        try:\n",
    "                            comment_text = p_tag.text.strip()\n",
    "                            if comment_text and comment_text not in unique_comment_texts_scraped:\n",
    "                                unique_comment_texts_scraped.add(comment_text)\n",
    "                                new_comments_found_this_pass +=1\n",
    "                        except StaleElementReferenceException: continue\n",
    "                        except Exception as e_text: print(f\"    Error getting text from a p_tag: {e_text}\")\n",
    "                    if new_comments_found_this_pass > 0: print(f\"    Added {new_comments_found_this_pass} new unique comments.\")\n",
    "            except Exception as e_find: print(f\"  Error finding comment <p> tags: {e_find}\")\n",
    "\n",
    "            # 4. Click \"展开查看更多\"\n",
    "            load_more_comments_xpath = \"//div[contains(@class,'more-comment') and (contains(., '展开查看更多') or contains(., '加载更多'))]\"\n",
    "            try:\n",
    "                load_more_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多/加载更多' button. Clicking...\"); driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button); time.sleep(0.3)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button); print(\"  Clicked '展开查看更多/加载更多'.\"); time.sleep(scroll_pause_time)\n",
    "            except TimeoutException: pass\n",
    "            except Exception as e_load_more: print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            print(f\"  Current total comments scraped: {len(unique_comment_texts_scraped)}. Scroll height: {current_height}\")\n",
    "            if len(unique_comment_texts_scraped) > initial_comment_count: no_new_content_strikes = 0\n",
    "            else: no_new_content_strikes += 1\n",
    "\n",
    "            if current_height == last_height and no_new_content_strikes >= 2 :\n",
    "                print(\"Scroll height unchanged and no new comments for 2 strikes. Assuming all loaded or stuck.\")\n",
    "                break\n",
    "            elif no_new_content_strikes >= 3:\n",
    "                print(\"No new comments found for 3 consecutive strikes. Assuming all loaded.\")\n",
    "                break\n",
    "            last_height = current_height\n",
    "            if i == max_scroll_loops -1 : print(\"Reached max scroll loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"loop_end_{i+1}.png\"))\n",
    "\n",
    "        scraped_data[\"comments\"] = list(unique_comment_texts_scraped)\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        # --- CORRECTED INDENTATION FOR ERROR SCREENSHOT TRY-EXCEPT ---\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "        # --- END CORRECTION ---\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing the browser...\")\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_and_comments_on_scroll(target_url, max_scroll_loops=10, scroll_pause_time=3.0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\")\n",
    "        print(data[\"post_content\"][:500] + \"...\" if len(data[\"post_content\"]) > 500 else data[\"post_content\"])\n",
    "    else:\n",
    "        print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment in enumerate(data[\"comments\"][:20]): # Print first 20\n",
    "            print(f\"{i+1}. {comment[:150]}\" + \"...\" if len(comment)>150 else comment)\n",
    "        if len(data[\"comments\"]) > 20:\n",
    "            print(f\"... and {len(data['comments']) - 20} more comments.\")\n",
    "    else:\n",
    "        print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Check console logs and '{screenshot_dir}' folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f0ee95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_post_all_comments' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content...\n",
      "Post content scraped (length: 706).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Main Loop Iteration #1 ---\n",
      "  Scrolling down...\n",
      "    Added 18 new unique comments this pass.\n",
      "  '展开查看更多/加载更多' button not found or not clickable this pass.\n",
      "  Loop 1 end. Total unique comments: 18. Scroll height: 4924. Last height: 4924\n",
      "--- Main Loop Iteration #2 ---\n",
      "  Scrolling down...\n",
      "  '展开查看更多/加载更多' button not found or not clickable this pass.\n",
      "  Loop 2 end. Total unique comments: 18. Scroll height: 4924. Last height: 4924\n",
      "  No new actions or comments strike: 1\n",
      "--- Main Loop Iteration #3 ---\n",
      "  Scrolling down...\n",
      "  '展开查看更多/加载更多' button not found or not clickable this pass.\n",
      "  Loop 3 end. Total unique comments: 18. Scroll height: 4924. Last height: 4924\n",
      "  No new actions or comments strike: 2\n",
      "No new comments found and no interaction buttons successfully clicked for 2 consecutive loops. Assuming completion.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 18 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "零食甚至“宠物饮品”一应俱全\n",
      "。\n",
      "独酌时\n",
      "，\n",
      "希望宠物也能“陪一杯”\n",
      "。\n",
      "\n",
      "3️⃣ 陪伴型机器人：技术不卖功能\n",
      "，\n",
      "卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT\n",
      "，\n",
      "不扫地\n",
      "、\n",
      "不聊天\n",
      "、\n",
      "不能干活——但它会“撒娇”\n",
      "、\n",
      "会“跟着你”\n",
      "、\n",
      "会“让你想抱”\n",
      "。\n",
      "\n",
      "这是一台卖情绪价值的机器人\n",
      "。\n",
      "\n",
      "$恒生指数(HKHSI)$ $上证指数(SH000001)$ $招商银行(SH600036)$\n",
      "\n",
      "--- Comments (18) ---\n",
      "1. 神经，愚蠢没有逻辑，人类是群居动物，它们渴望伴侣，阶段性的单身是因为经济原因，因为穷，男的穷养不起家，结不了婚，女的穷，养不起自己，并且找不到能养起自己的人，所以不嫁。\n",
      "未来随着机器人人工智能，经济指数级增长，人类会涨校园里的大学生一样无忧无虑，男人和女人因为爱情成群结队。\n",
      "2. 有个手机就够了，老公老婆都可以不要\n",
      "3. 十年二十年后，大概也这样了，会出什么龙头公司股票呢？\n",
      "4. 总需求停滞，新需求就是存量竞争，从物质需求到精神需求，从奢靡需求到精简需求，从传统需求到新型，个性需求。。但事实上，看看日本的股市龙头，这些需求转向基本做不大，市值最大的还是一些大企业，银行，保险，电信，商社，汽车，电气，半导体，互联网；到了中国大概率还是中字头，资源类，制造业龙头，龙头科创互联网。。\n",
      "5. 不婚不育最根本原因是上世纪不准生，一旦生态链损毁，修复何其难。\n",
      "6. 没有人喜欢照顾别人的情绪，但是所有人都想从别人那里找到情绪价值。\n",
      "7. 我这段时间的观察与思考和这部分内容是一致的。我女儿最喜欢的便利店是全家Family Mart，里面的海苔饭团、咖喱鸡饭都是冷藏食品，她一点不嫌弃，反而很热爱，还兴高采烈地告诉我，她很喜欢这些食品。\n",
      "宠物这一部分我前段时间也在关注，今早还发了一条有关于此的消息。\n",
      "五一节期间和一位50岁的大哥聊天，他说令他意外的是，成都天府广场一座被传统零售商业模式淘汰的商场，现在被谷子经济救活了，里面人山人海，交易量火爆。\n",
      "8. 不可能的。两个国家本质上的价值观完全不一样。我身边蛮多00后，目前没听到一个说30以后不结婚的。穷女和稳定男是最迫切想结婚的人群。这还是经常打游戏的宅群。\n",
      "9. 我们的社会确实朝着这个方向走\n",
      "个人取代家庭，成为社会的最小单元了\n",
      "不管我们承不承认，年轻人的消费习惯和我们完全不一样了\n",
      "$泡泡玛特(09992)$\n",
      "10. 性爱机器人产品会供不应求，这类公司会卷出一个品牌。\n",
      "11. 不一定会成日鬼那样的社会，天朝…\n",
      "12. 我们这现在最先来的应该是老年经济\n",
      "13. 内卷国家的必然\n",
      "14. 如果真能成为日本那样的发达国家就知足吧、优衣库2000多美刀的收入，物价再贵去掉吃喝拉撒也能一个月换个苹果手机了，这就是我们这里看不起的售货员.\n",
      "15. 东亚社畜主义社会\n",
      "16. 文明的诅咒\n",
      "17. 社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "18. 中国人均财富水平距离日本还差的太远，更大的可能性是未富先老，或许更应该思考的是一大堆拮据的老年人最可能消费什么\n",
      "\n",
      "==============================\n",
      "Check console logs and folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def scrape_post_and_all_comments(url, max_main_loops=15, scroll_pause_time=2.5):\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_all_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        short_wait = WebDriverWait(driver, 5) # Shorter wait for elements within loops\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "\n",
    "        # Scrape Main Post Content\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        no_new_actions_or_comments_strikes = 0\n",
    "\n",
    "        for i in range(max_main_loops):\n",
    "            print(f\"--- Main Loop Iteration #{i+1} ---\")\n",
    "            action_taken_this_loop = False\n",
    "            comments_found_before_interactions = len(unique_comment_texts_scraped)\n",
    "\n",
    "            # 1. Click ALL visible \"查看N条回复\" (Expand Replies)\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            # Loop to click these as new ones might appear after expanding others\n",
    "            while True:\n",
    "                clicked_an_expand_button_this_pass = False\n",
    "                try:\n",
    "                    expand_buttons = driver.find_elements(By.XPATH, expand_reply_xpath)\n",
    "                    if not expand_buttons: # print(\"  No 'Expand Replies' links found this pass.\");\n",
    "                        break\n",
    "\n",
    "                    # Filter only visible buttons before attempting to click\n",
    "                    visible_expand_buttons = [b for b in expand_buttons if b.is_displayed()]\n",
    "                    if not visible_expand_buttons: # print(\"  No *visible* 'Expand Replies' links.\");\n",
    "                        break\n",
    "\n",
    "                    print(f\"  Found {len(visible_expand_buttons)} visible 'Expand Replies' links to click.\")\n",
    "                    for button in visible_expand_buttons:\n",
    "                        try:\n",
    "                            # print(\"    Scrolling to 'Expand Replies' and clicking...\")\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", button)\n",
    "                            time.sleep(0.5) # Wait for scroll\n",
    "                            # Use a short wait for clickability\n",
    "                            button_to_click = short_wait.until(EC.element_to_be_clickable(button))\n",
    "                            driver.execute_script(\"arguments[0].click();\", button_to_click) # JS click\n",
    "                            # button_to_click.click() # Regular click\n",
    "                            print(f\"    Clicked 'Expand Replies': {button.text[:20]}\")\n",
    "                            action_taken_this_loop = True\n",
    "                            clicked_an_expand_button_this_pass = True\n",
    "                            time.sleep(1.5) # Wait for replies to load\n",
    "                            # After clicking, the DOM might change, so we might need to re-find buttons in the next 'while True' iteration\n",
    "                        except StaleElementReferenceException: print(\"    Stale 'Expand Replies' link during click, will re-evaluate.\"); break # Break inner for to re-find\n",
    "                        except ElementNotInteractableException: print(\"    'Expand Replies' link not interactable, might be covered or disabled.\")\n",
    "                        except TimeoutException: print(\"    Timeout waiting for 'Expand Replies' to be clickable.\")\n",
    "                        except Exception as e_expand: print(f\"    Error clicking one 'Expand Replies': {e_expand}\")\n",
    "                    if not clicked_an_expand_button_this_pass: # No visible ones were successfully clicked\n",
    "                        break\n",
    "                except Exception as e_find_expand: print(f\"  Error finding 'Expand Replies': {e_find_expand}\"); break\n",
    "                if not clicked_an_expand_button_this_pass: break # If loop completes without any clicks\n",
    "\n",
    "            # 2. Scroll down\n",
    "            print(\"  Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            # 3. Attempt to scrape comments\n",
    "            comment_text_xpath = \"//div[@class='comment__item__main']/p\" # This XPath seems correct\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "                if comment_p_tags:\n",
    "                    # print(f\"  Found {len(comment_p_tags)} potential comment <p> tags on this pass.\")\n",
    "                    new_comments_this_pass = 0\n",
    "                    for p_tag in comment_p_tags:\n",
    "                        try:\n",
    "                            comment_text = p_tag.text.strip()\n",
    "                            if comment_text and comment_text not in unique_comment_texts_scraped:\n",
    "                                unique_comment_texts_scraped.add(comment_text)\n",
    "                                new_comments_this_pass += 1\n",
    "                        except StaleElementReferenceException: continue\n",
    "                        except Exception: continue # Ignore errors for individual comment text retrieval\n",
    "                    if new_comments_this_pass > 0:\n",
    "                        print(f\"    Added {new_comments_this_pass} new unique comments this pass.\")\n",
    "                        action_taken_this_loop = True # Finding new comments is an action\n",
    "            except Exception as e_find_comments: print(f\"  Error finding comment <p> tags: {e_find_comments}\")\n",
    "\n",
    "            # 4. Click \"展开查看更多\" (Load More Main Comments)\n",
    "            load_more_comments_xpath = \"//div[contains(@class,'more-comment') and (contains(., '展开查看更多') or contains(., '加载更多'))]\"\n",
    "            try:\n",
    "                # Use short_wait for this button as it might appear/disappear\n",
    "                load_more_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多/加载更多' button. Clicking...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", load_more_button); time.sleep(0.3)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                print(\"    Clicked '展开查看更多/加载更多'.\")\n",
    "                action_taken_this_loop = True\n",
    "                time.sleep(scroll_pause_time + 1) # Wait longer after this action\n",
    "            except TimeoutException: print(\"  '展开查看更多/加载更多' button not found or not clickable this pass.\")\n",
    "            except Exception as e_load_more: print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            print(f\"  Loop {i+1} end. Total unique comments: {len(unique_comment_texts_scraped)}. Scroll height: {current_height}. Last height: {last_height}\")\n",
    "\n",
    "            if not action_taken_this_loop and len(unique_comment_texts_scraped) == comments_found_before_interactions:\n",
    "                no_new_actions_or_comments_strikes += 1\n",
    "                print(f\"  No new actions or comments strike: {no_new_actions_or_comments_strikes}\")\n",
    "            else:\n",
    "                no_new_actions_or_comments_strikes = 0 # Reset if something happened\n",
    "\n",
    "            if no_new_actions_or_comments_strikes >= 2:\n",
    "                print(\"No new comments found and no interaction buttons successfully clicked for 2 consecutive loops. Assuming completion.\")\n",
    "                break\n",
    "\n",
    "            last_height = current_height\n",
    "            if i == max_main_loops - 1: print(\"Reached max main loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"main_loop_end_{i+1}.png\"))\n",
    "\n",
    "        scraped_data[\"comments\"] = list(unique_comment_texts_scraped)\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_and_all_comments(target_url, max_main_loops=10, scroll_pause_time=2.5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\")\n",
    "        print(data[\"post_content\"]) # Print full post content\n",
    "    else:\n",
    "        print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment in enumerate(data[\"comments\"]):\n",
    "            print(f\"{i+1}. {comment}\") # Print full comment\n",
    "    else:\n",
    "        print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Check console logs and folder for details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
