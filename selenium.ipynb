{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ef4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://selenium.dev/')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "507ab0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Incremental Scroll for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_incremental_scroll' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Body element loaded.\n",
      "Looking for the first pop-up ('跳过')...\n",
      "First pop-up ('跳过') not found or timed out.\n",
      "Looking for the second pop-up ('X')...\n",
      "Second pop-up ('X') not found or timed out.\n",
      "\n",
      "--- Starting Incremental Scroll and Screenshot Process ---\n",
      "--- Scroll Attempt #1 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_01_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 482, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #2 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_02_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 963, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #3 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_03_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 1445, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #4 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_04_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 1926, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #5 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_05_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 2408, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #6 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_06_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 2890, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #7 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_07_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 3371, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #8 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_08_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 3853, Viewport height: 602, Total scrollHeight: 4924\n",
      "--- Scroll Attempt #9 ---\n",
      "Saved screenshot: screenshots_incremental_scroll\\scroll_09_before.png\n",
      "Scrolling down using: window.scrollBy(0, window.innerHeight * 0.8);\n",
      "Pausing for 2.0 seconds...\n",
      "  Current scrollY: 4322, Viewport height: 602, Total scrollHeight: 4924\n",
      "Reached the bottom of the page.\n",
      "Saved final bottom screenshot: screenshots_incremental_scroll\\scroll_09_at_bottom.png\n",
      "\n",
      "--- Incremental Scroll and Screenshot Process Finished ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "--- Script Finished ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'screenshot_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    147\u001b[39m scroll_and_screenshot_by_distance(\n\u001b[32m    148\u001b[39m     target_url,\n\u001b[32m    149\u001b[39m     scroll_pause_time=\u001b[32m2.0\u001b[39m,\n\u001b[32m    150\u001b[39m     scroll_increment_js=\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollBy(0, window.innerHeight * 0.8);\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    151\u001b[39m     )\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Script Finished ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheck the \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mscreenshot_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m folder for screenshots.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'screenshot_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException\n",
    ")\n",
    "\n",
    "def scroll_and_screenshot_by_distance(url, scroll_pause_time=1.5, scroll_increment_js=\"window.scrollBy(0, window.innerHeight);\"):\n",
    "    \"\"\"\n",
    "    Navigates to a URL, handles popups, scrolls down in increments, taking a screenshot\n",
    "    at each step until the bottom of the page is reached.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the page to scroll.\n",
    "        scroll_pause_time (float): Time in seconds to wait after each scroll\n",
    "                                   for content to potentially load.\n",
    "        scroll_increment_js (str): JavaScript to execute for scrolling.\n",
    "                                   Default scrolls by one viewport height.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Keep browser visible to observe\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,800\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_incremental_scroll\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        # driver.maximize_window() # Maximize or set specific size above\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            print(\"Body element loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Page body did not become present within timeout.\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        # --- Handle Initial Pop-ups (Integrated robust logic) ---\n",
    "        try:\n",
    "            print(\"Looking for the first pop-up ('跳过')...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            print(\"Clicking '跳过'...\"); driver.execute_script(\"arguments[0].click();\", skip_button); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"First pop-up ('跳过') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error handling first pop-up: {e}\")\n",
    "        try:\n",
    "            print(\"Looking for the second pop-up ('X')...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//div[@class='xq-dialog-wrapper']//i[contains(@class,'close')]\", \"//i[contains(@class, 'cube-dialog-close')]\", \"//div[contains(@class, 'Modal_modal')]//i[contains(@class, 'Modal_closeIcon')]\", \"//button[@aria-label='Close']\" ]\n",
    "            close_button_found = False\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, xpath))); print(f\"Clicking second pop-up 'X'...\"); driver.execute_script(\"arguments[0].click();\", close_button); close_button_found = True; time.sleep(0.5); break\n",
    "                 except TimeoutException: continue\n",
    "                 except Exception: continue\n",
    "            if not close_button_found: print(\"Second pop-up ('X') not found or timed out.\")\n",
    "        except Exception as e: print(f\"Error during second pop-up handling: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # Optional: Click 'Comments' Tab\n",
    "        # try:\n",
    "        #     print(\"Looking for and clicking '评论' tab before scrolling...\")\n",
    "        #     tab_xpath = \"//span[text()='评论' and contains(@class,'tabs__item__title')]/ancestor::div[contains(@class,'tabs__item')] | //div[contains(@class, 'tabs__item') and .//span[text()='评论']] | //div[contains(@class, 'action-bar__item') and contains(., '评论')]\"\n",
    "        #     comments_tab_element = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, tab_xpath)))\n",
    "        #     driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', inline: 'nearest'});\", comments_tab_element)\n",
    "        #     time.sleep(0.5)\n",
    "        #     comments_tab_clickable = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, tab_xpath)))\n",
    "        #     driver.execute_script(\"arguments[0].click();\", comments_tab_clickable)\n",
    "        #     print(\"Clicked '评论' tab. Waiting before scroll...\")\n",
    "        #     time.sleep(2.5)\n",
    "        # except Exception as e_tab:\n",
    "        #     print(f\"Could not click '评论' tab before scrolling (maybe not needed or error): {e_tab}\")\n",
    "\n",
    "        print(\"\\n--- Starting Incremental Scroll and Screenshot Process ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_attempt = 0\n",
    "\n",
    "        while True:\n",
    "            scroll_attempt += 1\n",
    "            print(f\"--- Scroll Attempt #{scroll_attempt} ---\")\n",
    "\n",
    "            screenshot_path = os.path.join(screenshot_dir, f\"scroll_{scroll_attempt:02d}_before.png\")\n",
    "            try:\n",
    "                driver.save_screenshot(screenshot_path)\n",
    "                print(f\"Saved screenshot: {screenshot_path}\")\n",
    "            except Exception as e_ss:\n",
    "                print(f\"Failed to save screenshot {screenshot_path}: {e_ss}\")\n",
    "\n",
    "            print(f\"Scrolling down using: {scroll_increment_js}\")\n",
    "            driver.execute_script(scroll_increment_js)\n",
    "\n",
    "            print(f\"Pausing for {scroll_pause_time} seconds...\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            current_scroll_y = driver.execute_script(\"return window.pageYOffset || document.documentElement.scrollTop;\")\n",
    "            viewport_height = driver.execute_script(\"return window.innerHeight;\")\n",
    "            print(f\"  Current scrollY: {round(current_scroll_y)}, Viewport height: {round(viewport_height)}, Total scrollHeight: {new_height}\")\n",
    "\n",
    "            if current_scroll_y + viewport_height >= new_height - 10:\n",
    "                print(\"Reached the bottom of the page.\")\n",
    "                final_bottom_path = os.path.join(screenshot_dir, f\"scroll_{scroll_attempt:02d}_at_bottom.png\")\n",
    "                try: driver.save_screenshot(final_bottom_path)\n",
    "                except Exception as e_fin_ss: print(f\"Could not save final screenshot: {e_fin_ss}\")\n",
    "                print(f\"Saved final bottom screenshot: {final_bottom_path}\")\n",
    "                break\n",
    "            elif scroll_attempt > 50:\n",
    "                print(\"Reached max scroll attempts (50). Stopping.\")\n",
    "                break\n",
    "            last_height = new_height\n",
    "        print(\"\\n--- Incremental Scroll and Screenshot Process Finished ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        # --- CORRECTED INDENTATION FOR ERROR SCREENSHOT TRY-EXCEPT ---\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error_scroll_script.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved error screenshot: {error_ss_path}\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "        # --- END CORRECTION ---\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing the browser...\")\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Incremental Scroll for URL: {target_url} ---\")\n",
    "    scroll_and_screenshot_by_distance(\n",
    "        target_url,\n",
    "        scroll_pause_time=2.0,\n",
    "        scroll_increment_js=\"window.scrollBy(0, window.innerHeight * 0.8);\"\n",
    "        )\n",
    "    print(\"--- Script Finished ---\")\n",
    "    print(f\"Check the '{screenshot_dir}' folder for screenshots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6347192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content...\n",
      "Post content scraped (length: 627).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Loop/Scroll attempt #1 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags.\n",
      "    Added 2 new unique comments.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "--- Loop/Scroll attempt #2 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "--- Loop/Scroll attempt #3 ---\n",
      "Scrolling down...\n",
      "  Found 18 potential comment <p> tags.\n",
      "  Current total comments scraped: 2. Scroll height: 4924\n",
      "Scroll height unchanged and no new comments for 2 strikes. Assuming all loaded or stuck.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 2 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "...\n",
      "\n",
      "--- Comments (2) ---\n",
      "不一定会成日鬼那样的社会，天朝…\n",
      "社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'screenshot_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 197\u001b[39m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> No comments were scraped. <<<\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheck console logs and \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mscreenshot_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m folder for details.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'screenshot_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException\n",
    ")\n",
    "\n",
    "def scrape_post_and_comments_on_scroll(url, max_scroll_loops=15, scroll_pause_time=2.5):\n",
    "    \"\"\"\n",
    "    Navigates to a Xueqiu post, handles popups, scrolls to load comments,\n",
    "    and attempts to scrape them. Also clicks 'expand replies' and 'load more'.\n",
    "    \"\"\"\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        short_wait = WebDriverWait(driver, 7)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load. Page might be different.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\") # Confirmation\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "        # --- End Pop-up Handling ---\n",
    "\n",
    "        # Scrape Main Post Content\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        no_new_content_strikes = 0\n",
    "\n",
    "        for i in range(max_scroll_loops):\n",
    "            print(f\"--- Loop/Scroll attempt #{i+1} ---\")\n",
    "            initial_comment_count = len(unique_comment_texts_scraped)\n",
    "\n",
    "            # 1. Click \"查看N条回复\"\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            try:\n",
    "                expand_buttons = driver.find_elements(By.XPATH, expand_reply_xpath)\n",
    "                if expand_buttons:\n",
    "                    print(f\"Found {len(expand_buttons)} 'Expand Replies' links.\")\n",
    "                    for button_idx, button in enumerate(expand_buttons):\n",
    "                        try:\n",
    "                            if button.is_displayed():\n",
    "                                print(f\"  Clicking 'Expand Replies' #{button_idx+1}...\"); driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button); time.sleep(0.3)\n",
    "                                driver.execute_script(\"arguments[0].click();\", button); time.sleep(1.5)\n",
    "                        except StaleElementReferenceException: print(\"  Stale 'Expand Replies' link, skipping.\")\n",
    "                        except ElementNotInteractableException: print(\"  'Expand Replies' link not interactable, skipping.\")\n",
    "                        except Exception as e_expand: print(f\"  Error clicking 'Expand Replies': {e_expand}\")\n",
    "            except Exception as e_find_expand: print(f\"Could not search for 'Expand Replies' buttons: {e_find_expand}\")\n",
    "\n",
    "            # 2. Scroll down\n",
    "            print(\"Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight + 500);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            # 3. Attempt to scrape comments\n",
    "            comment_text_xpath = \"//div[@class='comment__item__main']/p\"\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "                if comment_p_tags:\n",
    "                    print(f\"  Found {len(comment_p_tags)} potential comment <p> tags.\")\n",
    "                    new_comments_found_this_pass = 0\n",
    "                    for p_tag in comment_p_tags:\n",
    "                        try:\n",
    "                            comment_text = p_tag.text.strip()\n",
    "                            if comment_text and comment_text not in unique_comment_texts_scraped:\n",
    "                                unique_comment_texts_scraped.add(comment_text)\n",
    "                                new_comments_found_this_pass +=1\n",
    "                        except StaleElementReferenceException: continue\n",
    "                        except Exception as e_text: print(f\"    Error getting text from a p_tag: {e_text}\")\n",
    "                    if new_comments_found_this_pass > 0: print(f\"    Added {new_comments_found_this_pass} new unique comments.\")\n",
    "            except Exception as e_find: print(f\"  Error finding comment <p> tags: {e_find}\")\n",
    "\n",
    "            # 4. Click \"展开查看更多\"\n",
    "            load_more_comments_xpath = \"//div[contains(@class,'more-comment') and (contains(., '展开查看更多') or contains(., '加载更多'))]\"\n",
    "            try:\n",
    "                load_more_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多/加载更多' button. Clicking...\"); driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button); time.sleep(0.3)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button); print(\"  Clicked '展开查看更多/加载更多'.\"); time.sleep(scroll_pause_time)\n",
    "            except TimeoutException: pass\n",
    "            except Exception as e_load_more: print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            print(f\"  Current total comments scraped: {len(unique_comment_texts_scraped)}. Scroll height: {current_height}\")\n",
    "            if len(unique_comment_texts_scraped) > initial_comment_count: no_new_content_strikes = 0\n",
    "            else: no_new_content_strikes += 1\n",
    "\n",
    "            if current_height == last_height and no_new_content_strikes >= 2 :\n",
    "                print(\"Scroll height unchanged and no new comments for 2 strikes. Assuming all loaded or stuck.\")\n",
    "                break\n",
    "            elif no_new_content_strikes >= 3:\n",
    "                print(\"No new comments found for 3 consecutive strikes. Assuming all loaded.\")\n",
    "                break\n",
    "            last_height = current_height\n",
    "            if i == max_scroll_loops -1 : print(\"Reached max scroll loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"loop_end_{i+1}.png\"))\n",
    "\n",
    "        scraped_data[\"comments\"] = list(unique_comment_texts_scraped)\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        # --- CORRECTED INDENTATION FOR ERROR SCREENSHOT TRY-EXCEPT ---\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "        # --- END CORRECTION ---\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing the browser...\")\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_and_comments_on_scroll(target_url, max_scroll_loops=10, scroll_pause_time=3.0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\")\n",
    "        print(data[\"post_content\"][:500] + \"...\" if len(data[\"post_content\"]) > 500 else data[\"post_content\"])\n",
    "    else:\n",
    "        print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment in enumerate(data[\"comments\"][:20]): # Print first 20\n",
    "            print(f\"{i+1}. {comment[:150]}\" + \"...\" if len(comment)>150 else comment)\n",
    "        if len(data[\"comments\"]) > 20:\n",
    "            print(f\"... and {len(data['comments']) - 20} more comments.\")\n",
    "    else:\n",
    "        print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Check console logs and '{screenshot_dir}' folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f0ee95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_post_all_comments' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content...\n",
      "Post content scraped (length: 706).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Main Loop Iteration #1 ---\n",
      "  Scrolling down...\n",
      "    Added 18 new unique comments this pass.\n",
      "  '展开查看更多/加载更多' button not found or not clickable this pass.\n",
      "  Loop 1 end. Total unique comments: 18. Scroll height: 4924. Last height: 4924\n",
      "--- Main Loop Iteration #2 ---\n",
      "  Scrolling down...\n",
      "  '展开查看更多/加载更多' button not found or not clickable this pass.\n",
      "  Loop 2 end. Total unique comments: 18. Scroll height: 4924. Last height: 4924\n",
      "  No new actions or comments strike: 1\n",
      "--- Main Loop Iteration #3 ---\n",
      "  Scrolling down...\n",
      "  '展开查看更多/加载更多' button not found or not clickable this pass.\n",
      "  Loop 3 end. Total unique comments: 18. Scroll height: 4924. Last height: 4924\n",
      "  No new actions or comments strike: 2\n",
      "No new comments found and no interaction buttons successfully clicked for 2 consecutive loops. Assuming completion.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 18 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "零食甚至“宠物饮品”一应俱全\n",
      "。\n",
      "独酌时\n",
      "，\n",
      "希望宠物也能“陪一杯”\n",
      "。\n",
      "\n",
      "3️⃣ 陪伴型机器人：技术不卖功能\n",
      "，\n",
      "卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT\n",
      "，\n",
      "不扫地\n",
      "、\n",
      "不聊天\n",
      "、\n",
      "不能干活——但它会“撒娇”\n",
      "、\n",
      "会“跟着你”\n",
      "、\n",
      "会“让你想抱”\n",
      "。\n",
      "\n",
      "这是一台卖情绪价值的机器人\n",
      "。\n",
      "\n",
      "$恒生指数(HKHSI)$ $上证指数(SH000001)$ $招商银行(SH600036)$\n",
      "\n",
      "--- Comments (18) ---\n",
      "1. 神经，愚蠢没有逻辑，人类是群居动物，它们渴望伴侣，阶段性的单身是因为经济原因，因为穷，男的穷养不起家，结不了婚，女的穷，养不起自己，并且找不到能养起自己的人，所以不嫁。\n",
      "未来随着机器人人工智能，经济指数级增长，人类会涨校园里的大学生一样无忧无虑，男人和女人因为爱情成群结队。\n",
      "2. 有个手机就够了，老公老婆都可以不要\n",
      "3. 十年二十年后，大概也这样了，会出什么龙头公司股票呢？\n",
      "4. 总需求停滞，新需求就是存量竞争，从物质需求到精神需求，从奢靡需求到精简需求，从传统需求到新型，个性需求。。但事实上，看看日本的股市龙头，这些需求转向基本做不大，市值最大的还是一些大企业，银行，保险，电信，商社，汽车，电气，半导体，互联网；到了中国大概率还是中字头，资源类，制造业龙头，龙头科创互联网。。\n",
      "5. 不婚不育最根本原因是上世纪不准生，一旦生态链损毁，修复何其难。\n",
      "6. 没有人喜欢照顾别人的情绪，但是所有人都想从别人那里找到情绪价值。\n",
      "7. 我这段时间的观察与思考和这部分内容是一致的。我女儿最喜欢的便利店是全家Family Mart，里面的海苔饭团、咖喱鸡饭都是冷藏食品，她一点不嫌弃，反而很热爱，还兴高采烈地告诉我，她很喜欢这些食品。\n",
      "宠物这一部分我前段时间也在关注，今早还发了一条有关于此的消息。\n",
      "五一节期间和一位50岁的大哥聊天，他说令他意外的是，成都天府广场一座被传统零售商业模式淘汰的商场，现在被谷子经济救活了，里面人山人海，交易量火爆。\n",
      "8. 不可能的。两个国家本质上的价值观完全不一样。我身边蛮多00后，目前没听到一个说30以后不结婚的。穷女和稳定男是最迫切想结婚的人群。这还是经常打游戏的宅群。\n",
      "9. 我们的社会确实朝着这个方向走\n",
      "个人取代家庭，成为社会的最小单元了\n",
      "不管我们承不承认，年轻人的消费习惯和我们完全不一样了\n",
      "$泡泡玛特(09992)$\n",
      "10. 性爱机器人产品会供不应求，这类公司会卷出一个品牌。\n",
      "11. 不一定会成日鬼那样的社会，天朝…\n",
      "12. 我们这现在最先来的应该是老年经济\n",
      "13. 内卷国家的必然\n",
      "14. 如果真能成为日本那样的发达国家就知足吧、优衣库2000多美刀的收入，物价再贵去掉吃喝拉撒也能一个月换个苹果手机了，这就是我们这里看不起的售货员.\n",
      "15. 东亚社畜主义社会\n",
      "16. 文明的诅咒\n",
      "17. 社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "18. 中国人均财富水平距离日本还差的太远，更大的可能性是未富先老，或许更应该思考的是一大堆拮据的老年人最可能消费什么\n",
      "\n",
      "==============================\n",
      "Check console logs and folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def scrape_post_and_all_comments(url, max_main_loops=15, scroll_pause_time=2.5):\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_all_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        short_wait = WebDriverWait(driver, 5) # Shorter wait for elements within loops\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "\n",
    "        # Scrape Main Post Content\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        no_new_actions_or_comments_strikes = 0\n",
    "\n",
    "        for i in range(max_main_loops):\n",
    "            print(f\"--- Main Loop Iteration #{i+1} ---\")\n",
    "            action_taken_this_loop = False\n",
    "            comments_found_before_interactions = len(unique_comment_texts_scraped)\n",
    "\n",
    "            # 1. Click ALL visible \"查看N条回复\" (Expand Replies)\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            # Loop to click these as new ones might appear after expanding others\n",
    "            while True:\n",
    "                clicked_an_expand_button_this_pass = False\n",
    "                try:\n",
    "                    expand_buttons = driver.find_elements(By.XPATH, expand_reply_xpath)\n",
    "                    if not expand_buttons: # print(\"  No 'Expand Replies' links found this pass.\");\n",
    "                        break\n",
    "\n",
    "                    # Filter only visible buttons before attempting to click\n",
    "                    visible_expand_buttons = [b for b in expand_buttons if b.is_displayed()]\n",
    "                    if not visible_expand_buttons: # print(\"  No *visible* 'Expand Replies' links.\");\n",
    "                        break\n",
    "\n",
    "                    print(f\"  Found {len(visible_expand_buttons)} visible 'Expand Replies' links to click.\")\n",
    "                    for button in visible_expand_buttons:\n",
    "                        try:\n",
    "                            # print(\"    Scrolling to 'Expand Replies' and clicking...\")\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", button)\n",
    "                            time.sleep(0.5) # Wait for scroll\n",
    "                            # Use a short wait for clickability\n",
    "                            button_to_click = short_wait.until(EC.element_to_be_clickable(button))\n",
    "                            driver.execute_script(\"arguments[0].click();\", button_to_click) # JS click\n",
    "                            # button_to_click.click() # Regular click\n",
    "                            print(f\"    Clicked 'Expand Replies': {button.text[:20]}\")\n",
    "                            action_taken_this_loop = True\n",
    "                            clicked_an_expand_button_this_pass = True\n",
    "                            time.sleep(1.5) # Wait for replies to load\n",
    "                            # After clicking, the DOM might change, so we might need to re-find buttons in the next 'while True' iteration\n",
    "                        except StaleElementReferenceException: print(\"    Stale 'Expand Replies' link during click, will re-evaluate.\"); break # Break inner for to re-find\n",
    "                        except ElementNotInteractableException: print(\"    'Expand Replies' link not interactable, might be covered or disabled.\")\n",
    "                        except TimeoutException: print(\"    Timeout waiting for 'Expand Replies' to be clickable.\")\n",
    "                        except Exception as e_expand: print(f\"    Error clicking one 'Expand Replies': {e_expand}\")\n",
    "                    if not clicked_an_expand_button_this_pass: # No visible ones were successfully clicked\n",
    "                        break\n",
    "                except Exception as e_find_expand: print(f\"  Error finding 'Expand Replies': {e_find_expand}\"); break\n",
    "                if not clicked_an_expand_button_this_pass: break # If loop completes without any clicks\n",
    "\n",
    "            # 2. Scroll down\n",
    "            print(\"  Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            # 3. Attempt to scrape comments\n",
    "            comment_text_xpath = \"//div[@class='comment__item__main']/p\" # This XPath seems correct\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "                if comment_p_tags:\n",
    "                    # print(f\"  Found {len(comment_p_tags)} potential comment <p> tags on this pass.\")\n",
    "                    new_comments_this_pass = 0\n",
    "                    for p_tag in comment_p_tags:\n",
    "                        try:\n",
    "                            comment_text = p_tag.text.strip()\n",
    "                            if comment_text and comment_text not in unique_comment_texts_scraped:\n",
    "                                unique_comment_texts_scraped.add(comment_text)\n",
    "                                new_comments_this_pass += 1\n",
    "                        except StaleElementReferenceException: continue\n",
    "                        except Exception: continue # Ignore errors for individual comment text retrieval\n",
    "                    if new_comments_this_pass > 0:\n",
    "                        print(f\"    Added {new_comments_this_pass} new unique comments this pass.\")\n",
    "                        action_taken_this_loop = True # Finding new comments is an action\n",
    "            except Exception as e_find_comments: print(f\"  Error finding comment <p> tags: {e_find_comments}\")\n",
    "\n",
    "            # 4. Click \"展开查看更多\" (Load More Main Comments)\n",
    "            load_more_comments_xpath = \"//div[contains(@class,'more-comment') and (contains(., '展开查看更多') or contains(., '加载更多'))]\"\n",
    "            try:\n",
    "                # Use short_wait for this button as it might appear/disappear\n",
    "                load_more_button = short_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多/加载更多' button. Clicking...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", load_more_button); time.sleep(0.3)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                print(\"    Clicked '展开查看更多/加载更多'.\")\n",
    "                action_taken_this_loop = True\n",
    "                time.sleep(scroll_pause_time + 1) # Wait longer after this action\n",
    "            except TimeoutException: print(\"  '展开查看更多/加载更多' button not found or not clickable this pass.\")\n",
    "            except Exception as e_load_more: print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            print(f\"  Loop {i+1} end. Total unique comments: {len(unique_comment_texts_scraped)}. Scroll height: {current_height}. Last height: {last_height}\")\n",
    "\n",
    "            if not action_taken_this_loop and len(unique_comment_texts_scraped) == comments_found_before_interactions:\n",
    "                no_new_actions_or_comments_strikes += 1\n",
    "                print(f\"  No new actions or comments strike: {no_new_actions_or_comments_strikes}\")\n",
    "            else:\n",
    "                no_new_actions_or_comments_strikes = 0 # Reset if something happened\n",
    "\n",
    "            if no_new_actions_or_comments_strikes >= 2:\n",
    "                print(\"No new comments found and no interaction buttons successfully clicked for 2 consecutive loops. Assuming completion.\")\n",
    "                break\n",
    "\n",
    "            last_height = current_height\n",
    "            if i == max_main_loops - 1: print(\"Reached max main loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"main_loop_end_{i+1}.png\"))\n",
    "\n",
    "        scraped_data[\"comments\"] = list(unique_comment_texts_scraped)\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_and_all_comments(target_url, max_main_loops=10, scroll_pause_time=2.5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\")\n",
    "        print(data[\"post_content\"]) # Print full post content\n",
    "    else:\n",
    "        print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment in enumerate(data[\"comments\"]):\n",
    "            print(f\"{i+1}. {comment}\") # Print full comment\n",
    "    else:\n",
    "        print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Check console logs and folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "511f1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content...\n",
      "Post content scraped (length: 706).\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Main Loop Iteration #1 ---\n",
      "  Scrolling down...\n",
      "    Added 18 new unique comments from page.\n",
      "  Found '展开查看更多' button. Clicking...\n",
      "    Clicked '展开查看更多'.\n",
      "  Loop 1 end. Total unique comments: 18. Previously: -1\n",
      "--- Main Loop Iteration #2 ---\n",
      "  Scrolling down...\n",
      "    Added 15 new unique comments from page.\n",
      "  '展开查看更多' button not found or not clickable this pass (might be all loaded).\n",
      "  Loop 2 end. Total unique comments: 33. Previously: 18\n",
      "--- Main Loop Iteration #3 ---\n",
      "  Scrolling down...\n",
      "  '展开查看更多' button not found or not clickable this pass (might be all loaded).\n",
      "  Loop 3 end. Total unique comments: 33. Previously: 33\n",
      "No actions taken (no buttons clicked, no new comments found) and comment count unchanged. Assuming completion.\n",
      "\n",
      "--- Finished comment scraping. Total unique comments: 33 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "零食甚至“宠物饮品”一应俱全\n",
      "。\n",
      "独酌时\n",
      "，\n",
      "希望宠物也能“陪一杯”\n",
      "。\n",
      "\n",
      "3️⃣ 陪伴型机器人：技术不卖功能\n",
      "，\n",
      "卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT\n",
      "，\n",
      "不扫地\n",
      "、\n",
      "不聊天\n",
      "、\n",
      "不能干活——但它会“撒娇”\n",
      "、\n",
      "会“跟着你”\n",
      "、\n",
      "会“让你想抱”\n",
      "。\n",
      "\n",
      "这是一台卖情绪价值的机器人\n",
      "。\n",
      "\n",
      "$恒生指数(HKHSI)$ $上证指数(SH000001)$ $招商银行(SH600036)$\n",
      "\n",
      "--- Comments (33) ---\n",
      "1. 我这段时间的观察与思考和这部分内容是一致的。我女儿最喜欢的便利店是全家Family Mart，里面的海苔饭团、咖喱鸡饭都是冷藏食品，她一点不嫌弃，反而很热爱，还兴高采烈地告诉我，她很喜欢这些食品。\n",
      "宠物这一部分我前段时间也在关注，今早还发了一条有关于此的消息。\n",
      "五一节期间和一位50岁的大哥聊天，他说令他意外的是，成都天府广场一座被传统零售商业模式淘汰的商场，现在被谷子经济救活了，里面人山人海，交易量火爆。\n",
      "2. 不可能的。两个国家本质上的价值观完全不一样。我身边蛮多00后，目前没听到一个说30以后不结婚的。穷女和稳定男是最迫切想结婚的人群。这还是经常打游戏的宅群。\n",
      "3. 干宠物经济就行了\n",
      "4. 不婚不育最根本原因是上世纪不准生，一旦生态链损毁，修复何其难。\n",
      "5. 中国人均财富水平距离日本还差的太远，更大的可能性是未富先老，或许更应该思考的是一大堆拮据的老年人最可能消费什么\n",
      "6. 我们的社会确实朝着这个方向走\n",
      "个人取代家庭，成为社会的最小单元了\n",
      "不管我们承不承认，年轻人的消费习惯和我们完全不一样了\n",
      "$泡泡玛特(09992)$\n",
      "7. 情绪经济啊\n",
      "8. 内卷国家的必然\n",
      "9. AI娃娃\n",
      "10. 东亚社畜主义社会\n",
      "11. 文明的诅咒\n",
      "12. 社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "13. 神经，愚蠢没有逻辑，人类是群居动物，它们渴望伴侣，阶段性的单身是因为经济原因，因为穷，男的穷养不起家，结不了婚，女的穷，养不起自己，并且找不到能养起自己的人，所以不嫁。\n",
      "未来随着机器人人工智能，经济指数级增长，人类会涨校园里的大学生一样无忧无虑，男人和女人因为爱情成群结队。\n",
      "14. 不一定会成日鬼那样的社会，天朝…\n",
      "15. 没有人喜欢照顾别人的情绪，但是所有人都想从别人那里找到情绪价值。\n",
      "16. 性爱机器人产品会供不应求，这类公司会卷出一个品牌。\n",
      "17. 手动转存\n",
      "18. 养娃成本太高了，养差了自己又觉得对不起孩子，还不如不生。我亲戚去帮人补课，机构收1200一节课，她自己得100块，这种价格很多家庭都是难以承受的。各种培训班真的该好好打一下，往死里打。不然就会引起人的攀比心理，越攀比越累，干脆不生。\n",
      "19. 圣杯\n",
      "20. 如果真能成为日本那样的发达国家就知足吧、优衣库2000多美刀的收入，物价再贵去掉吃喝拉撒也能一个月换个苹果手机了，这就是我们这里看不起的售货员.\n",
      "21. mark\n",
      "22. 转发。感觉陪伴型机器人在AI时代有巨大的发展潜力。\n",
      "23. 感觉怡红院很快会出牌照？？\n",
      "24. 总需求停滞，新需求就是存量竞争，从物质需求到精神需求，从奢靡需求到精简需求，从传统需求到新型，个性需求。。但事实上，看看日本的股市龙头，这些需求转向基本做不大，市值最大的还是一些大企业，银行，保险，电信，商社，汽车，电气，半导体，互联网；到了中国大概率还是中字头，资源类，制造业龙头，龙头科创互联网。。\n",
      "25. 转：\n",
      "1980年代，日本一人户占比只有20%，如今逼近40%。未来的日本将成为半数人口是单身的“超级单身社会”。\n",
      "而在中国，独居、晚婚、不婚人群也正快速上升。\n",
      "这不是偶然，而是结构性变化。\n",
      "一个人生活，意味着从吃饭、出行、情感，到陪伴、安全感，都要独自完成。这背后，藏着海量“新需求”和“新供给”。\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭、饭团、即食热食，成了日本最大的餐饮品牌。\n",
      "2023财年营收破10万亿日元（约5000亿人民币），稳压传统连锁餐饮。\n",
      "东京的面馆、咖啡馆大量设置“一个人座位”，配隔板、配平板，边吃边追剧，社交压力为零——孤独，是可以被尊重的生活方式。\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万，远超15岁以下儿童。孤独都市人，把情感投射给了猫狗。\n",
      "日本宠物主对待他们的宠物就像对待家人一样，甚至比自己的地位还高。宠物用品已全面“母婴化”：推车、衣服、零食甚至“宠物饮品”一应俱全。独酌时，希望宠物也能“陪一杯”。\n",
      "3️⃣ 陪伴型机器人：技术不卖功能，卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT，不扫地、不聊天、不能干活——但它会“撒娇”、会“跟着你”、会“让你想抱”。\n",
      "这是一台卖情绪价值的机器人\n",
      "26. 转\n",
      "27. 存\n",
      "28. 为宠物股炒作提供注释\n",
      "29. 有个手机就够了，老公老婆都可以不要\n",
      "30. 十年二十年后，大概也这样了，会出什么龙头公司股票呢？\n",
      "31. 东达不可避免\n",
      "32. 我们这现在最先来的应该是老年经济\n",
      "33. 利好养老类股票。\n",
      "\n",
      "==============================\n",
      "Check console logs and folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def scrape_post_and_all_comments(url, max_main_loops=20, scroll_pause_time=2.5): # Increased max_main_loops\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_all_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        # Shorter wait for elements that appear/disappear within loops\n",
    "        interaction_wait = WebDriverWait(driver, 7)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "\n",
    "        # Scrape Main Post Content\n",
    "        try:\n",
    "            print(\"Scraping main post content...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_total_comments = -1 # Initialize to a value different from initial count\n",
    "\n",
    "        for i in range(max_main_loops):\n",
    "            print(f\"--- Main Loop Iteration #{i+1} ---\")\n",
    "            action_taken_this_loop = False\n",
    "            comments_at_loop_start = len(unique_comment_texts_scraped)\n",
    "\n",
    "            # 1. Click ALL visible \"查看N条回复\" (Expand Replies)\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复')]\"\n",
    "            # Inner loop to keep clicking expand replies as long as new ones appear or are clickable\n",
    "            expand_attempts = 0\n",
    "            while expand_attempts < 5: # Limit attempts to avoid infinite loop if something goes wrong\n",
    "                expand_attempts += 1\n",
    "                clicked_an_expand_button_this_pass = False\n",
    "                try:\n",
    "                    # Re-find elements each time as DOM changes\n",
    "                    visible_expand_buttons = [b for b in driver.find_elements(By.XPATH, expand_reply_xpath) if b.is_displayed()]\n",
    "                    if not visible_expand_buttons: break # No more visible expand buttons\n",
    "\n",
    "                    print(f\"  Found {len(visible_expand_buttons)} visible 'Expand Replies' links.\")\n",
    "                    for button in visible_expand_buttons:\n",
    "                        try:\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", button); time.sleep(0.4)\n",
    "                            # Using a short wait for the specific button to ensure it's ready\n",
    "                            button_to_click = WebDriverWait(driver, 3).until(EC.element_to_be_clickable(button))\n",
    "                            driver.execute_script(\"arguments[0].click();\", button_to_click)\n",
    "                            print(f\"    Clicked 'Expand Replies': {button.text[:20]}\")\n",
    "                            action_taken_this_loop = True\n",
    "                            clicked_an_expand_button_this_pass = True\n",
    "                            time.sleep(1.5) # Wait for replies\n",
    "                        except (StaleElementReferenceException, TimeoutException, ElementNotInteractableException): continue # Try next button or re-evaluate\n",
    "                        except Exception as e_expand: print(f\"    Error clicking one 'Expand Replies': {e_expand}\")\n",
    "                    if not clicked_an_expand_button_this_pass: break # No more were clicked this pass\n",
    "                except Exception as e_find_expand: print(f\"  Error finding 'Expand Replies': {e_find_expand}\"); break\n",
    "\n",
    "\n",
    "            # 2. Scroll down (helps reveal \"Load More\" and more comments)\n",
    "            print(\"  Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time) # Allow content to load after scroll\n",
    "\n",
    "            # 3. Attempt to scrape comments (after potential expansions and scroll)\n",
    "            comment_text_xpath = \"//div[@class='comment__item__main']/p\"\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_text_xpath)\n",
    "                new_comments_this_pass = 0\n",
    "                for p_tag in comment_p_tags:\n",
    "                    try:\n",
    "                        comment_text = p_tag.text.strip()\n",
    "                        if comment_text and comment_text not in unique_comment_texts_scraped:\n",
    "                            unique_comment_texts_scraped.add(comment_text)\n",
    "                            new_comments_this_pass += 1\n",
    "                    except StaleElementReferenceException: continue\n",
    "                if new_comments_this_pass > 0:\n",
    "                    print(f\"    Added {new_comments_this_pass} new unique comments from page.\")\n",
    "                    action_taken_this_loop = True\n",
    "            except Exception as e_find_comments: print(f\"  Error finding comment <p> tags: {e_find_comments}\")\n",
    "\n",
    "\n",
    "            # 4. Click \"展开查看更多\" (Load More Main Comments)\n",
    "            # UPDATED XPATH based on your screenshot:\n",
    "            load_more_comments_xpath = \"//a[@class='show_more' and .//span[text()='展开查看更多']]\"\n",
    "            try:\n",
    "                # Use interaction_wait for this button\n",
    "                load_more_button = interaction_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多' button. Clicking...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", load_more_button); time.sleep(0.4)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                print(\"    Clicked '展开查看更多'.\")\n",
    "                action_taken_this_loop = True\n",
    "                time.sleep(scroll_pause_time + 0.5) # Wait longer after this significant action\n",
    "            except TimeoutException:\n",
    "                print(\"  '展开查看更多' button not found or not clickable this pass (might be all loaded).\")\n",
    "            except Exception as e_load_more:\n",
    "                print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_total_comments = len(unique_comment_texts_scraped)\n",
    "            print(f\"  Loop {i+1} end. Total unique comments: {current_total_comments}. Previously: {last_total_comments}\")\n",
    "\n",
    "            if not action_taken_this_loop and current_total_comments == last_total_comments:\n",
    "                print(\"No actions taken (no buttons clicked, no new comments found) and comment count unchanged. Assuming completion.\")\n",
    "                break\n",
    "            \n",
    "            last_total_comments = current_total_comments\n",
    "            if i == max_main_loops - 1: print(\"Reached max main loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"main_loop_end_{i+1}.png\"))\n",
    "\n",
    "        scraped_data[\"comments\"] = list(unique_comment_texts_scraped)\n",
    "        print(f\"\\n--- Finished comment scraping. Total unique comments: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "\n",
    "    data = scrape_post_and_all_comments(target_url, max_main_loops=20, scroll_pause_time=2.5) # Increased max_main_loops\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\"); print(data[\"post_content\"])\n",
    "    else: print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment in enumerate(data[\"comments\"]): print(f\"{i+1}. {comment}\")\n",
    "    else: print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30); print(f\"Check console logs and folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcb50a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 ---\n",
      "Setting up WebDriver...\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content and author ID...\n",
      "Post content scraped (length: 706).\n",
      "Post author ID scraped: 5669998349\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Main Loop Iteration #1 ---\n",
      "  Scrolling down...\n",
      "    Found 19 potential comment <p> tags.\n",
      "    Added 19 new unique comments to data list.\n",
      "  Found '展开查看更多' button. Clicking...\n",
      "    Clicked '展开查看更多'.\n",
      "  Loop 1 end. Total unique comment texts scraped: 19. Previously: -1. Total comments in list: 19\n",
      "--- Main Loop Iteration #2 ---\n",
      "  Scrolling down...\n",
      "    Found 33 potential comment <p> tags.\n",
      "    Added 14 new unique comments to data list.\n",
      "  '展开查看更多' button not found or not clickable this pass (might be all loaded).\n",
      "  Loop 2 end. Total unique comment texts scraped: 33. Previously: 19. Total comments in list: 33\n",
      "--- Main Loop Iteration #3 ---\n",
      "  Scrolling down...\n",
      "    Found 33 potential comment <p> tags.\n",
      "  '展开查看更多' button not found or not clickable this pass (might be all loaded).\n",
      "  Loop 3 end. Total unique comment texts scraped: 33. Previously: 33. Total comments in list: 33\n",
      "No actions taken and unique comment text count unchanged. Assuming completion.\n",
      "\n",
      "--- Finished comment scraping. Total comments in list: 33 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "Author ID: 5669998349\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "零食甚至“宠物饮品”一应俱全\n",
      "。\n",
      "独酌时\n",
      "，\n",
      "希望宠物也能“陪一杯”\n",
      "。\n",
      "\n",
      "3️⃣ 陪伴型机器人：技术不卖功能\n",
      "，\n",
      "卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT\n",
      "，\n",
      "不扫地\n",
      "、\n",
      "不聊天\n",
      "、\n",
      "不能干活——但它会“撒娇”\n",
      "、\n",
      "会“跟着你”\n",
      "、\n",
      "会“让你想抱”\n",
      "。\n",
      "\n",
      "这是一台卖情绪价值的机器人\n",
      "。\n",
      "\n",
      "$恒生指数(HKHSI)$ $上证指数(SH000001)$ $招商银行(SH600036)$\n",
      "\n",
      "--- Comments (33) ---\n",
      "1. Author ID: None, Comment: 十年二十年后，大概也这样了，会出什么龙头公司股票呢？\n",
      "2. Author ID: None, Comment: 没有人喜欢照顾别人的情绪，但是所有人都想从别人那里找到情绪价值。\n",
      "3. Author ID: None, Comment: 神经，愚蠢没有逻辑，人类是群居动物，它们渴望伴侣，阶段性的单身是因为经济原因，因为穷，男的穷养不起家，结不了婚，女的穷，养不起自己，并且找不到能养起自己的人，所以不嫁。\n",
      "未来随着机器人人工智能，经济指数级增长，人类会涨校园里的大学生一样无忧无虑，男人和女人因为爱情成群结队。\n",
      "4. Author ID: None, Comment: 我们这现在最先来的应该是老年经济\n",
      "5. Author ID: None, Comment: 总需求停滞，新需求就是存量竞争，从物质需求到精神需求，从奢靡需求到精简需求，从传统需求到新型，个性需求。。但事实上，看看日本的股市龙头，这些需求转向基本做不大，市值最大的还是一些大企业，银行，保险，电信，商社，汽车，电气，半导体，互联网；到了中国大概率还是中字头，资源类，制造业龙头，龙头科创互联网。。\n",
      "6. Author ID: None, Comment: 我们的社会确实朝着这个方向走\n",
      "个人取代家庭，成为社会的最小单元了\n",
      "不管我们承不承认，年轻人的消费习惯和我们完全不一样了\n",
      "$泡泡玛特(09992)$\n",
      "7. Author ID: None, Comment: 内卷国家的必然\n",
      "8. Author ID: None, Comment: 有个手机就够了，老公老婆都可以不要\n",
      "9. Author ID: None, Comment: 不可能的。两个国家本质上的价值观完全不一样。我身边蛮多00后，目前没听到一个说30以后不结婚的。穷女和稳定男是最迫切想结婚的人群。这还是经常打游戏的宅群。\n",
      "10. Author ID: None, Comment: 文明的诅咒\n",
      "11. Author ID: None, Comment: 如果真能成为日本那样的发达国家就知足吧、优衣库2000多美刀的收入，物价再贵去掉吃喝拉撒也能一个月换个苹果手机了，这就是我们这里看不起的售货员.\n",
      "12. Author ID: None, Comment: 对于不婚比例上升这个现象，思考两个原因：1.这是不是阶层固化的症状？没背景的年轻人对于未来失去希望和想象，然后躺平。2.在男女平等的舆论导向上，过度宣扬女权，抬高女性地位，造成青年男女对立？\n",
      "13. Author ID: None, Comment: 不一定会成日鬼那样的社会，天朝…\n",
      "14. Author ID: None, Comment: 我这段时间的观察与思考和这部分内容是一致的。我女儿最喜欢的便利店是全家Family Mart，里面的海苔饭团、咖喱鸡饭都是冷藏食品，她一点不嫌弃，反而很热爱，还兴高采烈地告诉我，她很喜欢这些食品。\n",
      "宠物这一部分我前段时间也在关注，今早还发了一条有关于此的消息。\n",
      "五一节期间和一位50岁的大哥聊天，他说令他意外的是，成都天府广场一座被传统零售商业模式淘汰的商场，现在被谷子经济救活了，里面人山人海，交易量火爆。\n",
      "15. Author ID: None, Comment: 性爱机器人产品会供不应求，这类公司会卷出一个品牌。\n",
      "16. Author ID: None, Comment: 中国人均财富水平距离日本还差的太远，更大的可能性是未富先老，或许更应该思考的是一大堆拮据的老年人最可能消费什么\n",
      "17. Author ID: None, Comment: 不婚不育最根本原因是上世纪不准生，一旦生态链损毁，修复何其难。\n",
      "18. Author ID: None, Comment: 社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "19. Author ID: None, Comment: 圣杯\n",
      "20. Author ID: None, Comment: 情绪经济啊\n",
      "21. Author ID: None, Comment: 感觉怡红院很快会出牌照？？\n",
      "22. Author ID: None, Comment: 养娃成本太高了，养差了自己又觉得对不起孩子，还不如不生。我亲戚去帮人补课，机构收1200一节课，她自己得100块，这种价格很多家庭都是难以承受的。各种培训班真的该好好打一下，往死里打。不然就会引起人的攀比心理，越攀比越累，干脆不生。\n",
      "23. Author ID: None, Comment: 手动转存\n",
      "24. Author ID: None, Comment: 干宠物经济就行了\n",
      "25. Author ID: None, Comment: 东达不可避免\n",
      "26. Author ID: None, Comment: 为宠物股炒作提供注释\n",
      "27. Author ID: None, Comment: 利好养老类股票。\n",
      "28. Author ID: None, Comment: 存\n",
      "29. Author ID: None, Comment: mark\n",
      "30. Author ID: None, Comment: 转\n",
      "31. Author ID: None, Comment: 转发。感觉陪伴型机器人在AI时代有巨大的发展潜力。\n",
      "32. Author ID: None, Comment: 转：\n",
      "1980年代，日本一人户占比只有20%，如今逼近40%。未来的日本将成为半数人口是单身的“超级单身社会”。\n",
      "而在中国，独居、晚婚、不婚人群也正快速上升。\n",
      "这不是偶然，而是结构性变化。\n",
      "一个人生活，意味着从吃饭、出行、情感，到陪伴、安全感，都要独自完成。这背后，藏着海量“新需求”和“新供给”。\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭、饭团、即食热食，成了日本最大的餐饮品牌。\n",
      "2023财年营收破10万亿日元（约5000亿人民币），稳压传统连锁餐饮。\n",
      "东京的面馆、咖啡馆大量设置“一个人座位”，配隔板、配平板，边吃边追剧，社交压力为零——孤独，是可以被尊重的生活方式。\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万，远超15岁以下儿童。孤独都市人，把情感投射给了猫狗。\n",
      "日本宠物主对待他们的宠物就像对待家人一样，甚至比自己的地位还高。宠物用品已全面“母婴化”：推车、衣服、零食甚至“宠物饮品”一应俱全。独酌时，希望宠物也能“陪一杯”。\n",
      "3️⃣ 陪伴型机器人：技术不卖功能，卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT，不扫地、不聊天、不能干活——但它会“撒娇”、会“跟着你”、会“让你想抱”。\n",
      "这是一台卖情绪价值的机器人\n",
      "33. Author ID: None, Comment: AI娃娃\n",
      "\n",
      "==============================\n",
      "Check console logs and '20' folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def scrape_post_and_all_comments(url, max_main_loops=20, scroll_pause_time=2.5):\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_post_all_comments\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"post_author_id\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    # Updated scraped_data structure\n",
    "    scraped_data = {\"post_content\": None, \"post_author_id\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set() # Still used to track uniqueness of comment TEXTS\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        interaction_wait = WebDriverWait(driver, 7)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "\n",
    "        # Scrape Main Post Content and Author ID\n",
    "        try:\n",
    "            print(\"Scraping main post content and author ID...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "            try:\n",
    "                post_author_link_xpath = \"//div[contains(@class, 'article__author')]//a[@data-tooltip and starts-with(@href, '/')]\"\n",
    "                author_link_element = wait.until(EC.presence_of_element_located((By.XPATH, post_author_link_xpath)))\n",
    "                author_id_val = author_link_element.get_attribute('data-tooltip')\n",
    "                if author_id_val and author_id_val.isdigit():\n",
    "                    scraped_data[\"post_author_id\"] = author_id_val\n",
    "                    print(f\"Post author ID scraped: {author_id_val}\")\n",
    "                else:\n",
    "                    href = author_link_element.get_attribute('href')\n",
    "                    if href: potential_id = href.split('/')[-1]\n",
    "                    if potential_id.isdigit(): scraped_data[\"post_author_id\"] = potential_id; print(f\"Post author ID scraped from href: {potential_id}\")\n",
    "            except TimeoutException: print(\"Post author link (for ID) not found.\")\n",
    "            except Exception as e_author: print(f\"Error scraping post author ID: {e_author}\")\n",
    "        except Exception as e_post:\n",
    "            print(f\"Error scraping post content/author: {e_post}\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,\"error_post_scrape.png\"))\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_total_unique_texts_count = -1 # Based on unique_comment_texts_scraped\n",
    "\n",
    "        for i in range(max_main_loops):\n",
    "            print(f\"--- Main Loop Iteration #{i+1} ---\")\n",
    "            action_taken_this_loop = False\n",
    "            # comments_at_loop_start = len(unique_comment_texts_scraped) # original logic used len of this set\n",
    "\n",
    "            # 1. Click ALL visible \"查看N条回复\" (Expand Replies)\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复') and not(ancestor::div[contains(@style,'display: none')]) and not(ancestor::div[contains(@class,'hide')])]\"\n",
    "            expand_attempts = 0\n",
    "            while expand_attempts < 5:\n",
    "                expand_attempts += 1\n",
    "                clicked_an_expand_button_this_pass = False\n",
    "                try:\n",
    "                    visible_expand_buttons = [b for b in driver.find_elements(By.XPATH, expand_reply_xpath) if b.is_displayed() and b.is_enabled()]\n",
    "                    if not visible_expand_buttons:\n",
    "                        if expand_attempts > 1: print(f\"  No more 'Expand Replies' visible in sub-attempt {expand_attempts}.\")\n",
    "                        break\n",
    "                    print(f\"  Found {len(visible_expand_buttons)} visible 'Expand Replies' links (Attempt {expand_attempts}).\")\n",
    "                    for button in visible_expand_buttons:\n",
    "                        try:\n",
    "                            if not button.is_displayed() or not button.is_enabled(): continue # Re-check\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", button); time.sleep(0.4)\n",
    "                            button_to_click = WebDriverWait(driver, 3).until(EC.element_to_be_clickable(button))\n",
    "                            driver.execute_script(\"arguments[0].click();\", button_to_click)\n",
    "                            print(f\"    Clicked 'Expand Replies': {button.text[:30]}...\")\n",
    "                            action_taken_this_loop = True\n",
    "                            clicked_an_expand_button_this_pass = True\n",
    "                            time.sleep(1.5) # Wait for replies\n",
    "                        except (StaleElementReferenceException, TimeoutException, ElementNotInteractableException): continue\n",
    "                        except Exception as e_expand: print(f\"    Error clicking one 'Expand Replies': {e_expand}\")\n",
    "                    if not clicked_an_expand_button_this_pass and expand_attempts > 1 : break # No more were clicked this pass\n",
    "                    if clicked_an_expand_button_this_pass : time.sleep(0.5) # Short pause if something was clicked\n",
    "                except Exception as e_find_expand: print(f\"  Error finding 'Expand Replies': {e_find_expand}\"); break\n",
    "\n",
    "            # 2. Scroll down (helps reveal \"Load More\" and more comments)\n",
    "            print(\"  Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            # 3. Attempt to scrape comments (text and author ID)\n",
    "            # XPath for comment text p tags, combining original and a variant for replies\n",
    "            comment_p_tag_xpath = \"//div[@class='comment__item__main']/p | //div[contains(@class, 'comment__content')]/p\"\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_p_tag_xpath)\n",
    "                new_comments_added_to_list_this_pass = 0\n",
    "                \n",
    "                if comment_p_tags:\n",
    "                    print(f\"    Found {len(comment_p_tags)} potential comment <p> tags.\")\n",
    "\n",
    "                for p_tag in comment_p_tags:\n",
    "                    comment_text = \"\"\n",
    "                    author_id = None\n",
    "                    try:\n",
    "                        comment_text = p_tag.text.strip()\n",
    "                        if not comment_text: # Skip if p_tag is empty after strip\n",
    "                            continue\n",
    "\n",
    "                        if comment_text not in unique_comment_texts_scraped:\n",
    "                            unique_comment_texts_scraped.add(comment_text) # Add to set for uniqueness check\n",
    "                            \n",
    "                            # Find author ID related to this p_tag\n",
    "                            try:\n",
    "                                # Navigate up to the common ancestor 'comment_item' or 'reply_item'\n",
    "                                # This XPath tries to find the closest div ancestor that contains 'comment_item' in its class\n",
    "                                comment_block_ancestor = p_tag.find_element(By.XPATH, \"./ancestor::div[contains(@class, 'comment_item')][1]\")\n",
    "                                \n",
    "                                try:\n",
    "                                    # From this ancestor, find the author link (e.g., <a data-tooltip=\"ID\" class=\"avatar\">)\n",
    "                                    author_link_el = comment_block_ancestor.find_element(By.XPATH, \".//a[@data-tooltip and starts-with(@href, '/')]\")\n",
    "                                    temp_author_id = author_link_el.get_attribute('data-tooltip')\n",
    "                                    if temp_author_id and temp_author_id.isdigit():\n",
    "                                        author_id = temp_author_id\n",
    "                                    else: \n",
    "                                        href = author_link_el.get_attribute('href')\n",
    "                                        if href:\n",
    "                                            potential_id_from_href = href.split('/')[-1]\n",
    "                                            if potential_id_from_href.isdigit():\n",
    "                                                author_id = potential_id_from_href\n",
    "                                except NoSuchElementException:\n",
    "                                    # print(f\"      Author ID link not found for comment text: '{comment_text[:30]}...'\")\n",
    "                                    pass # Author ID will remain None\n",
    "                            except NoSuchElementException:\n",
    "                                # print(f\"      Could not find 'comment_item' ancestor for text: '{comment_text[:30]}...'\")\n",
    "                                pass # Author ID will remain None\n",
    "\n",
    "                            scraped_data[\"comments\"].append({\"text\": comment_text, \"author_id\": author_id})\n",
    "                            new_comments_added_to_list_this_pass += 1\n",
    "                            \n",
    "                    except StaleElementReferenceException:\n",
    "                        # print(\"      Stale p_tag encountered.\")\n",
    "                        continue \n",
    "                    except Exception as e_proc_p:\n",
    "                        print(f\"      Error processing one p_tag for comment: {e_proc_p}\")\n",
    "                \n",
    "                if new_comments_added_to_list_this_pass > 0:\n",
    "                    print(f\"    Added {new_comments_added_to_list_this_pass} new unique comments to data list.\")\n",
    "                    action_taken_this_loop = True # Mark action if new comments were actually added to the list\n",
    "            except Exception as e_find_comments:\n",
    "                print(f\"  Error finding comment <p> tags: {e_find_comments}\")\n",
    "\n",
    "            # 4. Click \"展开查看更多\" (Load More Main Comments)\n",
    "            load_more_comments_xpath = \"//a[@class='show_more' and .//span[text()='展开查看更多']]\"\n",
    "            try:\n",
    "                load_more_button = interaction_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                print(\"  Found '展开查看更多' button. Clicking...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", load_more_button); time.sleep(0.4)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                print(\"    Clicked '展开查看更多'.\")\n",
    "                action_taken_this_loop = True\n",
    "                time.sleep(scroll_pause_time + 0.5) # Wait longer\n",
    "            except TimeoutException:\n",
    "                print(\"  '展开查看更多' button not found or not clickable this pass (might be all loaded).\")\n",
    "            except Exception as e_load_more:\n",
    "                print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            # 5. Check for loop termination conditions\n",
    "            current_unique_texts_count = len(unique_comment_texts_scraped)\n",
    "            print(f\"  Loop {i+1} end. Total unique comment texts scraped: {current_unique_texts_count}. Previously: {last_total_unique_texts_count}. Total comments in list: {len(scraped_data['comments'])}\")\n",
    "\n",
    "            if not action_taken_this_loop and current_unique_texts_count == last_total_unique_texts_count:\n",
    "                # Ensure we don't break on the very first loop if last_total_unique_texts_count is still -1 and no actions taken\n",
    "                if last_total_unique_texts_count != -1 or i > 0 : \n",
    "                    print(\"No actions taken and unique comment text count unchanged. Assuming completion.\")\n",
    "                    break\n",
    "            \n",
    "            last_total_unique_texts_count = current_unique_texts_count\n",
    "            if i == max_main_loops - 1: print(\"Reached max main loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"main_loop_end_{i+1}.png\"))\n",
    "\n",
    "        # No need to convert unique_comment_texts_scraped to list, as scraped_data[\"comments\"] is already the list of dicts\n",
    "        print(f\"\\n--- Finished comment scraping. Total comments in list: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\")\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try:\n",
    "                error_ss_path = os.path.join(screenshot_dir, \"critical_error.png\")\n",
    "                driver.save_screenshot(error_ss_path)\n",
    "                print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit:\n",
    "                 print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\"\n",
    "    # For testing with a page known to have many comments and replies:\n",
    "    # target_url = \"https://xueqiu.com/1929796349/272374603\" # Example with many comments\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} ---\")\n",
    "    \n",
    "    # Accessing the default value of screenshot_dir for the final print message.\n",
    "    # Better would be to return it or make it a global constant if needed outside.\n",
    "    global_screenshot_dir_name = scrape_post_and_all_comments.__defaults__[0]\n",
    "\n",
    "\n",
    "    data = scrape_post_and_all_comments(target_url, max_main_loops=20, scroll_pause_time=2.5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\")\n",
    "        print(f\"Author ID: {data.get('post_author_id', 'N/A')}\")\n",
    "        print(data[\"post_content\"])\n",
    "    else: print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for i, comment_data in enumerate(data[\"comments\"]):\n",
    "            print(f\"{i+1}. Author ID: {comment_data.get('author_id', 'N/A')}, Comment: {comment_data['text']}\")\n",
    "    else: print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30); print(f\"Check console logs and '{global_screenshot_dir_name}' folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ace51c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for Main Xueqiu Feed Post IDs ---\n",
      "Setting up WebDriver for https://xueqiu.com/...\n",
      "Navigating to: https://xueqiu.com/\n",
      "Main feed container indicator loaded.\n",
      "Looking for '跳过' (Skip) pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up (modal close)...\n",
      "Finished checking for 'X' pop-ups.\n",
      "\n",
      "--- Attempting to scrape first 3 post IDs ---\n",
      "Attempt #1 to find articles...\n",
      "  Found 7 article elements on page in attempt 1.\n",
      "    Article 0: Found link using XPath variant 2\n",
      "    Article 0: Link href: https://xueqiu.com/5367879511/334490187\n",
      "      Href 'https://xueqiu.com/5367879511/334490187' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 1: Found link using XPath variant 2\n",
      "    Article 1: Link href: https://xueqiu.com/1760673340/334516549\n",
      "      Href 'https://xueqiu.com/1760673340/334516549' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 2: Found link using XPath variant 2\n",
      "    Article 2: Link href: https://xueqiu.com/7789206927/334567710\n",
      "      Href 'https://xueqiu.com/7789206927/334567710' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 3: Found link using XPath variant 2\n",
      "    Article 3: Link href: https://xueqiu.com/9081441836/334507190\n",
      "      Href 'https://xueqiu.com/9081441836/334507190' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 4: Found link using XPath variant 2\n",
      "    Article 4: Link href: https://xueqiu.com/1821992043/334498948\n",
      "      Href 'https://xueqiu.com/1821992043/334498948' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 5: Found link using XPath variant 2\n",
      "    Article 5: Link href: https://xueqiu.com/8414744881/334490193\n",
      "      Href 'https://xueqiu.com/8414744881/334490193' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 6: Found link using XPath variant 2\n",
      "    Article 6: Link href: https://xueqiu.com/3529405310/334568432\n",
      "      Href 'https://xueqiu.com/3529405310/334568432' does not have '/user_id/post_id' structure after splitting.\n",
      "  Still need more IDs (0/3). Scrolling down...\n",
      "Attempt #2 to find articles...\n",
      "  Found 7 article elements on page in attempt 2.\n",
      "    Article 0: Found link using XPath variant 2\n",
      "    Article 0: Link href: https://xueqiu.com/5367879511/334490187\n",
      "      Href 'https://xueqiu.com/5367879511/334490187' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 1: Found link using XPath variant 2\n",
      "    Article 1: Link href: https://xueqiu.com/1760673340/334516549\n",
      "      Href 'https://xueqiu.com/1760673340/334516549' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 2: Found link using XPath variant 2\n",
      "    Article 2: Link href: https://xueqiu.com/7789206927/334567710\n",
      "      Href 'https://xueqiu.com/7789206927/334567710' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 3: Found link using XPath variant 2\n",
      "    Article 3: Link href: https://xueqiu.com/9081441836/334507190\n",
      "      Href 'https://xueqiu.com/9081441836/334507190' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 4: Found link using XPath variant 2\n",
      "    Article 4: Link href: https://xueqiu.com/1821992043/334498948\n",
      "      Href 'https://xueqiu.com/1821992043/334498948' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 5: Found link using XPath variant 2\n",
      "    Article 5: Link href: https://xueqiu.com/8414744881/334490193\n",
      "      Href 'https://xueqiu.com/8414744881/334490193' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 6: Found link using XPath variant 2\n",
      "    Article 6: Link href: https://xueqiu.com/3529405310/334568432\n",
      "      Href 'https://xueqiu.com/3529405310/334568432' does not have '/user_id/post_id' structure after splitting.\n",
      "  Still need more IDs (0/3). Scrolling down...\n",
      "Attempt #3 to find articles...\n",
      "  Found 7 article elements on page in attempt 3.\n",
      "    Article 0: Found link using XPath variant 2\n",
      "    Article 0: Link href: https://xueqiu.com/5367879511/334490187\n",
      "      Href 'https://xueqiu.com/5367879511/334490187' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 1: Found link using XPath variant 2\n",
      "    Article 1: Link href: https://xueqiu.com/1760673340/334516549\n",
      "      Href 'https://xueqiu.com/1760673340/334516549' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 2: Found link using XPath variant 2\n",
      "    Article 2: Link href: https://xueqiu.com/7789206927/334567710\n",
      "      Href 'https://xueqiu.com/7789206927/334567710' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 3: Found link using XPath variant 2\n",
      "    Article 3: Link href: https://xueqiu.com/9081441836/334507190\n",
      "      Href 'https://xueqiu.com/9081441836/334507190' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 4: Found link using XPath variant 2\n",
      "    Article 4: Link href: https://xueqiu.com/1821992043/334498948\n",
      "      Href 'https://xueqiu.com/1821992043/334498948' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 5: Found link using XPath variant 2\n",
      "    Article 5: Link href: https://xueqiu.com/8414744881/334490193\n",
      "      Href 'https://xueqiu.com/8414744881/334490193' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 6: Found link using XPath variant 2\n",
      "    Article 6: Link href: https://xueqiu.com/3529405310/334568432\n",
      "      Href 'https://xueqiu.com/3529405310/334568432' does not have '/user_id/post_id' structure after splitting.\n",
      "  Still need more IDs (0/3). Scrolling down...\n",
      "Attempt #4 to find articles...\n",
      "  Found 7 article elements on page in attempt 4.\n",
      "    Article 0: Found link using XPath variant 2\n",
      "    Article 0: Link href: https://xueqiu.com/5367879511/334490187\n",
      "      Href 'https://xueqiu.com/5367879511/334490187' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 1: Found link using XPath variant 2\n",
      "    Article 1: Link href: https://xueqiu.com/1760673340/334516549\n",
      "      Href 'https://xueqiu.com/1760673340/334516549' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 2: Found link using XPath variant 2\n",
      "    Article 2: Link href: https://xueqiu.com/7789206927/334567710\n",
      "      Href 'https://xueqiu.com/7789206927/334567710' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 3: Found link using XPath variant 2\n",
      "    Article 3: Link href: https://xueqiu.com/9081441836/334507190\n",
      "      Href 'https://xueqiu.com/9081441836/334507190' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 4: Found link using XPath variant 2\n",
      "    Article 4: Link href: https://xueqiu.com/1821992043/334498948\n",
      "      Href 'https://xueqiu.com/1821992043/334498948' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 5: Found link using XPath variant 2\n",
      "    Article 5: Link href: https://xueqiu.com/8414744881/334490193\n",
      "      Href 'https://xueqiu.com/8414744881/334490193' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 6: Found link using XPath variant 2\n",
      "    Article 6: Link href: https://xueqiu.com/3529405310/334568432\n",
      "      Href 'https://xueqiu.com/3529405310/334568432' does not have '/user_id/post_id' structure after splitting.\n",
      "  Still need more IDs (0/3). Scrolling down...\n",
      "Attempt #5 to find articles...\n",
      "  Found 7 article elements on page in attempt 5.\n",
      "    Article 0: Found link using XPath variant 2\n",
      "    Article 0: Link href: https://xueqiu.com/5367879511/334490187\n",
      "      Href 'https://xueqiu.com/5367879511/334490187' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 1: Found link using XPath variant 2\n",
      "    Article 1: Link href: https://xueqiu.com/1760673340/334516549\n",
      "      Href 'https://xueqiu.com/1760673340/334516549' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 2: Found link using XPath variant 2\n",
      "    Article 2: Link href: https://xueqiu.com/7789206927/334567710\n",
      "      Href 'https://xueqiu.com/7789206927/334567710' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 3: Found link using XPath variant 2\n",
      "    Article 3: Link href: https://xueqiu.com/9081441836/334507190\n",
      "      Href 'https://xueqiu.com/9081441836/334507190' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 4: Found link using XPath variant 2\n",
      "    Article 4: Link href: https://xueqiu.com/1821992043/334498948\n",
      "      Href 'https://xueqiu.com/1821992043/334498948' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 5: Found link using XPath variant 2\n",
      "    Article 5: Link href: https://xueqiu.com/8414744881/334490193\n",
      "      Href 'https://xueqiu.com/8414744881/334490193' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 6: Found link using XPath variant 2\n",
      "    Article 6: Link href: https://xueqiu.com/3529405310/334568432\n",
      "      Href 'https://xueqiu.com/3529405310/334568432' does not have '/user_id/post_id' structure after splitting.\n",
      "  Still need more IDs (0/3). Scrolling down...\n",
      "Attempt #6 to find articles...\n",
      "  Found 7 article elements on page in attempt 6.\n",
      "    Article 0: Found link using XPath variant 2\n",
      "    Article 0: Link href: https://xueqiu.com/5367879511/334490187\n",
      "      Href 'https://xueqiu.com/5367879511/334490187' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 1: Found link using XPath variant 2\n",
      "    Article 1: Link href: https://xueqiu.com/1760673340/334516549\n",
      "      Href 'https://xueqiu.com/1760673340/334516549' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 2: Found link using XPath variant 2\n",
      "    Article 2: Link href: https://xueqiu.com/7789206927/334567710\n",
      "      Href 'https://xueqiu.com/7789206927/334567710' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 3: Found link using XPath variant 2\n",
      "    Article 3: Link href: https://xueqiu.com/9081441836/334507190\n",
      "      Href 'https://xueqiu.com/9081441836/334507190' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 4: Found link using XPath variant 2\n",
      "    Article 4: Link href: https://xueqiu.com/1821992043/334498948\n",
      "      Href 'https://xueqiu.com/1821992043/334498948' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 5: Found link using XPath variant 2\n",
      "    Article 5: Link href: https://xueqiu.com/8414744881/334490193\n",
      "      Href 'https://xueqiu.com/8414744881/334490193' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 6: Found link using XPath variant 2\n",
      "    Article 6: Link href: https://xueqiu.com/3529405310/334568432\n",
      "      Href 'https://xueqiu.com/3529405310/334568432' does not have '/user_id/post_id' structure after splitting.\n",
      "  Still need more IDs (0/3). Scrolling down...\n",
      "Attempt #7 to find articles...\n",
      "  Found 7 article elements on page in attempt 7.\n",
      "    Article 0: Found link using XPath variant 2\n",
      "    Article 0: Link href: https://xueqiu.com/5367879511/334490187\n",
      "      Href 'https://xueqiu.com/5367879511/334490187' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 1: Found link using XPath variant 2\n",
      "    Article 1: Link href: https://xueqiu.com/1760673340/334516549\n",
      "      Href 'https://xueqiu.com/1760673340/334516549' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 2: Found link using XPath variant 2\n",
      "    Article 2: Link href: https://xueqiu.com/7789206927/334567710\n",
      "      Href 'https://xueqiu.com/7789206927/334567710' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 3: Found link using XPath variant 2\n",
      "    Article 3: Link href: https://xueqiu.com/9081441836/334507190\n",
      "      Href 'https://xueqiu.com/9081441836/334507190' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 4: Found link using XPath variant 2\n",
      "    Article 4: Link href: https://xueqiu.com/1821992043/334498948\n",
      "      Href 'https://xueqiu.com/1821992043/334498948' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 5: Found link using XPath variant 2\n",
      "    Article 5: Link href: https://xueqiu.com/8414744881/334490193\n",
      "      Href 'https://xueqiu.com/8414744881/334490193' does not have '/user_id/post_id' structure after splitting.\n",
      "    Article 6: Found link using XPath variant 2\n",
      "    Article 6: Link href: https://xueqiu.com/3529405310/334568432\n",
      "      Href 'https://xueqiu.com/3529405310/334568432' does not have '/user_id/post_id' structure after splitting.\n",
      "  Still need more IDs (0/3). Scrolling down...\n",
      "Could not find any post IDs after all attempts. Saving a screenshot.\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Post IDs\n",
      "==============================\n",
      "\n",
      ">>> No post IDs were scraped. <<<\n",
      "\n",
      "==============================\n",
      "Check console logs and the 'screenshots_main_feed' folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def scrape_main_feed_post_ids(url=\"https://xueqiu.com/\", num_ids_to_get=3):\n",
    "    print(f\"Setting up WebDriver for {url}...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_main_feed\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return []\n",
    "\n",
    "    driver = None\n",
    "    post_ids = []\n",
    "    # Using a set for unique_post_ids to ensure we don't add duplicates if multiple links point to the same post\n",
    "    unique_post_ids = set()\n",
    "\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "        wait = WebDriverWait(driver, 20) \n",
    "        interaction_wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//article[contains(@class, 'timeline__item')] | //div[contains(@class, 'home_timeline')] | //div[contains(@class, 'list__container')]\")) # Added another possible container\n",
    "            )\n",
    "            print(\"Main feed container indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Main feed container indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return []\n",
    "        time.sleep(3) \n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' (Skip) pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 7).until(EC.element_to_be_clickable((By.XPATH, skip_xpath))) # Shorter wait for popups\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error clicking '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up (modal close)...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\", \"//div[@class='optional_tip']/i[@class='icon-close']\" ]\n",
    "            for xpath_item in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.XPATH, xpath_item))); driver.execute_script(\"arguments[0].click();\", close_button); print(f\"Clicked 'X' for pop-up using: {xpath_item}\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        print(f\"\\n--- Attempting to scrape first {num_ids_to_get} post IDs ---\")\n",
    "        article_xpath = \"//article[contains(@class, 'timeline__item') or contains(@class, 'home__timeline__item')]\"\n",
    "        \n",
    "        attempts = 0\n",
    "        max_scroll_attempts = 7 # Increased scroll attempts\n",
    "\n",
    "        while len(unique_post_ids) < num_ids_to_get and attempts < max_scroll_attempts:\n",
    "            attempts += 1\n",
    "            print(f\"Attempt #{attempts} to find articles...\")\n",
    "            \n",
    "            article_elements = driver.find_elements(By.XPATH, article_xpath)\n",
    "            print(f\"  Found {len(article_elements)} article elements on page in attempt {attempts}.\")\n",
    "\n",
    "            if not article_elements and attempts == 1:\n",
    "                print(\"  No articles found on first pass, waiting a bit more...\")\n",
    "                time.sleep(5)\n",
    "                article_elements = driver.find_elements(By.XPATH, article_xpath)\n",
    "                print(f\"  Found {len(article_elements)} article elements after extra wait.\")\n",
    "\n",
    "            for article_idx, article in enumerate(article_elements):\n",
    "                if len(unique_post_ids) >= num_ids_to_get:\n",
    "                    break\n",
    "                try:\n",
    "                    link_element = None\n",
    "                    href_value = None\n",
    "                    \n",
    "                    # Define XPaths to try for the link\n",
    "                    # Path from your screenshot: <a class=\"style_fake-anchor_2cg fake-anchor\" href=\"/USERID/POSTID\">\n",
    "                    # It is inside <div class=\"style_timeline_item_content_38K\">\n",
    "                    # which is inside <div class=\"style_timeline_item_main_1HD\">\n",
    "                    # which is inside <article class=\"style_timeline__item_3WW\">\n",
    "                    \n",
    "                    # Most specific based on your screenshot's structure\n",
    "                    xpath_for_link_v1 = \".//div[contains(@class, 'timeline_item_content')]//a[contains(@class, 'fake-anchor_') and starts-with(@href, '/')]\"\n",
    "                    # General fake-anchor\n",
    "                    xpath_for_link_v2 = \".//a[contains(@class, 'fake-anchor_') and starts-with(@href, '/')]\"\n",
    "                    # More general link that has an h3 title, usually a main post link\n",
    "                    xpath_for_link_v3 = \".//a[starts-with(@href, '/') and not(contains(@class, 'avatar')) and .//h3]\"\n",
    "\n",
    "\n",
    "                    link_xpaths_to_try = [xpath_for_link_v1, xpath_for_link_v2, xpath_for_link_v3]\n",
    "                    \n",
    "                    for i_xpath, link_xpath in enumerate(link_xpaths_to_try):\n",
    "                        try:\n",
    "                            link_element = article.find_element(By.XPATH, link_xpath)\n",
    "                            if link_element:\n",
    "                                print(f\"    Article {article_idx}: Found link using XPath variant {i_xpath+1}\")\n",
    "                                break \n",
    "                        except NoSuchElementException:\n",
    "                            if i_xpath == len(link_xpaths_to_try) -1: # If last attempt failed\n",
    "                                print(f\"    Article {article_idx}: All XPath variants for link failed.\")\n",
    "                            continue # Try next XPath\n",
    "\n",
    "                    if link_element:\n",
    "                        href_value = link_element.get_attribute('href')\n",
    "                        if href_value:\n",
    "                            print(f\"    Article {article_idx}: Link href: {href_value}\")\n",
    "                            parts = href_value.strip('/').split('/')\n",
    "                            if len(parts) == 2: \n",
    "                                user_id, post_id_str = parts[0], parts[1]\n",
    "                                if post_id_str.isdigit():\n",
    "                                    if post_id_str not in unique_post_ids:\n",
    "                                        print(f\"      Extracted NEW Post ID: {post_id_str} (User ID: {user_id})\")\n",
    "                                        unique_post_ids.add(post_id_str)\n",
    "                                    # else:\n",
    "                                        # print(f\"      Post ID {post_id_str} already found.\")\n",
    "                                    if len(unique_post_ids) >= num_ids_to_get:\n",
    "                                        break\n",
    "                                else:\n",
    "                                    print(f\"      Post ID part '{post_id_str}' is not numeric from href '{href_value}'.\")\n",
    "                            else:\n",
    "                                print(f\"      Href '{href_value}' does not have '/user_id/post_id' structure after splitting.\")\n",
    "                        else:\n",
    "                            print(f\"    Article {article_idx}: Link found but href is empty.\")\n",
    "                    # If link_element is still None after trying all XPaths, it will be handled by the outer loop condition or next scroll.\n",
    "\n",
    "                except Exception as e_article:\n",
    "                    print(f\"    Error processing article {article_idx}: {e_article}\")\n",
    "\n",
    "            if len(unique_post_ids) < num_ids_to_get:\n",
    "                if len(article_elements) > 0 :\n",
    "                    print(f\"  Still need more IDs ({len(unique_post_ids)}/{num_ids_to_get}). Scrolling down...\")\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "                    time.sleep(3.5) \n",
    "                else:\n",
    "                    print(\"  No articles found on this scroll pass, or all processed.\")\n",
    "                    # if attempts > 2 and not unique_post_ids : # If few attempts and still no IDs, might be an issue\n",
    "                    #    print(\"  Breaking scroll attempts as no articles seem to be loading or matching.\")\n",
    "                    #    break\n",
    "            \n",
    "        if not unique_post_ids:\n",
    "            print(\"Could not find any post IDs after all attempts. Saving a screenshot.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir, \"no_post_ids_found.png\"))\n",
    "        else:\n",
    "            post_ids = list(unique_post_ids)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\"); print(f\"Error Type: {type(e).__name__}\"); print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try: error_ss_path = os.path.join(screenshot_dir, \"critical_error_main_feed.png\"); driver.save_screenshot(error_ss_path); print(f\"Saved critical error screenshot to {error_ss_path}\")\n",
    "            except Exception as e_ss_crit: print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    \n",
    "    return post_ids[:num_ids_to_get]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Starting Scraper for Main Xueqiu Feed Post IDs ---\")\n",
    "    extracted_ids = scrape_main_feed_post_ids(num_ids_to_get=3)\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Post IDs\"); print(\"=\"*30)\n",
    "    if extracted_ids:\n",
    "        print(f\"Successfully scraped {len(extracted_ids)} post IDs:\")\n",
    "        for i, pid in enumerate(extracted_ids): print(f\"{i+1}. {pid}\")\n",
    "    else: print(\"\\n>>> No post IDs were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30); print(f\"Check console logs and the 'screenshots_main_feed' folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d56db6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for Main Xueqiu Feed Post Links ---\n",
      "Setting up WebDriver for https://xueqiu.com/...\n",
      "Created 'screenshots_main_feed_links' directory.\n",
      "Navigating to: https://xueqiu.com/\n",
      "Main feed container indicator loaded.\n",
      "Looking for '跳过' (Skip) pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up (modal close)...\n",
      "Finished checking for 'X' pop-ups.\n",
      "\n",
      "--- Attempting to scrape all unique post links (max scrolls: 3) ---\n",
      "Scroll Attempt #1...\n",
      "  Found 7 article elements on page.\n",
      "    Article 0: Found link using XPath variant 2\n",
      "    Article 0: Adding NEW unique link: https://xueqiu.com/5367879511/334490187\n",
      "    Article 1: Found link using XPath variant 2\n",
      "    Article 1: Adding NEW unique link: https://xueqiu.com/1760673340/334516549\n",
      "    Article 2: Found link using XPath variant 2\n",
      "    Article 2: Adding NEW unique link: https://xueqiu.com/3609236100/334567815\n",
      "    Article 3: Found link using XPath variant 2\n",
      "    Article 3: Adding NEW unique link: https://xueqiu.com/9081441836/334507190\n",
      "    Article 4: Found link using XPath variant 2\n",
      "    Article 4: Adding NEW unique link: https://xueqiu.com/1821992043/334498948\n",
      "    Article 5: Found link using XPath variant 2\n",
      "    Article 5: Adding NEW unique link: https://xueqiu.com/8414744881/334490193\n",
      "    Article 6: Found link using XPath variant 2\n",
      "    Article 6: Adding NEW unique link: https://xueqiu.com/1558671310/334556545\n",
      "  Scrolling down to load more articles... (Found 7 unique links so far)\n",
      "Scroll Attempt #2...\n",
      "  Found 7 article elements on page.\n",
      "    Article 0: Found link using XPath variant 2\n",
      "    Article 1: Found link using XPath variant 2\n",
      "    Article 2: Found link using XPath variant 2\n",
      "    Article 3: Found link using XPath variant 2\n",
      "    Article 4: Found link using XPath variant 2\n",
      "    Article 5: Found link using XPath variant 2\n",
      "    Article 6: Found link using XPath variant 2\n",
      "  Scrolling down to load more articles... (Found 7 unique links so far)\n",
      "  No new links found after scroll, and articles were present. Might be end of feed or slow load.\n",
      "  Number of unique links hasn't changed for two scrolls. Ending.\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Post Links\n",
      "==============================\n",
      "Successfully scraped 7 unique post links:\n",
      "1. https://xueqiu.com/1760673340/334516549\n",
      "2. https://xueqiu.com/1821992043/334498948\n",
      "3. https://xueqiu.com/9081441836/334507190\n",
      "4. https://xueqiu.com/1558671310/334556545\n",
      "5. https://xueqiu.com/8414744881/334490193\n",
      "6. https://xueqiu.com/3609236100/334567815\n",
      "7. https://xueqiu.com/5367879511/334490187\n",
      "\n",
      "==============================\n",
      "Check console logs and the 'https://xueqiu.com/' folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "from urllib.parse import urlparse # Import for URL parsing\n",
    "\n",
    "def scrape_main_feed_post_links(url=\"https://xueqiu.com/\", max_scroll_attempts=5):\n",
    "    print(f\"Setting up WebDriver for {url}...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_main_feed_links\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return []\n",
    "\n",
    "    driver = None\n",
    "    # Using a set for unique_links to ensure we don't add duplicates\n",
    "    unique_links_found = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "        # General wait not used as much here, waits are more specific\n",
    "        # wait = WebDriverWait(driver, 20)\n",
    "        # interaction_wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//article[contains(@class, 'timeline__item')] | //div[contains(@class, 'home_timeline')] | //div[contains(@class, 'list__container')]\"))\n",
    "            )\n",
    "            print(\"Main feed container indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Main feed container indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return []\n",
    "        time.sleep(3) \n",
    "\n",
    "        # --- Handle Initial Pop-ups ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' (Skip) pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 7).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error clicking '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up (modal close)...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\", \"//div[@class='optional_tip']/i[@class='icon-close']\" ]\n",
    "            for xpath_item in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.XPATH, xpath_item))); driver.execute_script(\"arguments[0].click();\", close_button); print(f\"Clicked 'X' for pop-up using: {xpath_item}\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        print(f\"\\n--- Attempting to scrape all unique post links (max scrolls: {max_scroll_attempts}) ---\")\n",
    "        article_xpath = \"//article[contains(@class, 'timeline__item') or contains(@class, 'home__timeline__item')]\"\n",
    "        \n",
    "        last_num_links_found = -1\n",
    "\n",
    "        for attempt in range(max_scroll_attempts):\n",
    "            current_links_before_scroll = len(unique_links_found)\n",
    "            print(f\"Scroll Attempt #{attempt+1}...\")\n",
    "            \n",
    "            article_elements = driver.find_elements(By.XPATH, article_xpath)\n",
    "            print(f\"  Found {len(article_elements)} article elements on page.\")\n",
    "\n",
    "            if not article_elements and attempt == 0: # Special wait if nothing on first try\n",
    "                print(\"  No articles found on first pass, waiting a bit more...\")\n",
    "                time.sleep(5)\n",
    "                article_elements = driver.find_elements(By.XPATH, article_xpath)\n",
    "                print(f\"  Found {len(article_elements)} article elements after extra wait.\")\n",
    "\n",
    "            for article_idx, article in enumerate(article_elements):\n",
    "                try:\n",
    "                    link_element = None\n",
    "                    href_value = None\n",
    "                    \n",
    "                    # Define XPaths to try for the link\n",
    "                    xpath_for_link_v1 = \".//div[contains(@class, 'timeline_item_content')]//a[contains(@class, 'fake-anchor_') and (starts-with(@href, '/') or starts-with(@href, 'http'))]\"\n",
    "                    xpath_for_link_v2 = \".//a[contains(@class, 'fake-anchor_') and (starts-with(@href, '/') or starts-with(@href, 'http'))]\"\n",
    "                    xpath_for_link_v3 = \".//a[(starts-with(@href, '/') or starts-with(@href, 'http')) and not(contains(@class, 'avatar')) and .//h3]\"\n",
    "                    \n",
    "                    link_xpaths_to_try = [xpath_for_link_v1, xpath_for_link_v2, xpath_for_link_v3]\n",
    "                    \n",
    "                    for i_xpath, link_xpath in enumerate(link_xpaths_to_try):\n",
    "                        try:\n",
    "                            # Use find_elements to catch all matching links, then process the first valid one\n",
    "                            # Some articles might have multiple such links, we want the primary post link.\n",
    "                            # Heuristic: the first one is usually the main link for the article title/content.\n",
    "                            potential_links = article.find_elements(By.XPATH, link_xpath)\n",
    "                            if potential_links:\n",
    "                                link_element = potential_links[0] # Take the first one\n",
    "                                print(f\"    Article {article_idx}: Found link using XPath variant {i_xpath+1}\")\n",
    "                                break \n",
    "                        except NoSuchElementException: # Should not happen with find_elements, but as a safeguard\n",
    "                            pass # Handled by if potential_links\n",
    "                        except StaleElementReferenceException:\n",
    "                            print(f\"    Article {article_idx}: Stale element reference trying XPath variant {i_xpath+1}. Skipping this article.\")\n",
    "                            link_element = \"STALE\" # Mark as stale to break from article processing\n",
    "                            break\n",
    "\n",
    "\n",
    "                    if link_element == \"STALE\":\n",
    "                        continue # Skip to next article\n",
    "\n",
    "                    if link_element:\n",
    "                        href_value = link_element.get_attribute('href')\n",
    "                        if href_value:\n",
    "                            # Ensure it's a full URL if it's a relative path\n",
    "                            if href_value.startswith(\"/\"):\n",
    "                                href_value = \"https://xueqiu.com\" + href_value\n",
    "                            \n",
    "                            if href_value not in unique_links_found:\n",
    "                                print(f\"    Article {article_idx}: Adding NEW unique link: {href_value}\")\n",
    "                                unique_links_found.add(href_value)\n",
    "                            # else:\n",
    "                            #    print(f\"    Article {article_idx}: Link already found: {href_value}\")\n",
    "                        else:\n",
    "                            print(f\"    Article {article_idx}: Link found but href is empty.\")\n",
    "                    elif i_xpath == len(link_xpaths_to_try) -1 : # Only print if all variants failed\n",
    "                        print(f\"    Article {article_idx}: All XPath variants for link failed to find an element.\")\n",
    "\n",
    "\n",
    "                except StaleElementReferenceException:\n",
    "                    print(f\"    Article {article_idx} became stale. Skipping.\")\n",
    "                except Exception as e_article:\n",
    "                    print(f\"    Error processing article {article_idx}: {type(e_article).__name__} - {e_article}\")\n",
    "\n",
    "            # Scroll down to load more content\n",
    "            if attempt < max_scroll_attempts -1: # Don't scroll on the last attempt\n",
    "                print(f\"  Scrolling down to load more articles... (Found {len(unique_links_found)} unique links so far)\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "                time.sleep(4) # Increased wait time after scroll for content to load\n",
    "                \n",
    "                if len(unique_links_found) == current_links_before_scroll and len(article_elements) > 0:\n",
    "                    print(\"  No new links found after scroll, and articles were present. Might be end of feed or slow load.\")\n",
    "                    # Consider breaking if no new links for a couple of scrolls\n",
    "                    if last_num_links_found == len(unique_links_found):\n",
    "                        print(\"  Number of unique links hasn't changed for two scrolls. Ending.\")\n",
    "                        break\n",
    "                last_num_links_found = len(unique_links_found)\n",
    "            \n",
    "        if not unique_links_found:\n",
    "            print(\"Could not find any post links after all attempts. Saving a screenshot.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir, \"no_post_links_found.png\"))\n",
    "        \n",
    "        final_links_list = list(unique_links_found)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\"); print(f\"Error Type: {type(e).__name__}\"); print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try: error_ss_path = os.path.join(screenshot_dir, \"critical_error_main_feed.png\"); driver.save_screenshot(error_ss_path); print(f\"Saved critical error screenshot to {error_ss_path}\")\n",
    "            except Exception as e_ss_crit: print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "        final_links_list = list(unique_links_found) # Return what we have so far\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    \n",
    "    return final_links_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Starting Scraper for Main Xueqiu Feed Post Links ---\")\n",
    "    # Set max_scroll_attempts to control how many times it tries to load more content\n",
    "    # For \"all\" on a very long page, this might need to run for a while.\n",
    "    extracted_links = scrape_main_feed_post_links(max_scroll_attempts=3) # Adjust as needed\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Post Links\"); print(\"=\"*30)\n",
    "    if extracted_links:\n",
    "        print(f\"Successfully scraped {len(extracted_links)} unique post links:\")\n",
    "        for i, link_url in enumerate(extracted_links): print(f\"{i+1}. {link_url}\")\n",
    "    else: print(\"\\n>>> No post links were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30); print(f\"Check console logs and the '{scrape_main_feed_post_links.__defaults__[0]}' folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c128ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5669998349/334081638 (Comment Author IDs) ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_comment_authors' directory.\n",
      "Navigating to: https://xueqiu.com/5669998349/334081638\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content and author ID...\n",
      "Post content scraped (length: 706).\n",
      "Post author ID scraped: 5669998349\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Main Loop Iteration #1 ---\n",
      "  Scrolling down...\n",
      "    Found 19 potential comment <p> tags for text.\n",
      "        Processing new unique text (idx 0): '十年二十年后，大概也这样了，会出什么龙头公司股票呢？...'\n",
      "            p_idx 0: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '十年二十年后，大概也这样了，会出什么龙头...'\n",
      "        Processing new unique text (idx 1): '没有人喜欢照顾别人的情绪，但是所有人都想从别人那里找到情绪价...'\n",
      "            p_idx 1: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '没有人喜欢照顾别人的情绪，但是所有人都想...'\n",
      "        Processing new unique text (idx 2): '神经，愚蠢没有逻辑，人类是群居动物，它们渴望伴侣，阶段性的单...'\n",
      "            p_idx 2: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '神经，愚蠢没有逻辑，人类是群居动物，它们...'\n",
      "        Processing new unique text (idx 3): '我们这现在最先来的应该是老年经济...'\n",
      "            p_idx 3: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '我们这现在最先来的应该是老年经济...'\n",
      "        Processing new unique text (idx 4): '总需求停滞，新需求就是存量竞争，从物质需求到精神需求，从奢靡...'\n",
      "            p_idx 4: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '总需求停滞，新需求就是存量竞争，从物质需...'\n",
      "        Processing new unique text (idx 5): '我们的社会确实朝着这个方向走\n",
      "个人取代家庭，成为社会的最小单...'\n",
      "            p_idx 5: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '我们的社会确实朝着这个方向走\n",
      "个人取代家...'\n",
      "        Processing new unique text (idx 6): '内卷国家的必然...'\n",
      "            p_idx 6: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '内卷国家的必然...'\n",
      "        Processing new unique text (idx 7): '有个手机就够了，老公老婆都可以不要...'\n",
      "            p_idx 7: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '有个手机就够了，老公老婆都可以不要...'\n",
      "        Processing new unique text (idx 8): '不可能的。两个国家本质上的价值观完全不一样。我身边蛮多00后...'\n",
      "            p_idx 8: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '不可能的。两个国家本质上的价值观完全不一...'\n",
      "        Processing new unique text (idx 9): '文明的诅咒...'\n",
      "            p_idx 9: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '文明的诅咒...'\n",
      "        Processing new unique text (idx 10): '如果真能成为日本那样的发达国家就知足吧、优衣库2000多美刀...'\n",
      "            p_idx 10: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '如果真能成为日本那样的发达国家就知足吧、...'\n",
      "        Processing new unique text (idx 11): '对于不婚比例上升这个现象，思考两个原因：1.这是不是阶层固化...'\n",
      "            p_idx 11: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '对于不婚比例上升这个现象，思考两个原因：...'\n",
      "        Processing new unique text (idx 12): '不一定会成日鬼那样的社会，天朝…...'\n",
      "            p_idx 12: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '不一定会成日鬼那样的社会，天朝…...'\n",
      "        Processing new unique text (idx 13): '我这段时间的观察与思考和这部分内容是一致的。我女儿最喜欢的便...'\n",
      "            p_idx 13: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '我这段时间的观察与思考和这部分内容是一致...'\n",
      "        Processing new unique text (idx 14): '性爱机器人产品会供不应求，这类公司会卷出一个品牌。...'\n",
      "            p_idx 14: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '性爱机器人产品会供不应求，这类公司会卷出...'\n",
      "        Processing new unique text (idx 15): '中国人均财富水平距离日本还差的太远，更大的可能性是未富先老，...'\n",
      "            p_idx 15: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '中国人均财富水平距离日本还差的太远，更大...'\n",
      "        Processing new unique text (idx 16): '不婚不育最根本原因是上世纪不准生，一旦生态链损毁，修复何其难...'\n",
      "            p_idx 16: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '不婚不育最根本原因是上世纪不准生，一旦生...'\n",
      "        Processing new unique text (idx 17): '社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在...'\n",
      "            p_idx 17: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '社会，经济，人伦，地产，消费，拿本子比的...'\n",
      "        Processing new unique text (idx 18): '圣杯...'\n",
      "            p_idx 18: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '圣杯...'\n",
      "    Added 19 new unique comments to data list.\n",
      "  Loop 1 end. Total unique comment texts scraped: 19. Previously: -1. Total comments in list: 19\n",
      "--- Main Loop Iteration #2 ---\n",
      "  Scrolling down...\n",
      "    Found 33 potential comment <p> tags for text.\n",
      "        Processing new unique text (idx 19): '情绪经济啊...'\n",
      "            p_idx 19: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '情绪经济啊...'\n",
      "        Processing new unique text (idx 20): '感觉怡红院很快会出牌照？？...'\n",
      "            p_idx 20: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '感觉怡红院很快会出牌照？？...'\n",
      "        Processing new unique text (idx 21): '养娃成本太高了，养差了自己又觉得对不起孩子，还不如不生。我亲...'\n",
      "            p_idx 21: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '养娃成本太高了，养差了自己又觉得对不起孩...'\n",
      "        Processing new unique text (idx 22): '手动转存...'\n",
      "            p_idx 22: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '手动转存...'\n",
      "        Processing new unique text (idx 23): '干宠物经济就行了...'\n",
      "            p_idx 23: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '干宠物经济就行了...'\n",
      "        Processing new unique text (idx 24): '东达不可避免...'\n",
      "            p_idx 24: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '东达不可避免...'\n",
      "        Processing new unique text (idx 25): '为宠物股炒作提供注释...'\n",
      "            p_idx 25: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '为宠物股炒作提供注释...'\n",
      "        Processing new unique text (idx 26): '利好养老类股票。...'\n",
      "            p_idx 26: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '利好养老类股票。...'\n",
      "        Processing new unique text (idx 27): '存...'\n",
      "            p_idx 27: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '存...'\n",
      "        Processing new unique text (idx 28): 'mark...'\n",
      "            p_idx 28: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: 'mark...'\n",
      "        Processing new unique text (idx 29): '转...'\n",
      "            p_idx 29: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '转...'\n",
      "        Processing new unique text (idx 30): '转发。感觉陪伴型机器人在AI时代有巨大的发展潜力。...'\n",
      "            p_idx 30: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '转发。感觉陪伴型机器人在AI时代有巨大的...'\n",
      "        Processing new unique text (idx 31): '转：\n",
      "1980年代，日本一人户占比只有20%，如今逼近40%...'\n",
      "            p_idx 31: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '转：\n",
      "1980年代，日本一人户占比只有2...'\n",
      "        Processing new unique text (idx 32): 'AI娃娃...'\n",
      "            p_idx 32: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: 'AI娃娃...'\n",
      "    Added 14 new unique comments to data list.\n",
      "  Loop 2 end. Total unique comment texts scraped: 33. Previously: 19. Total comments in list: 33\n",
      "--- Main Loop Iteration #3 ---\n",
      "  Scrolling down...\n",
      "    Found 33 potential comment <p> tags for text.\n",
      "  Loop 3 end. Total unique comment texts scraped: 33. Previously: 33. Total comments in list: 33\n",
      "No actions taken and unique comment text count unchanged. Assuming completion.\n",
      "\n",
      "--- Finished comment scraping. Total comments in list: 33 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "Author ID: 5669998349\n",
      "转：\n",
      "1980年代\n",
      "，\n",
      "日本一人户占比只有20%\n",
      "，\n",
      "如今逼近40%\n",
      "。\n",
      "未来的日本将成为半数人口是单身的“超级单身社会”\n",
      "。\n",
      "\n",
      "而在中国\n",
      "，\n",
      "独居\n",
      "、\n",
      "晚婚\n",
      "、\n",
      "不婚人群也正快速上升\n",
      "。\n",
      "\n",
      "这不是偶然\n",
      "，\n",
      "而是结构性变化\n",
      "。\n",
      "\n",
      "一个人生活\n",
      "，\n",
      "意味着从吃饭\n",
      "、\n",
      "出行\n",
      "、\n",
      "情感\n",
      "，\n",
      "到陪伴\n",
      "、\n",
      "安全感\n",
      "，\n",
      "都要独自完成\n",
      "。\n",
      "这背后\n",
      "，\n",
      "藏着海量“新需求”和“新供给”\n",
      "。\n",
      "\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭\n",
      "、\n",
      "饭团\n",
      "、\n",
      "即食热食\n",
      "，\n",
      "成了日本最大的餐饮品牌\n",
      "。\n",
      "\n",
      "2023财年营收破10万亿日元（约5000亿人民币）\n",
      "，\n",
      "稳压传统连锁餐饮\n",
      "。\n",
      "\n",
      "东京的面馆\n",
      "、\n",
      "咖啡馆大量设置“一个人座位”\n",
      "，\n",
      "配隔板\n",
      "、\n",
      "配平板\n",
      "，\n",
      "边吃边追剧\n",
      "，\n",
      "社交压力为零——孤独\n",
      "，\n",
      "是可以被尊重的生活方式\n",
      "。\n",
      "\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万\n",
      "，\n",
      "远超15岁以下儿童\n",
      "。\n",
      "孤独都市人\n",
      "，\n",
      "把情感投射给了猫狗\n",
      "。\n",
      "\n",
      "日本宠物主对待他们的宠物就像对待家人一样\n",
      "，\n",
      "甚至比自己的地位还高\n",
      "。\n",
      "宠物用品已全面“母婴化”：推车\n",
      "、\n",
      "衣服\n",
      "、\n",
      "零食甚至“宠物饮品”一应俱全\n",
      "。\n",
      "独酌时\n",
      "，\n",
      "希望宠物也能“陪一杯”\n",
      "。\n",
      "\n",
      "3️⃣ 陪伴型机器人：技术不卖功能\n",
      "，\n",
      "卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT\n",
      "，\n",
      "不扫地\n",
      "、\n",
      "不聊天\n",
      "、\n",
      "不能干活——但它会“撒娇”\n",
      "、\n",
      "会“跟着你”\n",
      "、\n",
      "会“让你想抱”\n",
      "。\n",
      "\n",
      "这是一台卖情绪价值的机器人\n",
      "。\n",
      "\n",
      "$恒生指数(HKHSI)$ $上证指数(SH000001)$ $招商银行(SH600036)$\n",
      "\n",
      "--- Comments (33) ---\n",
      "1. Author ID: None, Comment: 十年二十年后，大概也这样了，会出什么龙头公司股票呢？\n",
      "2. Author ID: None, Comment: 没有人喜欢照顾别人的情绪，但是所有人都想从别人那里找到情绪价值。\n",
      "3. Author ID: None, Comment: 神经，愚蠢没有逻辑，人类是群居动物，它们渴望伴侣，阶段性的单身是因为经济原因，因为穷，男的穷养不起家，结不了婚，女的穷，养不起自己，并且找不到能养起自己的人，所以不嫁。\n",
      "未来随着机器人人工智能，经济指数级增长，人类会涨校园里的大学生一样无忧无虑，男人和女人因为爱情成群结队。\n",
      "4. Author ID: None, Comment: 我们这现在最先来的应该是老年经济\n",
      "5. Author ID: None, Comment: 总需求停滞，新需求就是存量竞争，从物质需求到精神需求，从奢靡需求到精简需求，从传统需求到新型，个性需求。。但事实上，看看日本的股市龙头，这些需求转向基本做不大，市值最大的还是一些大企业，银行，保险，电信，商社，汽车，电气，半导体，互联网；到了中国大概率还是中字头，资源类，制造业龙头，龙头科创互联网。。\n",
      "6. Author ID: None, Comment: 我们的社会确实朝着这个方向走\n",
      "个人取代家庭，成为社会的最小单元了\n",
      "不管我们承不承认，年轻人的消费习惯和我们完全不一样了\n",
      "$泡泡玛特(09992)$\n",
      "7. Author ID: None, Comment: 内卷国家的必然\n",
      "8. Author ID: None, Comment: 有个手机就够了，老公老婆都可以不要\n",
      "9. Author ID: None, Comment: 不可能的。两个国家本质上的价值观完全不一样。我身边蛮多00后，目前没听到一个说30以后不结婚的。穷女和稳定男是最迫切想结婚的人群。这还是经常打游戏的宅群。\n",
      "10. Author ID: None, Comment: 文明的诅咒\n",
      "11. Author ID: None, Comment: 如果真能成为日本那样的发达国家就知足吧、优衣库2000多美刀的收入，物价再贵去掉吃喝拉撒也能一个月换个苹果手机了，这就是我们这里看不起的售货员.\n",
      "12. Author ID: None, Comment: 对于不婚比例上升这个现象，思考两个原因：1.这是不是阶层固化的症状？没背景的年轻人对于未来失去希望和想象，然后躺平。2.在男女平等的舆论导向上，过度宣扬女权，抬高女性地位，造成青年男女对立？\n",
      "13. Author ID: None, Comment: 不一定会成日鬼那样的社会，天朝…\n",
      "14. Author ID: None, Comment: 我这段时间的观察与思考和这部分内容是一致的。我女儿最喜欢的便利店是全家Family Mart，里面的海苔饭团、咖喱鸡饭都是冷藏食品，她一点不嫌弃，反而很热爱，还兴高采烈地告诉我，她很喜欢这些食品。\n",
      "宠物这一部分我前段时间也在关注，今早还发了一条有关于此的消息。\n",
      "五一节期间和一位50岁的大哥聊天，他说令他意外的是，成都天府广场一座被传统零售商业模式淘汰的商场，现在被谷子经济救活了，里面人山人海，交易量火爆。\n",
      "15. Author ID: None, Comment: 性爱机器人产品会供不应求，这类公司会卷出一个品牌。\n",
      "16. Author ID: None, Comment: 中国人均财富水平距离日本还差的太远，更大的可能性是未富先老，或许更应该思考的是一大堆拮据的老年人最可能消费什么\n",
      "17. Author ID: None, Comment: 不婚不育最根本原因是上世纪不准生，一旦生态链损毁，修复何其难。\n",
      "18. Author ID: None, Comment: 社会，经济，人伦，地产，消费，拿本子比的原则上不是脑残就是在带节奏。连主权都没有的殖民地，怎么比？\n",
      "19. Author ID: None, Comment: 圣杯\n",
      "20. Author ID: None, Comment: 情绪经济啊\n",
      "21. Author ID: None, Comment: 感觉怡红院很快会出牌照？？\n",
      "22. Author ID: None, Comment: 养娃成本太高了，养差了自己又觉得对不起孩子，还不如不生。我亲戚去帮人补课，机构收1200一节课，她自己得100块，这种价格很多家庭都是难以承受的。各种培训班真的该好好打一下，往死里打。不然就会引起人的攀比心理，越攀比越累，干脆不生。\n",
      "23. Author ID: None, Comment: 手动转存\n",
      "24. Author ID: None, Comment: 干宠物经济就行了\n",
      "25. Author ID: None, Comment: 东达不可避免\n",
      "26. Author ID: None, Comment: 为宠物股炒作提供注释\n",
      "27. Author ID: None, Comment: 利好养老类股票。\n",
      "28. Author ID: None, Comment: 存\n",
      "29. Author ID: None, Comment: mark\n",
      "30. Author ID: None, Comment: 转\n",
      "31. Author ID: None, Comment: 转发。感觉陪伴型机器人在AI时代有巨大的发展潜力。\n",
      "32. Author ID: None, Comment: 转：\n",
      "1980年代，日本一人户占比只有20%，如今逼近40%。未来的日本将成为半数人口是单身的“超级单身社会”。\n",
      "而在中国，独居、晚婚、不婚人群也正快速上升。\n",
      "这不是偶然，而是结构性变化。\n",
      "一个人生活，意味着从吃饭、出行、情感，到陪伴、安全感，都要独自完成。这背后，藏着海量“新需求”和“新供给”。\n",
      "1️⃣ 一人食经济：711其实是日本最大的“餐厅”\n",
      "你以为它是便利店？其实它靠盒饭、饭团、即食热食，成了日本最大的餐饮品牌。\n",
      "2023财年营收破10万亿日元（约5000亿人民币），稳压传统连锁餐饮。\n",
      "东京的面馆、咖啡馆大量设置“一个人座位”，配隔板、配平板，边吃边追剧，社交压力为零——孤独，是可以被尊重的生活方式。\n",
      "2️⃣ 宠物经济爆发：猫狗比孩子还多\n",
      "日本宠物总数超过1600万，远超15岁以下儿童。孤独都市人，把情感投射给了猫狗。\n",
      "日本宠物主对待他们的宠物就像对待家人一样，甚至比自己的地位还高。宠物用品已全面“母婴化”：推车、衣服、零食甚至“宠物饮品”一应俱全。独酌时，希望宠物也能“陪一杯”。\n",
      "3️⃣ 陪伴型机器人：技术不卖功能，卖“被需要”\n",
      "日本情感机器人公司 GROOVE X推出的LOVOT，不扫地、不聊天、不能干活——但它会“撒娇”、会“跟着你”、会“让你想抱”。\n",
      "这是一台卖情绪价值的机器人\n",
      "33. Author ID: None, Comment: AI娃娃\n",
      "\n",
      "==============================\n",
      "Check console logs and 'screenshots_comment_authors' folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "from urllib.parse import urlparse # For robust href parsing if needed, though data-tooltip is primary\n",
    "\n",
    "def scrape_post_and_all_comments_with_author_ids(url, max_main_loops=20, scroll_pause_time=2.5):\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_comment_authors\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"post_author_id\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"post_author_id\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        interaction_wait = WebDriverWait(driver, 7)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Pop-up Handling ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath_item in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath_item))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "\n",
    "        # Scrape Main Post Content and Author ID\n",
    "        try:\n",
    "            print(\"Scraping main post content and author ID...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "            try: # Post author ID\n",
    "                post_author_link_xpath = \"//div[contains(@class, 'article__author')]//a[@data-tooltip and starts-with(@href, '/')]\"\n",
    "                author_link_element = wait.until(EC.presence_of_element_located((By.XPATH, post_author_link_xpath)))\n",
    "                author_id_val = author_link_element.get_attribute('data-tooltip')\n",
    "                if author_id_val and author_id_val.isdigit(): scraped_data[\"post_author_id\"] = author_id_val\n",
    "                else:\n",
    "                    href = author_link_element.get_attribute('href')\n",
    "                    if href: potential_id = href.strip('/').split('/')[-1]\n",
    "                    if potential_id.isdigit(): scraped_data[\"post_author_id\"] = potential_id\n",
    "                print(f\"Post author ID scraped: {scraped_data.get('post_author_id', 'N/A')}\")\n",
    "            except TimeoutException: print(\"Post author link (for ID) not found.\")\n",
    "            except Exception as e_author: print(f\"Error scraping post author ID: {e_author}\")\n",
    "        except Exception as e_post: print(f\"Error scraping post content/author: {e_post}\")\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_total_unique_texts_count = -1\n",
    "\n",
    "        for i in range(max_main_loops):\n",
    "            print(f\"--- Main Loop Iteration #{i+1} ---\")\n",
    "            action_taken_this_loop = False\n",
    "\n",
    "            # Expand Replies Logic (kept from your working version)\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复') and not(ancestor::div[contains(@style,'display: none')]) and not(ancestor::div[contains(@class,'hide')])]\"\n",
    "            expand_attempts = 0\n",
    "            while expand_attempts < 5: \n",
    "                expand_attempts += 1; clicked_an_expand_button_this_pass = False\n",
    "                try:\n",
    "                    visible_expand_buttons = [b for b in driver.find_elements(By.XPATH, expand_reply_xpath) if b.is_displayed() and b.is_enabled()]\n",
    "                    if not visible_expand_buttons:\n",
    "                        if expand_attempts > 1: print(f\"  No more 'Expand Replies' visible in sub-attempt {expand_attempts}.\")\n",
    "                        break\n",
    "                    for button in visible_expand_buttons:\n",
    "                        try:\n",
    "                            if not button.is_displayed() or not button.is_enabled(): continue\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", button); time.sleep(0.4)\n",
    "                            button_to_click = WebDriverWait(driver, 3).until(EC.element_to_be_clickable(button))\n",
    "                            driver.execute_script(\"arguments[0].click();\", button_to_click)\n",
    "                            action_taken_this_loop = True; clicked_an_expand_button_this_pass = True; time.sleep(1.5)\n",
    "                        except (StaleElementReferenceException, TimeoutException, ElementNotInteractableException): continue\n",
    "                        except Exception as e_expand: print(f\"    Error clicking one 'Expand Replies': {e_expand}\")\n",
    "                    if not clicked_an_expand_button_this_pass and expand_attempts > 1 : break\n",
    "                    if clicked_an_expand_button_this_pass : time.sleep(0.5)\n",
    "                except Exception as e_find_expand: print(f\"  Error finding 'Expand Replies': {e_find_expand}\"); break\n",
    "\n",
    "            print(\"  Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "            \n",
    "            # Using the p_tag XPath that worked for text, and includes the double-underscore version from logs\n",
    "            comment_p_tag_xpath = \"//div[@class='comment__item__main']/p | //div[contains(@class, 'comment__content')]/p\"\n",
    "            # The key is that your screenshot showed comment_item_main (single underscore) for the HIS1963 comment.\n",
    "            # Let's ensure we prioritize the structure from the screenshot.\n",
    "            # The first part of the OR should ideally match the screenshot.\n",
    "            # If the log said 'comment__item__main' was parent of p_tags, let's try that too.\n",
    "            # For now, sticking to the structure that should lead to comment_item (single underscore) ancestor.\n",
    "            # If comment_item_main (single) is parent of P, then ancestor::div[@class='comment_item'] should work.\n",
    "\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_p_tag_xpath)\n",
    "                new_comments_added_to_list_this_pass = 0\n",
    "                \n",
    "                if comment_p_tags: print(f\"    Found {len(comment_p_tags)} potential comment <p> tags for text.\")\n",
    "\n",
    "                for p_tag_idx, p_tag in enumerate(comment_p_tags):\n",
    "                    comment_text = \"\"; author_id = None\n",
    "                    try:\n",
    "                        comment_text = p_tag.text.strip()\n",
    "                        if not comment_text: continue\n",
    "\n",
    "                        if comment_text not in unique_comment_texts_scraped:\n",
    "                            unique_comment_texts_scraped.add(comment_text)\n",
    "                            print(f\"        Processing new unique text (idx {p_tag_idx}): '{comment_text[:30]}...'\")\n",
    "                            author_id = None \n",
    "                            comment_item_boundary = None # Initialize\n",
    "\n",
    "                            try:\n",
    "                                # Attempt to find the 'comment_item' ancestor, exactly as per your screenshot structure\n",
    "                                comment_item_boundary = p_tag.find_element(By.XPATH, \"ancestor::div[@class='comment_item' and @data-id][1]\")\n",
    "                                boundary_data_id = comment_item_boundary.get_attribute('data-id')\n",
    "                                print(f\"            p_idx {p_tag_idx}: Found 'comment_item' boundary (data-id: {boundary_data_id}) for text '{comment_text[:20]}...'\")\n",
    "                                \n",
    "                                # 1. Try user-name link (most specific to your screenshot)\n",
    "                                try:\n",
    "                                    author_link_el = comment_item_boundary.find_element(By.XPATH, \".//div[@class='comment_item_main_hd']//a[@class='user-name' and @data-tooltip]\")\n",
    "                                    print(f\"                p_idx {p_tag_idx}: Found 'user-name' link.\")\n",
    "                                    temp_author_id = author_link_el.get_attribute('data-tooltip')\n",
    "                                    if temp_author_id and temp_author_id.isdigit(): author_id = temp_author_id\n",
    "                                    else:\n",
    "                                        href = author_link_el.get_attribute('href')\n",
    "                                        if href: potential_id = href.strip('/').split('/')[-1];\n",
    "                                        if potential_id and potential_id.isdigit(): author_id = potential_id # check potential_id is not empty\n",
    "                                    if author_id: print(f\"                    p_idx {p_tag_idx}: Extracted author_id: {author_id} (from user-name)\")\n",
    "                                \n",
    "                                except NoSuchElementException:\n",
    "                                    print(f\"                p_idx {p_tag_idx}: 'user-name' link NOT found in 'comment_item' (data-id: {boundary_data_id}).\")\n",
    "                                    # 2. Fallback to avatar link (also specific to screenshot structure from comment_item)\n",
    "                                    try:\n",
    "                                        author_link_el = comment_item_boundary.find_element(By.XPATH, \"./a[@class='avatar' and @data-tooltip]\") # Avatar is direct child of comment_item\n",
    "                                        print(f\"                    p_idx {p_tag_idx}: Found 'avatar' link (direct child of comment_item).\")\n",
    "                                        temp_author_id = author_link_el.get_attribute('data-tooltip')\n",
    "                                        if temp_author_id and temp_author_id.isdigit(): author_id = temp_author_id\n",
    "                                        else:\n",
    "                                            href = author_link_el.get_attribute('href')\n",
    "                                            if href: potential_id = href.strip('/').split('/')[-1];\n",
    "                                            if potential_id and potential_id.isdigit(): author_id = potential_id\n",
    "                                        if author_id: print(f\"                        p_idx {p_tag_idx}: Extracted author_id: {author_id} (from avatar)\")\n",
    "                                    except NoSuchElementException:\n",
    "                                        print(f\"                    p_idx {p_tag_idx}: 'avatar' link (direct child) also NOT found in 'comment_item' (data-id: {boundary_data_id}).\")\n",
    "                                        \n",
    "                            except NoSuchElementException:\n",
    "                                print(f\"            p_idx {p_tag_idx}: MAJOR FAIL: Could NOT find 'comment_item' (class='comment_item' and @data-id) ancestor for p_tag. Text: '{comment_text[:20]}...'\")\n",
    "                            except Exception as e_author_find:\n",
    "                                print(f\"            p_idx {p_tag_idx}: Error finding author ID for text '{comment_text[:20]}...': {type(e_author_find).__name__} - {e_author_find}\")\n",
    "\n",
    "                            scraped_data[\"comments\"].append({\"text\": comment_text, \"author_id\": author_id})\n",
    "                            new_comments_added_to_list_this_pass += 1\n",
    "                            \n",
    "                    except StaleElementReferenceException: print(f\"      Stale p_tag {p_tag_idx} encountered.\") ; continue \n",
    "                    except Exception as e_proc_p: print(f\"      Error processing one p_tag (idx {p_tag_idx}): {type(e_proc_p).__name__} - {e_proc_p}\")\n",
    "                \n",
    "                if new_comments_added_to_list_this_pass > 0:\n",
    "                    print(f\"    Added {new_comments_added_to_list_this_pass} new unique comments to data list.\")\n",
    "                    action_taken_this_loop = True\n",
    "            except Exception as e_find_comments: print(f\"  Error finding comment <p> tags: {e_find_comments}\")\n",
    "\n",
    "            # Load More Comments Logic\n",
    "            load_more_comments_xpath = \"//a[@class='show_more' and .//span[text()='展开查看更多']]\"\n",
    "            try:\n",
    "                load_more_button = interaction_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", load_more_button); time.sleep(0.4)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                action_taken_this_loop = True; time.sleep(scroll_pause_time + 0.5)\n",
    "            except TimeoutException: pass # Button not always present\n",
    "            except Exception as e_load_more: print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            current_unique_texts_count = len(unique_comment_texts_scraped)\n",
    "            print(f\"  Loop {i+1} end. Total unique comment texts scraped: {current_unique_texts_count}. Previously: {last_total_unique_texts_count}. Total comments in list: {len(scraped_data['comments'])}\")\n",
    "\n",
    "            if not action_taken_this_loop and current_unique_texts_count == last_total_unique_texts_count:\n",
    "                if last_total_unique_texts_count != -1 or i > 0 : \n",
    "                    print(\"No actions taken and unique comment text count unchanged. Assuming completion.\")\n",
    "                    break\n",
    "            last_total_unique_texts_count = current_unique_texts_count\n",
    "            if i == max_main_loops - 1: print(\"Reached max main loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"main_loop_end_{i+1}.png\"))\n",
    "\n",
    "        print(f\"\\n--- Finished comment scraping. Total comments in list: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\"); print(f\"Error Type: {type(e).__name__}\"); print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try: error_ss_path = os.path.join(screenshot_dir, \"critical_error_comment_authors.png\"); driver.save_screenshot(error_ss_path); print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit: print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5669998349/334081638\" # Your example post\n",
    "    # target_url = \"https://xueqiu.com/1929796349/272374603\" # Example with many comments & replies for testing robustness\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} (Comment Author IDs) ---\")\n",
    "    \n",
    "    data = scrape_post_and_all_comments_with_author_ids(target_url, max_main_loops=3, scroll_pause_time=2.5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\"); print(f\"Author ID: {data.get('post_author_id', 'N/A')}\"); print(data[\"post_content\"])\n",
    "    else: print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for idx, comment_data_item in enumerate(data[\"comments\"]):\n",
    "            print(f\"{idx+1}. Author ID: {comment_data_item.get('author_id', 'N/A')}, Comment: {comment_data_item['text']}\")\n",
    "    else: print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30); print(f\"Check console logs and 'screenshots_comment_authors' folder for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b329a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraper for URL: https://xueqiu.com/5367879511/334490187 (Comment Author IDs - Final Attempt Structure) ---\n",
      "Setting up WebDriver...\n",
      "Created 'screenshots_comment_authors_final' directory.\n",
      "Navigating to: https://xueqiu.com/5367879511/334490187\n",
      "Article body indicator loaded.\n",
      "Looking for '跳过' pop-up...\n",
      "'跳过' pop-up not found/timed out.\n",
      "Looking for 'X' pop-up...\n",
      "Finished checking for 'X' pop-ups.\n",
      "Scraping main post content and author ID...\n",
      "Post content scraped (length: 1032).\n",
      "Post author ID scraped: 5367879511\n",
      "\n",
      "--- Starting scroll and comment extraction ---\n",
      "--- Main Loop Iteration #1 ---\n",
      "  Scrolling down...\n",
      "    Found 20 potential comment <p> tags for text.\n",
      "        Processing new unique text (idx 0): '原来你是省心省力...'\n",
      "            p_idx 0: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 0: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 0: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 0: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 1): '一个可以借鉴的操作，至少应该包含以下几个要素：买什么，什么时...'\n",
      "            p_idx 1: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 1: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 1: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 1: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 2): '【存款没有亏本的风险，没有波动，一定是赚钱】\n",
      "存款一定是亏钱...'\n",
      "            p_idx 2: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 2: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 2: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 2: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 3): '为教授的无私奉献叫好...'\n",
      "            p_idx 3: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 3: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 3: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 3: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 4): '红利低波布道者...'\n",
      "            p_idx 4: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 4: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 4: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 4: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 5): '中证红利515080+红利低波563020+红利低波100 ...'\n",
      "            p_idx 5: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 5: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 5: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 5: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 6): '今天几分...'\n",
      "            p_idx 6: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 6: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 6: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 6: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 7): '红利低波类似狗股策略，不预测市场，定投就完事了...'\n",
      "            p_idx 7: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 7: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 7: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 7: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 8): '是实在人，干货难得，但散户得明白“替代存款”的本质是“用风险...'\n",
      "            p_idx 8: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 8: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 8: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 8: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 9): '红利低波确实好，现在估值高了，交易比较拥挤，适合大跌了再买，...'\n",
      "            p_idx 9: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 9: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 9: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 9: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 10): '教授你真是太伟大了，你是我网上唯一佩服的教授，你的打分系统就...'\n",
      "            p_idx 10: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 10: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 10: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 10: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 11): '拉长时间足够长，比如50年、100年，红利指数或者其他大盘指...'\n",
      "            p_idx 11: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 11: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 11: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 11: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 12): '感谢教授无私奉献！...'\n",
      "            p_idx 12: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 12: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 12: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 12: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 13): '还是建议普通人自己选20只股票，打造自己的红利低波指数。这有...'\n",
      "            p_idx 13: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 13: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 13: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 13: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 14): '讨论已被 红利低波第一人 删除...'\n",
      "            p_idx 14: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 14: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 14: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 14: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 15): '这个在股市一年的大部分时间还是不错的，但碰到年后一轮行情的时...'\n",
      "            p_idx 15: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 15: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 15: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 15: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 16): '教授的体系是一个简单、直接、粗暴的教我们赚长期收益的体系，希...'\n",
      "            p_idx 16: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 16: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 16: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 16: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 17): '感谢老师无私分享！...'\n",
      "            p_idx 17: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 17: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 17: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 17: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 18): '那现在距离3分还有一些啊...'\n",
      "            p_idx 18: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 18: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 18: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 18: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 19): '大家都来买 是不是该跑了 教授...'\n",
      "            p_idx 19: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 19: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 19: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 19: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "    Added 20 new unique comments to data list.\n",
      "  Loop 1 end. Total unique comment texts scraped: 20. Previously: -1. Total comments in list: 20\n",
      "--- Main Loop Iteration #2 ---\n",
      "  Scrolling down...\n",
      "    Found 40 potential comment <p> tags for text.\n",
      "        Processing new unique text (idx 20): '这个逻辑成立，跟我的想法一样。\n",
      "还有个点，如果遇到大跌，买这...'\n",
      "            p_idx 20: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 20: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 20: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 20: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 21): '教授，你年度到现在收益有多少哈？是定投的嘛...'\n",
      "            p_idx 21: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 21: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 21: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 21: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 22): '来不及加仓...'\n",
      "            p_idx 22: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 22: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 22: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 22: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 23): '涨的太快了，来不及加仓...'\n",
      "            p_idx 23: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 23: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 23: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 23: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 24): '教授您买的场内的还是场外的？...'\n",
      "            p_idx 24: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 24: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 24: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 24: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 25): '你这个系统打分的依据是什么？能让大家知道你是怎么打分的吗？...'\n",
      "            p_idx 25: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 25: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 25: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 25: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 26): '感谢教授，跟着教授的打分系统操作，虽然没有一天10厘米以上的...'\n",
      "            p_idx 26: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 26: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 26: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 26: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 27): '感谢每天的分享，我原来买银行理财的资金，也是逐笔到期转红利低...'\n",
      "            p_idx 27: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 27: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 27: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 27: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 28): '@红利低波第一人 教授，有没有可以跟低波红利玩翘翘板的ETF...'\n",
      "            p_idx 28: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 28: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 28: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 28: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 29): '楼主，512890不分红的，你介意吗？会考虑其他的有分红的红...'\n",
      "            p_idx 29: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 29: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 29: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 29: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 30): '学习...'\n",
      "            p_idx 30: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 30: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 30: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 30: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 31): '在教授这里收获很多，支持✊...'\n",
      "            p_idx 31: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 31: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 31: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 31: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 32): '请教一下您，您可以说一下，以红利低波为例，把您计算的公式写一...'\n",
      "            p_idx 32: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 32: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 32: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 32: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 33): '无风险收益率持续下降 都开始优质资产荒了 ，每年能产生稳定现...'\n",
      "            p_idx 33: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 33: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 33: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 33: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 34): '今年估计回不到3分以下了...'\n",
      "            p_idx 34: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 34: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 34: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 34: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 35): '银行存款是真分利息，红利低波动要看基金公司脸色分不分...'\n",
      "            p_idx 35: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 35: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 35: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 35: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 36): '长期定期存款是可以提前取出的，只不过是活期利息而已...'\n",
      "            p_idx 36: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 36: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 36: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 36: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 37): '弱弱的问一下，教授您的评分在哪里了看啊？我已经持有中证红利低...'\n",
      "            p_idx 37: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 37: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 37: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 37: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 38): '降息利好高股息资产，你看中人家的利息，已经涨了好多年的高股息...'\n",
      "            p_idx 38: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 38: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 38: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 38: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 39): '直接买 $中国海洋石油(00883)$ 一只就足够了，8.6...'\n",
      "            p_idx 39: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 39: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 39: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 39: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "    Added 20 new unique comments to data list.\n",
      "  Loop 2 end. Total unique comment texts scraped: 40. Previously: 20. Total comments in list: 40\n",
      "--- Main Loop Iteration #3 ---\n",
      "  Scrolling down...\n",
      "    Found 60 potential comment <p> tags for text.\n",
      "        Processing new unique text (idx 40): '用词再谨慎也没用，因为结论是错误的。...'\n",
      "            p_idx 40: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 40: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 40: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 40: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 41): '您说的 3 和 7 是什么意思呀 sorry小白 还望大神赐...'\n",
      "            p_idx 41: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 41: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 41: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 41: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 42): '3以下全仓，3-7之间定投，7以上不买，可以长期持有，也可以...'\n",
      "            p_idx 42: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 42: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 42: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 42: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 43): '感谢教授\n",
      "教授考不考虑给黄金也加个评分体系...'\n",
      "            p_idx 43: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 43: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 43: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 43: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 44): '跟我的感觉差不多，可惜两年前已经锁定了一大笔年金，还是认知不...'\n",
      "            p_idx 44: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 44: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 44: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 44: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 45): '好，支持，跟着...'\n",
      "            p_idx 45: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 45: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 45: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 45: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 46): '转发~...'\n",
      "            p_idx 46: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 46: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 46: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 46: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 47): '很好的建议，谢谢教授...'\n",
      "            p_idx 47: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 47: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 47: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 47: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 48): '红利低波历史数据已经做到6年1倍，对于散户来说是比较好的策略...'\n",
      "            p_idx 48: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 48: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 48: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 48: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 49): '每天必看，为教授的无私奉献点赞👍...'\n",
      "            p_idx 49: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 49: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 49: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 49: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 50): '非常同意，前段时间犹豫是不是买点年金保险，后来琢磨才发现，除...'\n",
      "            p_idx 50: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 50: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 50: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 50: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 51): '教授，在哪儿看分...'\n",
      "            p_idx 51: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 51: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 51: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 51: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 52): '营销广告...'\n",
      "            p_idx 52: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 52: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 52: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 52: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 53): '教授牛逼，顶起来！哈哈...'\n",
      "            p_idx 53: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 53: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 53: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 53: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 54): '时间足够长的话，任何指数都可以...'\n",
      "            p_idx 54: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 54: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 54: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 54: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 55): '@红利低波第一人 请问哪里能看到红利etf类似于pe的数据...'\n",
      "            p_idx 55: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 55: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 55: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 55: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 57): '通明、透彻，必须要赞！...'\n",
      "            p_idx 57: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 57: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 57: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 57: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 58): '谢谢教授！...'\n",
      "            p_idx 58: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 58: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 58: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 58: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "        Processing new unique text (idx 59): '这半年多来看至少3分买入的基本不会被套，感谢...'\n",
      "            p_idx 59: Parent of p_tag is <div class='comment__item__main'>\n",
      "                p_idx 59: 'user-name' structure NOT found under <div class='comment__item__main'>.\n",
      "            p_idx 59: Author_id still None. Trying avatar fallback.\n",
      "                p_idx 59: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\n",
      "    Added 19 new unique comments to data list.\n",
      "  Loop 3 end. Total unique comment texts scraped: 59. Previously: 40. Total comments in list: 59\n",
      "Reached max main loops.\n",
      "\n",
      "--- Finished comment scraping. Total comments in list: 59 ---\n",
      "Closing the browser...\n",
      "Browser closed.\n",
      "\n",
      "==============================\n",
      "      Scraped Data Summary\n",
      "==============================\n",
      "\n",
      "--- Main Post ---\n",
      "Author ID: 5367879511\n",
      "在接受这个理念前\n",
      "，\n",
      "一定要先考虑清楚\n",
      "，\n",
      "慎重做决定\n",
      "。\n",
      "红利低波是股票的集合\n",
      "，\n",
      "股票毕竟和存款是两种性质不同的东西\n",
      "。\n",
      "存款没有亏本的风险\n",
      "，\n",
      "没有波动\n",
      "，\n",
      "一定是赚钱的\n",
      "。\n",
      "红利低波有浮亏的风险\n",
      "，\n",
      "有价格的波动\n",
      "，\n",
      "可能在某段时间里\n",
      "，\n",
      "是亏钱的\n",
      "。\n",
      "我个人认为\n",
      "，\n",
      "红利低波可以替代长期定期存款\n",
      "。\n",
      "请注意我的用词\n",
      "，\n",
      "长期定期存款\n",
      "。\n",
      "长期指的是3年及以上\n",
      "，\n",
      "定期指的是在到期前无法取出\n",
      "，\n",
      "给你定死了\n",
      "。\n",
      "符合这些条件的存款\n",
      "，\n",
      "是可以用红利低波来代替它的\n",
      "。\n",
      "因为\n",
      "，\n",
      "用红利低波的收益更高\n",
      "。\n",
      "现在三年期存款的利息越来越低了\n",
      "，\n",
      "大概是每年2.5%左右\n",
      "，\n",
      "三年总收益大概是7-8%\n",
      "。\n",
      "买入红利低波\n",
      "，\n",
      "三年后的总收益肯定不止8%\n",
      "，\n",
      "年化收益5-10%\n",
      "，\n",
      "按最低5%算\n",
      "，\n",
      "也有16%的收益\n",
      "。\n",
      "对于想省心省力的人\n",
      "，\n",
      "跟随我的打分\n",
      "，\n",
      "3分以下全仓\n",
      "，\n",
      "买入后就可以不用管了\n",
      "，\n",
      "三年后再看\n",
      "，\n",
      "肯定比存银行赚的多\n",
      "。\n",
      "如果没来得及在3分以下全仓\n",
      "，\n",
      "可以在3-7之间定投\n",
      "。\n",
      "定投方法\n",
      "，\n",
      "把资金分成10份\n",
      "，\n",
      "每个月定投一份\n",
      "。\n",
      "对于能折腾的人\n",
      "，\n",
      "可以跟随我的打分\n",
      "，\n",
      "3以下全仓\n",
      "，\n",
      "3-7之间定投\n",
      "，\n",
      "7以上不买\n",
      "，\n",
      "可以长期持有\n",
      "，\n",
      "也可以择机卖出\n",
      "，\n",
      "高抛低吸\n",
      "。\n",
      "我个人是在9分以上才考虑卖出\n",
      "，\n",
      "没有到9\n",
      "，\n",
      "我就长期持有\n",
      "。\n",
      "我自己的长期年化是超过15%的\n",
      "。\n",
      "来雪球这么多年\n",
      "，\n",
      "那些大V的文章也看了不少\n",
      "，\n",
      "总体上有个感觉\n",
      "，\n",
      "就是总感觉遮遮掩掩的\n",
      "，\n",
      "缺少实质性的干货\n",
      "。\n",
      "有的人逻辑讲的很好\n",
      "，\n",
      "但是就是不告诉你具体买什么\n",
      "。\n",
      "有的人给出判断\n",
      "，\n",
      "说买什么\n",
      "，\n",
      "但是没有逻辑支撑或者逻辑经不起推敲\n",
      "，\n",
      "就是空洞的情绪和叙事\n",
      "。\n",
      "更多的人是含混不清\n",
      "，\n",
      "莫名其妙\n",
      "。\n",
      "一个可以借鉴的操作\n",
      "，\n",
      "至少应该包含以下几个要素：买什么\n",
      "，\n",
      "什么时候买\n",
      "，\n",
      "什么时候卖\n",
      "。\n",
      "不管是多大的大V\n",
      "，\n",
      "只要这三个要素不齐全\n",
      "，\n",
      "就都可以不用看了\n",
      "。\n",
      "因为\n",
      "，\n",
      "没有借鉴的意义\n",
      "。\n",
      "我的红利低波打分体系\n",
      "，\n",
      "就明确的告诉大家买红利低波指数基金\n",
      "，\n",
      "3以下全仓\n",
      "，\n",
      "3-7之间定投\n",
      "，\n",
      "7以上不买\n",
      "，\n",
      "可以长期持有\n",
      "，\n",
      "也可以择机卖出\n",
      "。\n",
      "是一个明确的操作体系\n",
      "，\n",
      "简单\n",
      "，\n",
      "直接\n",
      "，\n",
      "干货\n",
      "，\n",
      "不故弄玄虚\n",
      "，\n",
      "一看就明白\n",
      "。\n",
      "而且\n",
      "，\n",
      "我以前的文章把逻辑也讲得清清楚楚\n",
      "，\n",
      "经得起考验\n",
      "。\n",
      "$红利低波ETF(SH512890)$\n",
      "\n",
      "--- Comments (59) ---\n",
      "1. Author ID: None, Comment: 原来你是省心省力\n",
      "2. Author ID: None, Comment: 一个可以借鉴的操作，至少应该包含以下几个要素：买什么，什么时候买，什么时候卖。不管是多大的大V，只要这三个要素不齐全，就都可以不用看了。因为，没有借鉴的意义。\n",
      "3. Author ID: None, Comment: 【存款没有亏本的风险，没有波动，一定是赚钱】\n",
      "存款一定是亏钱的，只不过肉眼看不见\n",
      "4. Author ID: None, Comment: 为教授的无私奉献叫好\n",
      "5. Author ID: None, Comment: 红利低波布道者\n",
      "6. Author ID: None, Comment: 中证红利515080+红利低波563020+红利低波100 159307+红利低波50 515450+300红利低波515300+800红利低波159355(拟)+中证全指红利质量（159209）+现金流（国证+富时+中证）ETF，这种组合综合了格雷厄姆式与巴菲特芒格式两种价值投资理念。这些产品几乎（有些刚出来还不稳定，待观察）都有稳定的分红（现金流），利用这些分红（现金流）进行加仓复投、再平衡，只买不卖，靠分红退出（还可以省去卖出端的手续费 ）。实践下来，目前走势相当的稳健，体验感也相当的好。相比单一的用红利低波，也可以考虑下这一组合。\n",
      "7. Author ID: None, Comment: 今天几分\n",
      "8. Author ID: None, Comment: 红利低波类似狗股策略，不预测市场，定投就完事了\n",
      "9. Author ID: None, Comment: 是实在人，干货难得，但散户得明白“替代存款”的本质是“用风险换收益”，不是“稳赚不赔”。\n",
      "跟着体系操作前，先把仓位控制好（别超过家庭金融资产的30%），再设定好止损线（比如跌破打分体系20%强制检查逻辑），这样才能享受红利低波的长期红利，而不是被波动拍死在沙滩上。\n",
      "记住，在A股混，“不贪心”比“懂策略”更重要！\n",
      "10. Author ID: None, Comment: 红利低波确实好，现在估值高了，交易比较拥挤，适合大跌了再买，主要银行这两年涨太多了\n",
      "11. Author ID: None, Comment: 教授你真是太伟大了，你是我网上唯一佩服的教授，你的打分系统就是我加仓红利的底气和动力\n",
      "12. Author ID: None, Comment: 拉长时间足够长，比如50年、100年，红利指数或者其他大盘指数都将大幅盈利。关键是你要足够长寿\n",
      "13. Author ID: None, Comment: 感谢教授无私奉献！\n",
      "14. Author ID: None, Comment: 还是建议普通人自己选20只股票，打造自己的红利低波指数。这有几个好处：（1）锻炼自己的选股能力；（2）规避不必要的基金管理费和托管费；（3）还有额外的打新收益~\n",
      "15. Author ID: None, Comment: 讨论已被 红利低波第一人 删除\n",
      "16. Author ID: None, Comment: 这个在股市一年的大部分时间还是不错的，但碰到年后一轮行情的时候，或者来去年那样大行情的时候，就不香了\n",
      "17. Author ID: None, Comment: 教授的体系是一个简单、直接、粗暴的教我们赚长期收益的体系，希望教授坚持发布红利低波的评分。\n",
      "18. Author ID: None, Comment: 感谢老师无私分享！\n",
      "19. Author ID: None, Comment: 那现在距离3分还有一些啊\n",
      "20. Author ID: None, Comment: 大家都来买 是不是该跑了 教授\n",
      "21. Author ID: None, Comment: 这个逻辑成立，跟我的想法一样。\n",
      "还有个点，如果遇到大跌，买这个票也是非常合适的，基本上能拿到市场平均涨幅，而且风险基本没有。\n",
      "22. Author ID: None, Comment: 教授，你年度到现在收益有多少哈？是定投的嘛\n",
      "23. Author ID: None, Comment: 来不及加仓\n",
      "24. Author ID: None, Comment: 涨的太快了，来不及加仓\n",
      "25. Author ID: None, Comment: 教授您买的场内的还是场外的？\n",
      "26. Author ID: None, Comment: 你这个系统打分的依据是什么？能让大家知道你是怎么打分的吗？\n",
      "27. Author ID: None, Comment: 感谢教授，跟着教授的打分系统操作，虽然没有一天10厘米以上的快感，但睡的更踏实。希望教授打分持续下去\n",
      "28. Author ID: None, Comment: 感谢每天的分享，我原来买银行理财的资金，也是逐笔到期转红利低波etf了，省心。\n",
      "29. Author ID: None, Comment: @红利低波第一人 教授，有没有可以跟低波红利玩翘翘板的ETF\n",
      "30. Author ID: None, Comment: 楼主，512890不分红的，你介意吗？会考虑其他的有分红的红利低波ETF吗？\n",
      "31. Author ID: None, Comment: 学习\n",
      "32. Author ID: None, Comment: 在教授这里收获很多，支持✊\n",
      "33. Author ID: None, Comment: 请教一下您，您可以说一下，以红利低波为例，把您计算的公式写一下吗？\n",
      "34. Author ID: None, Comment: 无风险收益率持续下降 都开始优质资产荒了 ，每年能产生稳定现金流的资产才是好资产。\n",
      "35. Author ID: None, Comment: 今年估计回不到3分以下了\n",
      "36. Author ID: None, Comment: 银行存款是真分利息，红利低波动要看基金公司脸色分不分\n",
      "37. Author ID: None, Comment: 长期定期存款是可以提前取出的，只不过是活期利息而已\n",
      "38. Author ID: None, Comment: 弱弱的问一下，教授您的评分在哪里了看啊？我已经持有中证红利低波，正在考虑如何加仓了，不知道现在是几分。\n",
      "39. Author ID: None, Comment: 降息利好高股息资产，你看中人家的利息，已经涨了好多年的高股息资产正好要了你们的本金\n",
      "牛市，等加息利好消费板块吧\n",
      "40. Author ID: None, Comment: 直接买 $中国海洋石油(00883)$ 一只就足够了，8.6% 股息 垄断央企 回购40亿. 买这个一揽子不如买一个\n",
      "41. Author ID: None, Comment: 用词再谨慎也没用，因为结论是错误的。\n",
      "42. Author ID: None, Comment: 您说的 3 和 7 是什么意思呀 sorry小白 还望大神赐教\n",
      "43. Author ID: None, Comment: 3以下全仓，3-7之间定投，7以上不买，可以长期持有，也可以择机卖出。这个在哪里看呢？\n",
      "44. Author ID: None, Comment: 感谢教授\n",
      "教授考不考虑给黄金也加个评分体系\n",
      "45. Author ID: None, Comment: 跟我的感觉差不多，可惜两年前已经锁定了一大笔年金，还是认知不完美。\n",
      "46. Author ID: None, Comment: 好，支持，跟着\n",
      "47. Author ID: None, Comment: 转发~\n",
      "48. Author ID: None, Comment: 很好的建议，谢谢教授\n",
      "49. Author ID: None, Comment: 红利低波历史数据已经做到6年1倍，对于散户来说是比较好的策略。\n",
      "50. Author ID: None, Comment: 每天必看，为教授的无私奉献点赞👍\n",
      "51. Author ID: None, Comment: 非常同意，前段时间犹豫是不是买点年金保险，后来琢磨才发现，除了可能将来会有一些传承风险，当前的价格买入一些ETF稳稳的跑赢保险。\n",
      "52. Author ID: None, Comment: 教授，在哪儿看分\n",
      "53. Author ID: None, Comment: 营销广告\n",
      "54. Author ID: None, Comment: 教授牛逼，顶起来！哈哈\n",
      "55. Author ID: None, Comment: 时间足够长的话，任何指数都可以\n",
      "56. Author ID: None, Comment: @红利低波第一人 请问哪里能看到红利etf类似于pe的数据\n",
      "57. Author ID: None, Comment: 通明、透彻，必须要赞！\n",
      "58. Author ID: None, Comment: 谢谢教授！\n",
      "59. Author ID: None, Comment: 这半年多来看至少3分买入的基本不会被套，感谢\n",
      "\n",
      "==============================\n",
      "Check console logs and 'screenshots_comment_authors_final' folder for details.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException\n",
    ")\n",
    "\n",
    "def scrape_post_and_all_comments_final_attempt(url, max_main_loops=20, scroll_pause_time=2.5):\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\"); options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\"); options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--disable-notifications\"); options.add_argument(\"--lang=zh-CN\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    screenshot_dir = \"screenshots_comment_authors_final\"\n",
    "    if not os.path.exists(screenshot_dir):\n",
    "        try: os.makedirs(screenshot_dir); print(f\"Created '{screenshot_dir}' directory.\")\n",
    "        except OSError as e: print(f\"Error creating screenshot directory: {e}\"); return {\"post_content\": None, \"post_author_id\": None, \"comments\": []}\n",
    "\n",
    "    driver = None\n",
    "    scraped_data = {\"post_content\": None, \"post_author_id\": None, \"comments\": []}\n",
    "    unique_comment_texts_scraped = set()\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        interaction_wait = WebDriverWait(driver, 7)\n",
    "\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\")))\n",
    "            print(\"Article body indicator loaded.\")\n",
    "        except TimeoutException:\n",
    "            print(\"Article body indicator did not load.\"); driver.save_screenshot(os.path.join(screenshot_dir,\"error_page_load.png\")); return scraped_data\n",
    "        time.sleep(2)\n",
    "\n",
    "        # --- Pop-up Handling ---\n",
    "        try:\n",
    "            print(\"Looking for '跳过' pop-up...\"); skip_xpath = \"//span[text()='跳过'] | //button[contains(.,'跳过')]\"; skip_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, skip_xpath)))\n",
    "            driver.execute_script(\"arguments[0].click();\", skip_button); print(\"Clicked '跳过'.\"); time.sleep(0.5)\n",
    "        except TimeoutException: print(\"'跳过' pop-up not found/timed out.\")\n",
    "        except Exception as e: print(f\"Error '跳过': {e}\")\n",
    "        try:\n",
    "            print(\"Looking for 'X' pop-up...\"); close_xpaths = [ \"//div[contains(@class,'modal-wrapper')]//i[contains(@class,'icon-close')]\", \"//i[contains(@class, 'cube-dialog-close')]\" ]\n",
    "            for xpath_item in close_xpaths:\n",
    "                 try: close_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, xpath_item))); driver.execute_script(\"arguments[0].click();\", close_button); print(\"Clicked 'X'.\"); time.sleep(0.5); break\n",
    "                 except: continue\n",
    "            print(\"Finished checking for 'X' pop-ups.\")\n",
    "        except Exception as e: print(f\"Error 'X' pop-up: {e}\")\n",
    "\n",
    "        # Scrape Main Post Content and Author ID\n",
    "        try:\n",
    "            print(\"Scraping main post content and author ID...\")\n",
    "            post_content_xpath = \"//div[contains(@class, 'article__bd__detail')] | //div[contains(@class, 'article__content')]\"\n",
    "            post_element = wait.until(EC.visibility_of_element_located((By.XPATH, post_content_xpath)))\n",
    "            scraped_data[\"post_content\"] = post_element.text.strip()\n",
    "            print(f\"Post content scraped (length: {len(scraped_data['post_content'])}).\")\n",
    "            try: \n",
    "                post_author_link_xpath = \"//div[contains(@class, 'article__author')]//a[@data-tooltip and starts-with(@href, '/')]\"\n",
    "                author_link_element = wait.until(EC.presence_of_element_located((By.XPATH, post_author_link_xpath)))\n",
    "                author_id_val = author_link_element.get_attribute('data-tooltip')\n",
    "                if author_id_val and author_id_val.isdigit(): scraped_data[\"post_author_id\"] = author_id_val\n",
    "                else:\n",
    "                    href = author_link_element.get_attribute('href')\n",
    "                    if href: potential_id = href.strip('/').split('/')[-1]\n",
    "                    if potential_id.isdigit(): scraped_data[\"post_author_id\"] = potential_id\n",
    "                print(f\"Post author ID scraped: {scraped_data.get('post_author_id', 'N/A')}\")\n",
    "            except TimeoutException: print(\"Post author link (for ID) not found.\")\n",
    "            except Exception as e_author: print(f\"Error scraping post author ID: {e_author}\")\n",
    "        except Exception as e_post: print(f\"Error scraping post content/author: {e_post}\")\n",
    "\n",
    "        print(\"\\n--- Starting scroll and comment extraction ---\")\n",
    "        last_total_unique_texts_count = -1\n",
    "\n",
    "        for i in range(max_main_loops):\n",
    "            print(f\"--- Main Loop Iteration #{i+1} ---\")\n",
    "            action_taken_this_loop = False\n",
    "\n",
    "            expand_reply_xpath = \"//a[contains(text(), '查看') and contains(text(), '条回复') and not(ancestor::div[contains(@style,'display: none')]) and not(ancestor::div[contains(@class,'hide')])]\"\n",
    "            expand_attempts = 0\n",
    "            while expand_attempts < 5: \n",
    "                expand_attempts += 1; clicked_an_expand_button_this_pass = False\n",
    "                try:\n",
    "                    visible_expand_buttons = [b for b in driver.find_elements(By.XPATH, expand_reply_xpath) if b.is_displayed() and b.is_enabled()]\n",
    "                    if not visible_expand_buttons:\n",
    "                        if expand_attempts > 1: print(f\"  No more 'Expand Replies' visible in sub-attempt {expand_attempts}.\")\n",
    "                        break\n",
    "                    for button in visible_expand_buttons:\n",
    "                        try:\n",
    "                            if not button.is_displayed() or not button.is_enabled(): continue\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", button); time.sleep(0.4)\n",
    "                            button_to_click = WebDriverWait(driver, 3).until(EC.element_to_be_clickable(button))\n",
    "                            driver.execute_script(\"arguments[0].click();\", button_to_click)\n",
    "                            action_taken_this_loop = True; clicked_an_expand_button_this_pass = True; time.sleep(1.5)\n",
    "                        except (StaleElementReferenceException, TimeoutException, ElementNotInteractableException): continue\n",
    "                        except Exception as e_expand: print(f\"    Error clicking one 'Expand Replies': {e_expand}\")\n",
    "                    if not clicked_an_expand_button_this_pass and expand_attempts > 1 : break\n",
    "                    if clicked_an_expand_button_this_pass : time.sleep(0.5)\n",
    "                except Exception as e_find_expand: print(f\"  Error finding 'Expand Replies': {e_find_expand}\"); break\n",
    "\n",
    "            print(\"  Scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "            \n",
    "            # XPath for comment text <p> tags.\n",
    "            # Prioritizing the structure from your screenshot: div.comment_item_main (single underscore)\n",
    "            # Fallback to div.comment__item__main (double underscore from logs) or div.comment__content (for replies)\n",
    "            comment_p_tag_xpath = (\"//div[@class='comment_item_main']/p \" # Exact match for screenshot structure\n",
    "                                   \"| //div[@class='comment__item__main']/p \" # Match from logs\n",
    "                                   \"| //div[contains(@class, 'comment__content')]/p\") # For replies\n",
    "\n",
    "            try:\n",
    "                comment_p_tags = driver.find_elements(By.XPATH, comment_p_tag_xpath)\n",
    "                new_comments_added_to_list_this_pass = 0\n",
    "                \n",
    "                if comment_p_tags: print(f\"    Found {len(comment_p_tags)} potential comment <p> tags for text.\")\n",
    "\n",
    "                for p_tag_idx, p_tag in enumerate(comment_p_tags):\n",
    "                    comment_text = \"\"; author_id = None; p_parent_div = None\n",
    "                    try:\n",
    "                        comment_text = p_tag.text.strip()\n",
    "                        if not comment_text: continue\n",
    "\n",
    "                        if comment_text not in unique_comment_texts_scraped:\n",
    "                            unique_comment_texts_scraped.add(comment_text)\n",
    "                            print(f\"        Processing new unique text (idx {p_tag_idx}): '{comment_text[:30]}...'\")\n",
    "                            \n",
    "                            try:\n",
    "                                p_parent_div = p_tag.find_element(By.XPATH, \"./parent::div\")\n",
    "                                parent_class = p_parent_div.get_attribute('class')\n",
    "                                print(f\"            p_idx {p_tag_idx}: Parent of p_tag is <div class='{parent_class}'>\")\n",
    "\n",
    "                                # Strategy 1: Based on screenshot (p_parent is div.comment_item_main)\n",
    "                                if \"comment_item_main\" == parent_class or \"comment__item__main\" == parent_class: # Allow single or double underscore\n",
    "                                    try:\n",
    "                                        header_div = p_parent_div.find_element(By.XPATH, \"./div[@class='comment_item_main_hd']\") # Exact class\n",
    "                                        print(f\"                p_idx {p_tag_idx}: Found header_div <div class='{header_div.get_attribute('class')}'>\")\n",
    "                                        author_link_el = header_div.find_element(By.XPATH, \"./a[@class='user-name' and @data-tooltip]\")\n",
    "                                        print(f\"                    p_idx {p_tag_idx}: Found 'user-name' link.\")\n",
    "                                        temp_author_id = author_link_el.get_attribute('data-tooltip')\n",
    "                                        if temp_author_id and temp_author_id.isdigit(): author_id = temp_author_id\n",
    "                                        else:\n",
    "                                            href = author_link_el.get_attribute('href')\n",
    "                                            if href: potential_id = href.strip('/').split('/')[-1];\n",
    "                                            if potential_id and potential_id.isdigit(): author_id = potential_id\n",
    "                                        if author_id: print(f\"                        p_idx {p_tag_idx}: Extracted author_id: {author_id} (from user-name)\")\n",
    "                                    except NoSuchElementException:\n",
    "                                        print(f\"                p_idx {p_tag_idx}: 'user-name' structure NOT found under <div class='{parent_class}'>.\")\n",
    "                                \n",
    "                                # Fallback: If author_id still None, try avatar from grandparent (div.comment_item)\n",
    "                                if author_id is None and p_parent_div is not None: # Check p_parent_div exists\n",
    "                                    print(f\"            p_idx {p_tag_idx}: Author_id still None. Trying avatar fallback.\")\n",
    "                                    try:\n",
    "                                        # Parent of 'comment_item_main' (or 'comment__item__main') should be 'comment_item'\n",
    "                                        comment_item_div = p_parent_div.find_element(By.XPATH, \"./parent::div[@class='comment_item' and @data-id]\")\n",
    "                                        item_data_id = comment_item_div.get_attribute('data-id')\n",
    "                                        print(f\"                p_idx {p_tag_idx}: Fallback: Found grandparent 'comment_item' (data-id: {item_data_id}).\")\n",
    "                                        author_link_el = comment_item_div.find_element(By.XPATH, \"./a[@class='avatar' and @data-tooltip]\") # Avatar is direct child\n",
    "                                        print(f\"                    p_idx {p_tag_idx}: Fallback: Found 'avatar' link.\")\n",
    "                                        temp_author_id = author_link_el.get_attribute('data-tooltip')\n",
    "                                        if temp_author_id and temp_author_id.isdigit(): author_id = temp_author_id\n",
    "                                        else:\n",
    "                                            href = author_link_el.get_attribute('href')\n",
    "                                            if href: potential_id = href.strip('/').split('/')[-1];\n",
    "                                            if potential_id and potential_id.isdigit(): author_id = potential_id\n",
    "                                        if author_id: print(f\"                        p_idx {p_tag_idx}: Fallback: Extracted author_id: {author_id} (from avatar)\")\n",
    "                                    except NoSuchElementException:\n",
    "                                        print(f\"                p_idx {p_tag_idx}: Fallback: 'comment_item' grandparent or 'avatar' link NOT found.\")\n",
    "                                        \n",
    "                            except NoSuchElementException:\n",
    "                                print(f\"            p_idx {p_tag_idx}: Parent <div> of p_tag NOT found. Cannot determine author. Text: '{comment_text[:20]}...'\")\n",
    "                            except Exception as e_author_find:\n",
    "                                print(f\"            p_idx {p_tag_idx}: Error finding author ID for text '{comment_text[:20]}...': {type(e_author_find).__name__} - {e_author_find}\")\n",
    "\n",
    "                            scraped_data[\"comments\"].append({\"text\": comment_text, \"author_id\": author_id})\n",
    "                            new_comments_added_to_list_this_pass += 1\n",
    "                            \n",
    "                    except StaleElementReferenceException: print(f\"      Stale p_tag {p_tag_idx} encountered.\") ; continue \n",
    "                    except Exception as e_proc_p: print(f\"      Error processing one p_tag (idx {p_tag_idx}): {type(e_proc_p).__name__} - {e_proc_p}\")\n",
    "                \n",
    "                if new_comments_added_to_list_this_pass > 0:\n",
    "                    print(f\"    Added {new_comments_added_to_list_this_pass} new unique comments to data list.\")\n",
    "                    action_taken_this_loop = True\n",
    "            except Exception as e_find_comments: print(f\"  Error finding comment <p> tags: {e_find_comments}\")\n",
    "\n",
    "            # Load More Comments Logic\n",
    "            load_more_comments_xpath = \"//a[@class='show_more' and .//span[text()='展开查看更多']]\"\n",
    "            try:\n",
    "                load_more_button = interaction_wait.until(EC.element_to_be_clickable((By.XPATH, load_more_comments_xpath)))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", load_more_button); time.sleep(0.4)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "                action_taken_this_loop = True; time.sleep(scroll_pause_time + 0.5)\n",
    "            except TimeoutException: pass \n",
    "            except Exception as e_load_more: print(f\"  Error clicking '展开查看更多': {e_load_more}\")\n",
    "\n",
    "            current_unique_texts_count = len(unique_comment_texts_scraped)\n",
    "            print(f\"  Loop {i+1} end. Total unique comment texts scraped: {current_unique_texts_count}. Previously: {last_total_unique_texts_count}. Total comments in list: {len(scraped_data['comments'])}\")\n",
    "\n",
    "            if not action_taken_this_loop and current_unique_texts_count == last_total_unique_texts_count:\n",
    "                if last_total_unique_texts_count != -1 or i > 0 : \n",
    "                    print(\"No actions taken and unique comment text count unchanged. Assuming completion.\")\n",
    "                    break\n",
    "            last_total_unique_texts_count = current_unique_texts_count\n",
    "            if i == max_main_loops - 1: print(\"Reached max main loops.\")\n",
    "            driver.save_screenshot(os.path.join(screenshot_dir,f\"main_loop_end_{i+1}.png\"))\n",
    "\n",
    "        print(f\"\\n--- Finished comment scraping. Total comments in list: {len(scraped_data['comments'])} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An critical error occurred ---\"); print(f\"Error Type: {type(e).__name__}\"); print(f\"Error Details: {e}\")\n",
    "        if driver:\n",
    "            try: error_ss_path = os.path.join(screenshot_dir, \"critical_error_comment_authors_final.png\"); driver.save_screenshot(error_ss_path); print(f\"Saved critical error screenshot.\")\n",
    "            except Exception as e_ss_crit: print(f\"Could not save critical error screenshot: {e_ss_crit}\")\n",
    "    finally:\n",
    "        if driver: print(\"Closing the browser...\"); driver.quit(); print(\"Browser closed.\")\n",
    "    return scraped_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = \"https://xueqiu.com/5367879511/334490187\" # Using your new example URL\n",
    "    print(f\"--- Starting Scraper for URL: {target_url} (Comment Author IDs - Final Attempt Structure) ---\")\n",
    "    \n",
    "    data = scrape_post_and_all_comments_final_attempt(target_url, max_main_loops=3, scroll_pause_time=2.5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30); print(\"      Scraped Data Summary\"); print(\"=\"*30)\n",
    "    if data[\"post_content\"]:\n",
    "        print(\"\\n--- Main Post ---\"); print(f\"Author ID: {data.get('post_author_id', 'N/A')}\"); print(data[\"post_content\"])\n",
    "    else: print(\"\\n>>> Main post content not scraped. <<<\")\n",
    "\n",
    "    if data[\"comments\"]:\n",
    "        print(f\"\\n--- Comments ({len(data['comments'])}) ---\")\n",
    "        for idx, comment_data_item in enumerate(data[\"comments\"]):\n",
    "            print(f\"{idx+1}. Author ID: {comment_data_item.get('author_id', 'N/A')}, Comment: {comment_data_item['text']}\")\n",
    "    else: print(\"\\n>>> No comments were scraped. <<<\")\n",
    "    print(\"\\n\" + \"=\"*30); print(f\"Check console logs and 'screenshots_comment_authors_final' folder for details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
